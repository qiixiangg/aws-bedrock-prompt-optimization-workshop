{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 07: Evaluations\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this final notebook, we run comprehensive evaluations across all deployed agent versions to validate that optimizations improved cost and latency **without degrading quality**.\n",
    "\n",
    "**What you'll learn:**\n",
    "- How to run systematic evaluations across agent versions\n",
    "- How to compare metrics side-by-side\n",
    "- How to validate quality hasn't degraded\n",
    "- How to generate a final optimization report\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed Labs 01-06 (all agent versions deployed)\n",
    "\n",
    "## Workshop Journey\n",
    "\n",
    "```\n",
    "01 Baseline → 02 Quick Wins → 03 Caching → 04 Routing → 05 Guardrails → 06 Gateway → [07 Evaluations]\n",
    "                                                                                          ↑\n",
    "                                                                                     You are here\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport json\nimport uuid\nimport time\nfrom pathlib import Path\nfrom dotenv import load_dotenv\n\nload_dotenv(override=True)\n\nimport boto3\nimport pandas as pd\n\nregion = os.environ.get(\"AWS_DEFAULT_REGION\", \"us-east-1\")\ncontrol_client = boto3.client(\"bedrock-agentcore-control\", region_name=region)\ndata_client = boto3.client(\"bedrock-agentcore\", region_name=region)\n\nprint(f\"Region: {region}\")\nprint(f\"Langfuse Host: {os.environ.get('LANGFUSE_HOST', 'https://cloud.langfuse.com')}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Find All Deployed Agent Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def find_agent_by_name(name_pattern):\n    \"\"\"Find agent ARN by name pattern (handles both hyphen and underscore naming).\"\"\"\n    response = control_client.list_agent_runtimes()\n    agents = response.get(\"agentRuntimes\", [])\n    for agent in agents:\n        agent_name = agent[\"agentRuntimeName\"]\n        # Handle both hyphen and underscore naming conventions\n        if name_pattern.replace(\"-\", \"_\") in agent_name or name_pattern in agent_name:\n            return agent[\"agentRuntimeArn\"]\n    return None\n\n# Find all agent versions\nversions = {\n    \"v1-baseline\": find_agent_by_name(\"v1-baseline\"),\n    \"v2-quick-wins\": find_agent_by_name(\"v2-quick-wins\"),\n    \"v3-caching\": find_agent_by_name(\"v3-caching\"),\n    \"v4-routing\": find_agent_by_name(\"v4-routing\"),\n    \"v5-guardrails\": find_agent_by_name(\"v5-guardrails\"),\n    \"v6-gateway\": find_agent_by_name(\"v6-gateway\"),\n}\n\nprint(\"Found agent versions:\")\nfor name, arn in versions.items():\n    status = \"Found\" if arn else \"Not found\"\n    print(f\"  {name}: {status}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Test Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Standard test prompts - each demonstrates a specific tool usage pattern\nTEST_PROMPTS = [\n    # Single tool: get_return_policy\n    {\"id\": \"return-policy\", \"query\": \"What is your return policy for laptops?\"},\n\n    # Single tool: get_product_info\n    {\"id\": \"product-info\", \"query\": \"Tell me about your smartphone options\"},\n\n    # Single tool: get_technical_support (Bedrock KB)\n    {\"id\": \"technical-support\", \"query\": \"My laptop won't turn on, can you help me troubleshoot?\"},\n\n    # Multi-tool: get_product_info + get_return_policy\n    {\"id\": \"multi-part\", \"query\": \"I want to buy a laptop. What are the specs and what's the return policy?\"},\n\n    # No tool: General greeting\n    {\"id\": \"general\", \"query\": \"Hello! What can you help me with today?\"},\n]\n\ntest_scenarios = TEST_PROMPTS\n\nprint(f\"Loaded {len(test_scenarios)} test scenarios:\")\nfor scenario in test_scenarios:\n    print(f\"  - {scenario['id']}: {scenario['query'][:50]}...\")"
  },
  {
   "cell_type": "code",
   "source": "# Import Langfuse metrics helper\nfrom utils.langfuse_metrics import (\n    get_latest_trace_metrics,\n    print_metrics,\n    clear_metrics,\n    collect_metric,\n    print_metrics_table,\n    get_collected_metrics\n)\n\nprint(\"Langfuse metrics helper imported\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Map version name to agent trace name (uses hyphens)\nTRACE_NAME_MAP = {\n    \"v1-baseline\": \"customer-support-v1-baseline\",\n    \"v2-quick-wins\": \"customer-support-v2-quick-wins\",\n    \"v3-caching\": \"customer-support-v3-caching\",\n    \"v4-routing\": \"customer-support-v4-routing\",\n    \"v5-guardrails\": \"customer-support-v5-guardrails\",\n    \"v6-gateway\": \"customer-support-v6-gateway\",\n}\n\n\ndef invoke_agent(agent_arn, prompt):\n    \"\"\"Invoke agent and measure latency.\"\"\"\n    start_time = time.time()\n    response = data_client.invoke_agent_runtime(\n        agentRuntimeArn=agent_arn,\n        runtimeSessionId=str(uuid.uuid4()),\n        payload=json.dumps({\"prompt\": prompt}).encode(),\n    )\n    latency_ms = (time.time() - start_time) * 1000\n    result = json.loads(response[\"response\"].read().decode(\"utf-8\"))\n    return result, latency_ms\n\n\ndef run_evaluation(version_name, agent_arn, scenarios):\n    \"\"\"Run all scenarios against an agent version and collect Langfuse metrics.\"\"\"\n    results = []\n    total_latency = 0\n    successful = 0\n    langfuse_metrics_list = []\n    \n    agent_trace_name = TRACE_NAME_MAP.get(version_name, version_name)\n    \n    print(f\"\\nEvaluating {version_name}...\")\n    clear_metrics()  # Clear for this version\n    \n    for scenario in scenarios:\n        try:\n            result, latency = invoke_agent(agent_arn, scenario[\"query\"])\n            \n            # Fetch Langfuse metrics for this trace\n            metrics = get_latest_trace_metrics(\n                agent_name=agent_trace_name,\n                wait_seconds=3,\n                max_retries=3,\n                timeout_seconds=60,\n            )\n            langfuse_metrics_list.append(metrics)\n            \n            results.append({\n                \"scenario_id\": scenario[\"id\"],\n                \"success\": True,\n                \"latency_ms\": latency,\n            })\n            total_latency += latency\n            successful += 1\n            print(f\"  [{scenario['id']}] {latency:.0f}ms\")\n        except Exception as e:\n            results.append({\n                \"scenario_id\": scenario[\"id\"],\n                \"success\": False,\n                \"error\": str(e),\n            })\n            langfuse_metrics_list.append({\"error\": str(e)})\n            print(f\"  [{scenario['id']}] FAILED: {e}\")\n    \n    return {\n        \"version\": version_name,\n        \"results\": results,\n        \"total_scenarios\": len(scenarios),\n        \"successful\": successful,\n        \"avg_latency_ms\": total_latency / successful if successful > 0 else 0,\n        \"langfuse_metrics\": langfuse_metrics_list,\n    }"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluations for all available versions\n",
    "all_results = {}\n",
    "\n",
    "for version_name, agent_arn in versions.items():\n",
    "    if agent_arn:\n",
    "        all_results[version_name] = run_evaluation(version_name, agent_arn, test_scenarios)\n",
    "    else:\n",
    "        print(f\"\\nSkipping {version_name} (not deployed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Generate Comparison Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "rows = []\n",
    "for version_name, eval_result in all_results.items():\n",
    "    success_rate = (eval_result[\"successful\"] / eval_result[\"total_scenarios\"]) * 100\n",
    "    rows.append({\n",
    "        \"Version\": version_name,\n",
    "        \"Success Rate\": f\"{success_rate:.0f}%\",\n",
    "        \"Avg Latency (ms)\": f\"{eval_result['avg_latency_ms']:.0f}\",\n",
    "        \"Successful\": eval_result[\"successful\"],\n",
    "        \"Failed\": eval_result[\"total_scenarios\"] - eval_result[\"successful\"],\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Show Langfuse metrics for each version\nprint(\"\\n\" + \"=\" * 100)\nprint(\"LANGFUSE METRICS BY VERSION\")\nprint(\"=\" * 100)\n\nversion_summaries = []\nfor version_name, eval_result in all_results.items():\n    lf_metrics = eval_result.get(\"langfuse_metrics\", [])\n    valid = [m for m in lf_metrics if \"error\" not in m]\n    \n    if valid:\n        total_input = sum(m.get('input_tokens', 0) for m in valid)\n        total_output = sum(m.get('output_tokens', 0) for m in valid)\n        total_cache_read = sum(m.get('cache_read_tokens', 0) for m in valid)\n        total_cache_write = sum(m.get('cache_write_tokens', 0) for m in valid)\n        total_cost = sum(m.get('cost_usd', 0) for m in valid)\n        avg_latency = sum(m.get('latency_seconds', 0) or 0 for m in valid) / len(valid)\n        avg_input = total_input / len(valid)\n        \n        version_summaries.append({\n            \"Version\": version_name,\n            \"Avg Input Tokens\": f\"{avg_input:,.0f}\",\n            \"Avg Latency (s)\": f\"{avg_latency:.2f}\",\n            \"Total Cost\": f\"${total_cost:.4f}\",\n            \"Cache Read\": f\"{total_cache_read:,}\",\n            \"Cache Write\": f\"{total_cache_write:,}\",\n        })\n\nif version_summaries:\n    df_langfuse = pd.DataFrame(version_summaries)\n    print(df_langfuse.to_string(index=False))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Calculate comprehensive improvement from baseline (including token metrics)\nif \"v1-baseline\" in all_results and len(all_results) > 1:\n    baseline = all_results[\"v1-baseline\"]\n    baseline_latency = baseline[\"avg_latency_ms\"]\n    \n    # Get baseline Langfuse metrics\n    baseline_lf = baseline.get(\"langfuse_metrics\", [])\n    baseline_valid = [m for m in baseline_lf if \"error\" not in m]\n    baseline_input = sum(m.get('input_tokens', 0) for m in baseline_valid) / len(baseline_valid) if baseline_valid else 0\n    baseline_cost = sum(m.get('cost_usd', 0) for m in baseline_valid)\n    \n    print(\"\\n\" + \"=\" * 80)\n    print(\"IMPROVEMENT VS BASELINE (v1)\")\n    print(\"=\" * 80)\n    print(f\"{'Version':<20} {'Input Tokens':<18} {'Latency':<15} {'Cost':<15}\")\n    print(\"-\" * 80)\n    print(f\"{'v1-baseline':<20} {'(baseline)':<18} {'(baseline)':<15} {'(baseline)':<15}\")\n    \n    for version_name, eval_result in all_results.items():\n        if version_name == \"v1-baseline\":\n            continue\n        \n        current_latency = eval_result[\"avg_latency_ms\"]\n        lf_metrics = eval_result.get(\"langfuse_metrics\", [])\n        valid = [m for m in lf_metrics if \"error\" not in m]\n        \n        if valid and baseline_input > 0:\n            avg_input = sum(m.get('input_tokens', 0) for m in valid) / len(valid)\n            total_cost = sum(m.get('cost_usd', 0) for m in valid)\n            \n            token_change = ((baseline_input - avg_input) / baseline_input) * 100\n            latency_change = ((baseline_latency - current_latency) / baseline_latency) * 100 if baseline_latency > 0 else 0\n            cost_change = ((baseline_cost - total_cost) / baseline_cost) * 100 if baseline_cost > 0 else 0\n            \n            print(f\"{version_name:<20} {token_change:+.1f}%{'':<12} {latency_change:+.1f}%{'':<10} {cost_change:+.1f}%\")\n        else:\n            print(f\"{version_name:<20} {'N/A':<18} {'N/A':<15} {'N/A':<15}\")\n    \n    print(\"=\" * 80)\n    print(\"\\nPositive % = improvement (reduction), Negative % = regression (increase)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: View Langfuse Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langfuse_host = os.environ.get(\"LANGFUSE_HOST\", \"https://cloud.langfuse.com\")\n",
    "print(f\"View comprehensive metrics at: {langfuse_host}\")\n",
    "print(\"\\nFor detailed comparison:\")\n",
    "print(\"1. Filter by version tags (v1-baseline, v2-quick-wins, etc.)\")\n",
    "print(\"2. Compare token usage across versions\")\n",
    "print(\"3. Check cache hit rates (v3+)\")\n",
    "print(\"4. Verify model routing (v4+)\")\n",
    "print(\"5. Review guardrail interventions (v5+)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"WORKSHOP SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "Optimizations Applied:\n",
    "\n",
    "1. Quick Wins (v2)\n",
    "   - Concise system prompt: ~60% token reduction\n",
    "   - max_tokens limit: Bounded output\n",
    "   - stop_sequences: Early termination\n",
    "\n",
    "2. Prompt Caching (v3)\n",
    "   - System prompt caching: 90% discount on repeated requests\n",
    "   - Tool definition caching: Additional savings\n",
    "\n",
    "3. Model Routing (v4)\n",
    "   - Haiku for simple queries: 12x cheaper input tokens\n",
    "   - Sonnet for complex queries: Maintained quality\n",
    "\n",
    "4. Guardrails (v5)\n",
    "   - Topic filtering: Block off-topic queries\n",
    "   - Content filtering: Improve safety\n",
    "   - Zero LLM tokens for blocked queries\n",
    "\n",
    "5. Gateway (v6)\n",
    "   - Semantic tool search: Load only relevant tools\n",
    "   - Reduced context size: Up to 75% fewer tool tokens\n",
    "\n",
    "Success Criteria:\n",
    "- Cost should decrease from v1 to v6\n",
    "- Latency should improve from v1 to v6\n",
    "- Quality (success rate) should NOT degrade\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup (Optional)\n",
    "\n",
    "Delete all deployed agents if you're done with the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to delete all customer-support agents\n",
    "# response = control_client.list_agent_runtimes()\n",
    "# for agent in response.get(\"agentRuntimes\", []):\n",
    "#     if \"customer-support\" in agent[\"agentRuntimeName\"]:\n",
    "#         agent_id = agent[\"agentRuntimeArn\"].split(\"/\")[-1]\n",
    "#         control_client.delete_agent_runtime(agentRuntimeId=agent_id)\n",
    "#         print(f\"Deleted: {agent['agentRuntimeName']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You've completed the Prompt Optimization Workshop!\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Start with measurement (baseline metrics)\n",
    "- Apply optimizations incrementally\n",
    "- Validate quality at each step\n",
    "- Use observability (Langfuse) to verify improvements\n",
    "\n",
    "**Next Steps:**\n",
    "- Apply these techniques to your own agents\n",
    "- Explore additional Bedrock features\n",
    "- Set up automated evaluation pipelines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}