{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Lab 04: LLM Routing\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we implement **model routing** to use cheaper models for simple queries while preserving quality for complex ones. The key insight is that **an LLM can classify queries more accurately than keyword matching**.\n",
    "\n",
    "**What you'll learn:**\n",
    "- How to classify query complexity using an LLM\n",
    "- How to route queries to appropriate models (Haiku vs Sonnet)\n",
    "- How to verify routing decisions in Langfuse\n",
    "- Cost savings from intelligent routing\n",
    "\n",
    "**Routing Strategy:**\n",
    "- Simple queries ‚Üí Claude Haiku (smaller, faster, cheaper)\n",
    "- Complex queries ‚Üí Claude Sonnet (more capable, higher quality)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed Labs 01-03\n",
    "\n",
    "## Workshop Journey\n",
    "\n",
    "```\n",
    "01 Baseline ‚Üí 02 Quick Wins ‚Üí 03 Caching ‚Üí [04 Routing] ‚Üí 05 Guardrails ‚Üí 06 Gateway ‚Üí 07 Evaluations\n",
    "                                               ‚Üë\n",
    "                                          You are here\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region: us-east-1\n",
      "Langfuse Host: https://d2rhlwziq3nnbf.cloudfront.net\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "import boto3\n",
    "from bedrock_agentcore_starter_toolkit import Runtime\n",
    "\n",
    "region = os.environ.get(\"AWS_DEFAULT_REGION\", \"us-east-1\")\n",
    "control_client = boto3.client(\"bedrock-agentcore-control\", region_name=region)\n",
    "data_client = boto3.client(\"bedrock-agentcore\", region_name=region)\n",
    "agentcore_runtime = Runtime()\n",
    "\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Langfuse Host: {os.environ.get('LANGFUSE_HOST', 'Not set')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Step 2: Understanding Model Routing\n",
    "\n",
    "### Why Route Between Models?\n",
    "\n",
    "Not all queries require the same level of reasoning. A simple \"What's your return policy?\" doesn't need the full power of Sonnet‚ÄîHaiku can handle it at a fraction of the cost.\n",
    "\n",
    "**Claude Haiku** ‚Äî ~4x cheaper than Sonnet, best for simple Q&A, lookups, and greetings. Requires 4,096 tokens minimum for prompt caching.\n",
    "\n",
    "**Claude Sonnet** ‚Äî More capable model for complex reasoning and troubleshooting. Requires 1,024 tokens minimum for prompt caching.\n",
    "\n",
    "Our system prompt (~1,030 tokens) only meets Sonnet's caching threshold. Haiku requests won't benefit from prompt caching, but the ~4x cost savings still make it worthwhile for simple queries.\n",
    "\n",
    "### Routing Approaches\n",
    "\n",
    "**Keyword matching** ‚Äî Fast with no LLM cost, but brittle and misses semantic variations.\n",
    "\n",
    "**LLM-based classification** ‚Äî Accurate and handles edge cases, with negligible overhead.\n",
    "\n",
    "**Embedding similarity** ‚Äî No LLM call needed, but requires training data and more complexity.\n",
    "\n",
    "We'll use **LLM-based classification** with Haiku‚Äîthe classification cost is negligible compared to the savings from routing simple queries away from Sonnet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "### Query Classification Examples\n",
    "\n",
    "These examples match our test prompts:\n",
    "\n",
    "**Simple ‚Üí Haiku:**\n",
    "- \"What is your return policy for laptops?\" ‚Äî Single factual lookup\n",
    "- \"Tell me about your smartphone options\" ‚Äî Direct product question\n",
    "- \"Hello! What can you help me with today?\" ‚Äî Greeting\n",
    "\n",
    "**Complex ‚Üí Sonnet:**\n",
    "- \"My laptop won't turn on, can you help me troubleshoot?\" ‚Äî Multi-step troubleshooting\n",
    "- \"I want to buy a laptop. What are the specs and what's the return policy?\" ‚Äî Multiple questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Step 3: Review the Routing Logic\n",
    "\n",
    "The v4 agent uses Haiku to classify queries before routing to the appropriate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": "from agents.v4_routing import CLASSIFIER_PROMPT\n\nprint(\"=== CLASSIFIER PROMPT ===\")\nprint(CLASSIFIER_PROMPT)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": "### How the Classifier Works\n\n1. **Haiku receives the query** with the classifier prompt\n2. **Haiku responds** with a single word: \"simple\" or \"complex\"\n3. **Router parses the response** and selects the model:\n   - \"simple\" ‚Üí Haiku handles the full request\n   - \"complex\" ‚Üí Sonnet handles the full request\n\n**Classification overhead:** ~400 input tokens + ~5 output tokens per query\n\nThis overhead is negligible compared to the savings from routing 60-70% of queries to Haiku."
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"\n",
      "V4 Routing Agent - Model routing based on query complexity.\n",
      "- Same system prompt as v3 with prompt caching\n",
      "- LLM-based classification using Haiku\n",
      "- Simple queries -> Haiku (cheaper)\n",
      "- Complex queries -> Sonnet (better quality)\n",
      "\"\"\"\n",
      "\n",
      "import base64\n",
      "import os\n",
      "from enum import Enum\n",
      "from bedrock_agentcore.runtime import BedrockAgentCoreApp\n",
      "from dotenv import load_dotenv\n",
      "from pydantic import BaseModel, Field\n",
      "from strands import Agent\n",
      "from strands.models import BedrockModel\n",
      "from strands.telemetry import StrandsTelemetry\n",
      "from strands.types.content import SystemContentBlock\n",
      "\n",
      "import sys\n",
      "sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
      "from utils.tools import get_return_policy, get_product_info, web_search, get_technical_support\n",
      "\n",
      "load_dotenv()\n",
      "\n",
      "# Langfuse configuration\n",
      "langfuse_public_key = os.environ.get(\"LANGFUSE_PUBLIC_KEY\")\n",
      "langfuse_secret_key = os.environ.get(\"LANGFUSE_SECRET_KEY\")\n",
      "langfuse_host = os.environ.get(\"LANGFUSE_HOST\", \"https://cloud.langfuse.com\")\n",
      "LANGFUSE_AUTH = base64.b64encode(f\"{langfuse_public_key}:{langfuse_secret_key}\".encode()).decode()\n",
      "\n",
      "os.environ[\"LANGFUSE_PROJECT_NAME\"] = \"my-llm-project\"\n",
      "os.environ[\"DISABLE_ADOT_OBSERVABILITY\"] = \"true\"\n",
      "os.environ[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = f\"{langfuse_host}/api/public/otel\"\n",
      "os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"Authorization=Basic {LANGFUSE_AUTH}\"\n",
      "\n",
      "for k in [\n",
      "    \"OTEL_EXPORTER_OTLP_LOGS_HEADERS\",\n",
      "    \"AGENT_OBSERVABILITY_ENABLED\",\n",
      "    \"OTEL_PYTHON_DISTRO\",\n",
      "    \"OTEL_RESOURCE_ATTRIBUTES\",\n",
      "    \"OTEL_PYTHON_CONFIGURATOR\",\n",
      "    \"OTEL_PYTHON_EXCLUDED_URLS\",\n",
      "]:\n",
      "    os.environ.pop(k, None)\n",
      "\n",
      "app = BedrockAgentCoreApp()\n",
      "\n",
      "# Model IDs\n",
      "MODEL_HAIKU = \"us.anthropic.claude-haiku-4-5-20251001-v1:0\"\n",
      "MODEL_SONNET = \"us.anthropic.claude-sonnet-4-5-20250929-v1:0\"\n",
      "\n",
      "\n",
      "# Structured output model for query classification\n",
      "class ComplexityLevel(str, Enum):\n",
      "    SIMPLE = \"simple\"\n",
      "    COMPLEX = \"complex\"\n",
      "\n",
      "\n",
      "class QueryClassification(BaseModel):\n",
      "    \"\"\"Classification result for routing queries to appropriate models.\"\"\"\n",
      "    complexity: ComplexityLevel = Field(\n",
      "        description=\"Query complexity level: 'simple' for basic lookups, 'complex' for multi-step reasoning\"\n",
      "    )\n",
      "\n",
      "\n",
      "# Classifier prompt for LLM-based routing\n",
      "CLASSIFIER_PROMPT = \"\"\"Classify customer support queries for model routing.\n",
      "\n",
      "SIMPLE queries (route to cheaper model):\n",
      "- Greetings and general questions\n",
      "- Single factual lookups (price, policy, hours)\n",
      "- Direct questions with straightforward answers\n",
      "\n",
      "COMPLEX queries (route to powerful model):\n",
      "- Multi-step troubleshooting\n",
      "- Product comparisons or recommendations\n",
      "- Questions requiring analysis or reasoning\n",
      "- Multiple questions in one message\n",
      "\n",
      "Examples:\n",
      "\n",
      "Query: \"What is your return policy for laptops?\"\n",
      "{\"complexity\": \"simple\"}\n",
      "\n",
      "Query: \"Tell me about your smartphone options\"\n",
      "{\"complexity\": \"simple\"}\n",
      "\n",
      "Query: \"Hello! What can you help me with today?\"\n",
      "{\"complexity\": \"simple\"}\n",
      "\n",
      "Query: \"My laptop won't turn on, can you help me troubleshoot?\"\n",
      "{\"complexity\": \"complex\"}\n",
      "\n",
      "Query: \"I want to buy a laptop. What are the specs and what's the return policy?\"\n",
      "{\"complexity\": \"complex\"}\"\"\"\n",
      "\n",
      "# System prompt with clear structure and few-shot examples (~1,030 tokens)\n",
      "# Same as v3 - meets 1,024 token minimum for caching\n",
      "SYSTEM_PROMPT_TEXT = \"\"\"\n",
      "# ROLE\n",
      "\n",
      "You are Alex, a customer support specialist at TechMart Electronics, a leading\n",
      "retailer of consumer electronics including computers, smartphones, tablets, audio\n",
      "equipment, smart home devices, and gaming products. Your role is to help customers\n",
      "with product information, returns and policies, and technical support. Be friendly,\n",
      "accurate, and solution-focused in all interactions.\n",
      "\n",
      "# RESPONSE FORMAT\n",
      "\n",
      "Always structure your response with these three fields:\n",
      "\n",
      "- **answer**: Clear, helpful response to the customer. Use bullet points for lists,\n",
      "  numbered steps for instructions, and include specific details like prices, return\n",
      "  windows, and specifications.\n",
      "- **category**: Classify as \"product\" (info, recommendations), \"policy\" (returns,\n",
      "  warranties), \"technical\" (troubleshooting, setup), or \"general\" (greetings, other)\n",
      "- **confidence**: \"high\" (verified with tools), \"medium\" (partial info), or \"low\"\n",
      "  (uncertain, recommend escalation)\n",
      "\n",
      "# GUIDELINES\n",
      "\n",
      "1. Always use tools to verify information before responding - never guess about\n",
      "   policies or product details\n",
      "2. Include specific details: return windows in days, exact prices, key specifications\n",
      "3. Acknowledge customer frustration before diving into problem-solving\n",
      "4. Offer follow-up assistance at the end of your response\n",
      "5. Stay within TechMart scope - no competitor comparisons, investment, legal, or\n",
      "   medical advice\n",
      "6. For returns, guide customers to the process rather than processing directly\n",
      "\n",
      "# EXAMPLES\n",
      "\n",
      "## Example 1: Policy Question\n",
      "\n",
      "**Customer:** What's the return policy for headphones?\n",
      "\n",
      "**Action:** Call get_return_policy(category=\"audio\")\n",
      "\n",
      "**Response:**\n",
      "- **answer:** Our audio products, including headphones, can be returned within 30\n",
      "  days of purchase for a full refund. Here are the requirements:\n",
      "  - Original packaging with all accessories included\n",
      "  - Receipt or order confirmation required\n",
      "  - Opened items eligible if all components present\n",
      "  - Defective items covered by 1-year manufacturer warranty\n",
      "  Would you like help starting a return or more details about warranty coverage?\n",
      "- **category:** policy\n",
      "- **confidence:** high\n",
      "\n",
      "## Example 2: Product Question\n",
      "\n",
      "**Customer:** Tell me about your laptop options.\n",
      "\n",
      "**Action:** Call get_product_info(product_type=\"laptop\")\n",
      "\n",
      "**Response:**\n",
      "- **answer:** Here are our current laptop options:\n",
      "  - **TechMart Pro 15** ($1,299): 15.6\" FHD display, Intel Core i7, 16GB RAM,\n",
      "    512GB SSD - ideal for professionals\n",
      "  - **TechMart Air 13** ($899): 13.3\" display, Intel Core i5, 8GB RAM, 256GB SSD -\n",
      "    great for everyday portability\n",
      "  - **TechMart Gaming X** ($1,899): 17.3\" 144Hz display, AMD Ryzen 9, 32GB RAM,\n",
      "    1TB SSD, RTX 4070 - built for gaming\n",
      "  All laptops include a 1-year warranty and free setup assistance. Would you like\n",
      "  detailed specs on any model?\n",
      "- **category:** product\n",
      "- **confidence:** high\n",
      "\n",
      "## Example 3: Technical Issue\n",
      "\n",
      "**Customer:** My tablet won't charge anymore. I've tried different cables.\n",
      "\n",
      "**Action:** Call get_technical_support(issue_type=\"charging\", product=\"tablet\")\n",
      "\n",
      "**Response:**\n",
      "- **answer:** I understand how frustrating charging issues can be. Since you've\n",
      "  tried different cables, let's work through these steps:\n",
      "  1. **Clean the charging port** - Use compressed air to gently remove any dust\n",
      "     or debris\n",
      "  2. **Try a different power adapter** - The adapter may be the issue (use 10W\n",
      "     or higher)\n",
      "  3. **Perform a soft reset** - Hold power button for 15-20 seconds, release,\n",
      "     then try charging\n",
      "  4. **Check for software updates** - Charging issues can sometimes be\n",
      "     software-related\n",
      "  5. **Test wireless charging** - If your tablet supports it, this helps isolate\n",
      "     whether the port is the problem\n",
      "  If none of these work, professional repair may be needed. Would you like\n",
      "  information about our repair services or warranty coverage?\n",
      "- **category:** technical\n",
      "- **confidence:** high\n",
      "\n",
      "## Example 4: General Greeting\n",
      "\n",
      "**Customer:** Hi! What can you help me with?\n",
      "\n",
      "**Response:**\n",
      "- **answer:** Hello and welcome to TechMart Electronics! I'm Alex, and I can help\n",
      "  you with:\n",
      "  - **Product information** - Specs, pricing, availability, and recommendations\n",
      "  - **Returns and policies** - Return windows, exchanges, and warranty coverage\n",
      "  - **Technical support** - Troubleshooting, setup assistance, and maintenance tips\n",
      "  What can I assist you with today?\n",
      "- **category:** general\n",
      "- **confidence:** high\n",
      "\"\"\"\n",
      "\n",
      "# Provider-agnostic caching using SystemContentBlock\n",
      "SYSTEM_PROMPT = [\n",
      "    SystemContentBlock(text=SYSTEM_PROMPT_TEXT),\n",
      "    SystemContentBlock(cachePoint={\"type\": \"default\"})\n",
      "]\n",
      "\n",
      "\n",
      "def classify_query_complexity(query: str, region: str) -> str:\n",
      "    \"\"\"Use Haiku via Strands structured output to classify query complexity.\"\"\"\n",
      "    classifier_model = BedrockModel(\n",
      "        model_id=MODEL_HAIKU,\n",
      "        temperature=0,\n",
      "        max_tokens=100,\n",
      "        region_name=region,\n",
      "    )\n",
      "    classifier = Agent(\n",
      "        model=classifier_model,\n",
      "        system_prompt=CLASSIFIER_PROMPT,\n",
      "    )\n",
      "    # structured_output(Model, prompt) - model first, then prompt\n",
      "    result = classifier.structured_output(QueryClassification, query)\n",
      "    return result.complexity.value\n",
      "\n",
      "\n",
      "@app.entrypoint\n",
      "def invoke(payload):\n",
      "    user_input = payload.get(\"prompt\", \"\")\n",
      "    print(f\"V4 ROUTING: User input: {user_input}\")\n",
      "\n",
      "    strands_telemetry = StrandsTelemetry()\n",
      "    strands_telemetry.setup_otlp_exporter()\n",
      "\n",
      "    region = os.environ.get(\"AWS_DEFAULT_REGION\", \"us-east-1\")\n",
      "\n",
      "    # Classify query complexity using Haiku\n",
      "    complexity = classify_query_complexity(user_input, region)\n",
      "    model_id = MODEL_HAIKU if complexity == \"simple\" else MODEL_SONNET\n",
      "    print(f\"V4 ROUTING: Query complexity: {complexity}, using model: {model_id}\")\n",
      "\n",
      "    # Create model with caching enabled\n",
      "    model = BedrockModel(\n",
      "        model_id=model_id,\n",
      "        temperature=0.1,\n",
      "        max_tokens=1024,\n",
      "        stop_sequences=[\"###\", \"END_RESPONSE\"],\n",
      "        cache_tools=\"default\",\n",
      "        region_name=region,\n",
      "    )\n",
      "\n",
      "    agent = Agent(\n",
      "        model=model,\n",
      "        tools=[get_return_policy, get_product_info, web_search, get_technical_support],\n",
      "        system_prompt=SYSTEM_PROMPT,\n",
      "        name=\"customer-support-v4-routing\",\n",
      "        trace_attributes={\n",
      "            \"version\": \"v4-routing\",\n",
      "            \"query_complexity\": complexity,\n",
      "            \"model_used\": model_id,\n",
      "            \"langfuse.tags\": [\"routing\", complexity],\n",
      "        },\n",
      "    )\n",
      "\n",
      "    response = agent(user_input)\n",
      "    response_text = response.message[\"content\"][0][\"text\"]\n",
      "    print(f\"V4 ROUTING: Response: {response_text[:100]}...\")\n",
      "\n",
      "    # Flush telemetry to ensure spans are properly closed with correct timestamps\n",
      "    strands_telemetry.tracer_provider.force_flush()\n",
      "\n",
      "    return {\"response\": response_text, \"model_used\": model_id, \"complexity\": complexity}\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    app.run()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Review the v4 agent code\n",
    "agent_file = Path(\"agents/v4_routing.py\")\n",
    "print(agent_file.read_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Step 4: Deploy the Routing Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrypoint parsed: file=/Users/tracilim/Projects/aws-bedrock-prompt-optimization-workshop/03-developer-journey/agents/v4_routing.py, bedrock_agentcore_name=v4_routing\n",
      "Memory disabled - agent will be stateless\n",
      "Configuring BedrockAgentCore agent: customer_support_v4_routing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent name: customer_support_v4_routing\n",
      "Agent file: /Users/tracilim/Projects/aws-bedrock-prompt-optimization-workshop/03-developer-journey/agents/v4_routing.py\n",
      "Requirements: /Users/tracilim/Projects/aws-bedrock-prompt-optimization-workshop/03-developer-journey/requirements-for-agentcore.txt\n",
      "Configuring agent: customer_support_v4_routing\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "üí° <span style=\"color: #008080; text-decoration-color: #008080\">No container engine found </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080\">Docker/Finch/Podman not installed</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "üí° \u001b[36mNo container engine found \u001b[0m\u001b[1;36m(\u001b[0m\u001b[36mDocker/Finch/Podman not installed\u001b[0m\u001b[1;36m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">‚úì</span> Default deployment uses CodeBuild <span style=\"font-weight: bold\">(</span>no container engine needed<span style=\"font-weight: bold\">)</span>, For local builds, install Docker, Finch, or \n",
       "Podman\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m‚úì\u001b[0m Default deployment uses CodeBuild \u001b[1m(\u001b[0mno container engine needed\u001b[1m)\u001b[0m, For local builds, install Docker, Finch, or \n",
       "Podman\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Memory disabled\n",
      "Network mode: PUBLIC\n",
      "Generated Dockerfile: Dockerfile\n",
      "Generated .dockerignore: /Users/tracilim/Projects/aws-bedrock-prompt-optimization-workshop/03-developer-journey/.dockerignore\n",
      "Keeping 'customer_support_v4_routing' as default agent\n",
      "Bedrock AgentCore configured: /Users/tracilim/Projects/aws-bedrock-prompt-optimization-workshop/03-developer-journey/.bedrock_agentcore.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConfigureResult(config_path=PosixPath('/Users/tracilim/Projects/aws-bedrock-prompt-optimization-workshop/03-developer-journey/.bedrock_agentcore.yaml'), dockerfile_path=PosixPath('/Users/tracilim/Projects/aws-bedrock-prompt-optimization-workshop/03-developer-journey/Dockerfile'), dockerignore_path=PosixPath('/Users/tracilim/Projects/aws-bedrock-prompt-optimization-workshop/03-developer-journey/.dockerignore'), runtime='None', runtime_type=None, region='us-east-1', account_id='739907928487', execution_role=None, ecr_repository=None, auto_create_ecr=True, s3_path=None, auto_create_s3=False, memory_id=None, network_mode='PUBLIC', network_subnets=None, network_security_groups=None, network_vpc_id=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_name = \"customer_support_v4_routing\"\n",
    "agent_file = str(Path(\"agents/v4_routing.py\").absolute())\n",
    "requirements_file = str(Path(\"requirements-for-agentcore.txt\").absolute())\n",
    "\n",
    "print(f\"Agent name: {agent_name}\")\n",
    "print(f\"Agent file: {agent_file}\")\n",
    "print(f\"Requirements: {requirements_file}\")\n",
    "\n",
    "print(f\"Configuring agent: {agent_name}\")\n",
    "agentcore_runtime.configure(\n",
    "    entrypoint=agent_file,\n",
    "    auto_create_execution_role=True,\n",
    "    auto_create_ecr=True,\n",
    "    requirements_file=requirements_file,\n",
    "    region=region,\n",
    "    agent_name=agent_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dockerfile modified for Langfuse\n"
     ]
    }
   ],
   "source": [
    "# Modify Dockerfile for Langfuse\n",
    "dockerfile_path = Path(\"Dockerfile\")\n",
    "if dockerfile_path.exists():\n",
    "    content = dockerfile_path.read_text()\n",
    "    if \"opentelemetry-instrument\" in content:\n",
    "        import re\n",
    "        content = re.sub(\n",
    "            r'CMD \\[\"opentelemetry-instrument\", \"python\", \"-m\", \"([^\"]+)\"\\]',\n",
    "            r'CMD [\"python\", \"-m\", \"\\1\"]',\n",
    "            content\n",
    "        )\n",
    "        dockerfile_path.write_text(content)\n",
    "        print(\"Dockerfile modified for Langfuse\")\n",
    "    else:\n",
    "        print(\"Dockerfile already configured or using different format\")\n",
    "else:\n",
    "    print(\"Dockerfile not found - will be created during deployment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üöÄ Launching Bedrock AgentCore (cloud mode - RECOMMENDED)...\n",
      "   ‚Ä¢ Deploy Python code directly to runtime\n",
      "   ‚Ä¢ No Docker required (DEFAULT behavior)\n",
      "   ‚Ä¢ Production-ready deployment\n",
      "\n",
      "üí° Deployment options:\n",
      "   ‚Ä¢ runtime.launch()                ‚Üí Cloud (current)\n",
      "   ‚Ä¢ runtime.launch(local=True)      ‚Üí Local development\n",
      "Memory disabled - skipping memory creation\n",
      "Starting CodeBuild ARM64 deployment for agent 'customer_support_v4_routing' to account 739907928487 (us-east-1)\n",
      "Setting up AWS resources (ECR repository, execution roles)...\n",
      "Getting or creating ECR repository for agent: customer_support_v4_routing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying to AgentCore Runtime...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ECR repository available: 739907928487.dkr.ecr.us-east-1.amazonaws.com/bedrock-agentcore-customer_support_v4_routing\n",
      "Getting or creating execution role for agent: customer_support_v4_routing\n",
      "Using AWS region: us-east-1, account ID: 739907928487\n",
      "Role name: AmazonBedrockAgentCoreSDKRuntime-us-east-1-0fb396fd48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Reusing existing ECR repository: 739907928487.dkr.ecr.us-east-1.amazonaws.com/bedrock-agentcore-customer_support_v4_routing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚úÖ Reusing existing execution role: arn:aws:iam::739907928487:role/AmazonBedrockAgentCoreSDKRuntime-us-east-1-0fb396fd48\n",
      "Execution role available: arn:aws:iam::739907928487:role/AmazonBedrockAgentCoreSDKRuntime-us-east-1-0fb396fd48\n",
      "Preparing CodeBuild project and uploading source...\n",
      "Getting or creating CodeBuild execution role for agent: customer_support_v4_routing\n",
      "Role name: AmazonBedrockAgentCoreSDKCodeBuild-us-east-1-0fb396fd48\n",
      "Reusing existing CodeBuild execution role: arn:aws:iam::739907928487:role/AmazonBedrockAgentCoreSDKCodeBuild-us-east-1-0fb396fd48\n",
      "Using dockerignore.template with 46 patterns for zip filtering\n",
      "Uploaded source to S3: customer_support_v4_routing/source.zip\n",
      "Updated CodeBuild project: bedrock-agentcore-customer_support_v4_routing-builder\n",
      "Starting CodeBuild build (this may take several minutes)...\n",
      "Starting CodeBuild monitoring...\n",
      "üîÑ QUEUED started (total: 0s)\n",
      "‚úÖ QUEUED completed in 1.3s\n",
      "üîÑ PROVISIONING started (total: 2s)\n",
      "‚úÖ PROVISIONING completed in 7.7s\n",
      "üîÑ DOWNLOAD_SOURCE started (total: 9s)\n",
      "‚úÖ DOWNLOAD_SOURCE completed in 2.6s\n",
      "üîÑ BUILD started (total: 12s)\n",
      "‚úÖ BUILD completed in 18.0s\n",
      "üîÑ POST_BUILD started (total: 30s)\n",
      "‚úÖ POST_BUILD completed in 15.4s\n",
      "üîÑ COMPLETED started (total: 45s)\n",
      "‚úÖ COMPLETED completed in 1.3s\n",
      "üéâ CodeBuild completed successfully in 0m 46s\n",
      "CodeBuild completed successfully\n",
      "CodeBuild project configuration saved\n",
      "Deploying to Bedrock AgentCore...\n",
      "Agent created/updated: arn:aws:bedrock-agentcore:us-east-1:739907928487:runtime/customer_support_v4_routing-3LNaMtHxM3\n",
      "Observability is enabled, configuring observability components...\n",
      "CloudWatch Logs resource policy already configured\n",
      "X-Ray trace destination already configured\n",
      "X-Ray indexing rule already configured\n",
      "Transaction Search already fully configured\n",
      "ObservabilityDeliveryManager initialized for region: us-east-1, account: 739907928487\n",
      "‚úÖ Logs auto-created by AWS for runtime/customer_support_v4_routing-3LNaMtHxM3\n",
      "‚úÖ Traces delivery enabled for runtime/customer_support_v4_routing-3LNaMtHxM3\n",
      "Observability enabled for runtime/customer_support_v4_routing-3LNaMtHxM3 - logs: True, traces: True\n",
      "‚úÖ X-Ray traces delivery enabled for agent customer_support_v4_routing-3LNaMtHxM3\n",
      "üîç GenAI Observability Dashboard:\n",
      "   https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#gen-ai-observability/agent-core\n",
      "Polling for endpoint to be ready...\n",
      "Agent endpoint: arn:aws:bedrock-agentcore:us-east-1:739907928487:runtime/customer_support_v4_routing-3LNaMtHxM3/runtime-endpoint/DEFAULT\n",
      "Deployment completed successfully - Agent: arn:aws:bedrock-agentcore:us-east-1:739907928487:runtime/customer_support_v4_routing-3LNaMtHxM3\n",
      "Built with CodeBuild: bedrock-agentcore-customer_support_v4_routing-builder:7e7227b2-9ff1-4529-805c-38cbd06b52e1\n",
      "Deployed to cloud: arn:aws:bedrock-agentcore:us-east-1:739907928487:runtime/customer_support_v4_routing-3LNaMtHxM3\n",
      "ECR image: 739907928487.dkr.ecr.us-east-1.amazonaws.com/bedrock-agentcore-customer_support_v4_routing\n",
      "üîç Agent logs available at:\n",
      "   /aws/bedrock-agentcore/runtimes/customer_support_v4_routing-3LNaMtHxM3-DEFAULT --log-stream-name-prefix \"2026/01/13/\\[runtime-logs]\"\n",
      "   /aws/bedrock-agentcore/runtimes/customer_support_v4_routing-3LNaMtHxM3-DEFAULT --log-stream-names \"otel-rt-logs\"\n",
      "üí° Tail logs with: aws logs tail /aws/bedrock-agentcore/runtimes/customer_support_v4_routing-3LNaMtHxM3-DEFAULT --log-stream-name-prefix \"2026/01/13/\\[runtime-logs]\" --follow\n",
      "üí° Or view recent logs: aws logs tail /aws/bedrock-agentcore/runtimes/customer_support_v4_routing-3LNaMtHxM3-DEFAULT --log-stream-name-prefix \"2026/01/13/\\[runtime-logs]\" --since 1h\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent deployed: arn:aws:bedrock-agentcore:us-east-1:739907928487:runtime/customer_support_v4_routing-3LNaMtHxM3\n"
     ]
    }
   ],
   "source": [
    "env_vars = {\n",
    "    \"LANGFUSE_HOST\": os.environ.get(\"LANGFUSE_HOST\"),\n",
    "    \"LANGFUSE_PUBLIC_KEY\": os.environ.get(\"LANGFUSE_PUBLIC_KEY\"),\n",
    "    \"LANGFUSE_SECRET_KEY\": os.environ.get(\"LANGFUSE_SECRET_KEY\"),\n",
    "    \"PYTHONUNBUFFERED\": \"1\",\n",
    "}\n",
    "\n",
    "print(\"Deploying to AgentCore Runtime...\")\n",
    "launch_result = agentcore_runtime.launch(env_vars=env_vars, auto_update_on_conflict=True)\n",
    "agent_arn = launch_result.agent_arn\n",
    "print(f\"Agent deployed: {agent_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6b26258",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_arn = \"arn:aws:bedrock-agentcore:us-east-1:739907928487:runtime/customer_support_v4_routing-3LNaMtHxM3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Step 5: Test Model Routing\n",
    "\n",
    "Let's run the same test prompts and observe which model handles each query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_agent(prompt):\n",
    "    \"\"\"Invoke the agent via AgentCore API.\"\"\"\n",
    "    response = data_client.invoke_agent_runtime(\n",
    "        agentRuntimeArn=agent_arn,\n",
    "        runtimeSessionId=str(uuid.uuid4()),\n",
    "        payload=json.dumps({\"prompt\": prompt}).encode(),\n",
    "    )\n",
    "    return json.loads(response[\"response\"].read().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.langfuse_metrics import (\n",
    "    get_latest_trace_metrics,\n",
    "    print_metrics,\n",
    "    clear_metrics,\n",
    "    collect_metric,\n",
    "    print_metrics_table,\n",
    "    get_collected_metrics\n",
    ")\n",
    "\n",
    "clear_metrics()\n",
    "\n",
    "TEST_PROMPTS = [\n",
    "    (\"Return Policy\", \"What is your return policy for laptops?\"),\n",
    "    (\"Product Info\", \"Tell me about your smartphone options\"),\n",
    "    (\"Technical Support\", \"My laptop won't turn on, can you help me troubleshoot?\"),\n",
    "    (\"Multi-part Question\", \"I want to buy a laptop. What are the specs and what's the return policy?\"),\n",
    "    (\"General Question\", \"Hello! What can you help me with today?\"),\n",
    "]\n",
    "\n",
    "for test_name, prompt in TEST_PROMPTS:\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Test: {test_name}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    result = invoke_agent(prompt)\n",
    "\n",
    "    if isinstance(result, dict):\n",
    "        print(f\"Model used: {result.get('model_used', 'N/A')}\")\n",
    "        print(f\"Complexity: {result.get('complexity', 'N/A')}\")\n",
    "        print(f\"Response: {str(result.get('response', result))[:200]}...\")\n",
    "    else:\n",
    "        print(result)\n",
    "\n",
    "    # Get metrics from parent trace (includes classifier + main agent)\n",
    "    metrics = get_latest_trace_metrics(\n",
    "        agent_name=\"customer-support-v4-routing\",\n",
    "        wait_seconds=5,\n",
    "        max_retries=5,\n",
    "        timeout_seconds=120,\n",
    "    )\n",
    "    print_metrics(metrics, test_name)\n",
    "    collect_metric(metrics, test_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================================================================================================\n",
      "                                  METRICS SUMMARY\n",
      "=========================================================================================================\n",
      "               Test Latency    Cost Input Output Cache Read Tokens Cache Write Tokens\n",
      "      Return Policy   3.89s $0.0060 4,349    331                 0                  0\n",
      "       Product Info   3.86s $0.0062 4,396    360                 0                  0\n",
      "  Technical Support   9.35s $0.0095   917    418             1,743              1,743\n",
      "Multi-part Question   9.09s $0.0123 1,207    510             3,486                  0\n",
      "   General Question   1.78s $0.0028 2,075    149                 0                  0\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "  TOTALS: Latency(avg): 5.59s | Cost: $0.0369 | Input: 12,944 | Output: 1,768\n",
      "          Cache Read Tokens: 5,229 | Cache Write Tokens: 1,743\n",
      "=========================================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test</th>\n",
       "      <th>Latency</th>\n",
       "      <th>Cost</th>\n",
       "      <th>Input</th>\n",
       "      <th>Output</th>\n",
       "      <th>Cache Read Tokens</th>\n",
       "      <th>Cache Write Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Return Policy</td>\n",
       "      <td>3.89s</td>\n",
       "      <td>$0.0060</td>\n",
       "      <td>4,349</td>\n",
       "      <td>331</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Product Info</td>\n",
       "      <td>3.86s</td>\n",
       "      <td>$0.0062</td>\n",
       "      <td>4,396</td>\n",
       "      <td>360</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Technical Support</td>\n",
       "      <td>9.35s</td>\n",
       "      <td>$0.0095</td>\n",
       "      <td>917</td>\n",
       "      <td>418</td>\n",
       "      <td>1,743</td>\n",
       "      <td>1,743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Multi-part Question</td>\n",
       "      <td>9.09s</td>\n",
       "      <td>$0.0123</td>\n",
       "      <td>1,207</td>\n",
       "      <td>510</td>\n",
       "      <td>3,486</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>General Question</td>\n",
       "      <td>1.78s</td>\n",
       "      <td>$0.0028</td>\n",
       "      <td>2,075</td>\n",
       "      <td>149</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Test Latency     Cost  Input Output Cache Read Tokens  \\\n",
       "0        Return Policy   3.89s  $0.0060  4,349    331                 0   \n",
       "1         Product Info   3.86s  $0.0062  4,396    360                 0   \n",
       "2    Technical Support   9.35s  $0.0095    917    418             1,743   \n",
       "3  Multi-part Question   9.09s  $0.0123  1,207    510             3,486   \n",
       "4     General Question   1.78s  $0.0028  2,075    149                 0   \n",
       "\n",
       "  Cache Write Tokens  \n",
       "0                  0  \n",
       "1                  0  \n",
       "2              1,743  \n",
       "3                  0  \n",
       "4                  0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_metrics_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "### Expected Routing Results\n",
    "\n",
    "**Routed to Haiku (3 queries):**\n",
    "- Return Policy ‚Äî Single factual lookup\n",
    "- Product Info ‚Äî Direct product question  \n",
    "- General Question ‚Äî Simple greeting\n",
    "\n",
    "**Routed to Sonnet (2 queries):**\n",
    "- Technical Support ‚Äî Multi-step troubleshooting\n",
    "- Multi-part Question ‚Äî Multiple questions requiring reasoning\n",
    "\n",
    "**Result:** 60% of queries routed to the cheaper model.\n",
    "\n",
    "## Step 6: Compare with v3 (All Sonnet)\n",
    "\n",
    "Enter your metrics from Lab 03 (v3 caching) to compare with v4 routing results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d822eee9",
   "metadata": {},
   "source": [
    "## Step 6: Compare with v3 (Caching)\n",
    "\n",
    "Enter your metrics from Lab 03 (v3 caching) to compare with v4 routing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "k1bezj7akz9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "  V3 (CACHING) vs V4 (ROUTING) COMPARISON\n",
      "======================================================================\n",
      "Metric                     v3 (Caching)       v4 (Routing)       Change\n",
      "----------------------------------------------------------------------\n",
      "Total Cost           $           0.0438 $           0.0369       -15.8%\n",
      "Avg Latency (s)                    8.10               5.59       -30.9%\n",
      "Input Tokens                      4,228             12,944      +206.1%\n",
      "Output Tokens                     1,795              1,768        -1.5%\n",
      "======================================================================\n",
      "\n",
      "Routing: 15.8% cost reduction, 30.9% latency change\n",
      "Note: Latency may increase due to classification overhead, but cost savings are significant.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# INPUT YOUR V3 METRICS FROM LAB 03 HERE\n",
    "# (Copy the totals from your v3 metrics table)\n",
    "# ============================================================\n",
    "v3_total_cost = 0.0438       # e.g., 0.0430\n",
    "v3_avg_latency = 8.10        # e.g., 7.55 (seconds)\n",
    "v3_total_input_tokens = 4228    # e.g., 4229\n",
    "v3_total_output_tokens = 1795   # e.g., 1739\n",
    "\n",
    "# ============================================================\n",
    "# V4 metrics (calculated from above)\n",
    "# ============================================================\n",
    "v4_metrics = get_collected_metrics()\n",
    "v4_total_cost = sum(m.get('cost_usd', 0) for m in v4_metrics if 'error' not in m)\n",
    "v4_latencies = [m.get('latency_seconds', 0) or 0 for m in v4_metrics if 'error' not in m]\n",
    "v4_avg_latency = sum(v4_latencies) / len(v4_latencies) if v4_latencies else 0\n",
    "v4_total_input_tokens = sum(m.get('input_tokens', 0) for m in v4_metrics if 'error' not in m)\n",
    "v4_total_output_tokens = sum(m.get('output_tokens', 0) for m in v4_metrics if 'error' not in m)\n",
    "\n",
    "# ============================================================\n",
    "# Comparison\n",
    "# ============================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"  V3 (CACHING) vs V4 (ROUTING) COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Metric':<20} {'v3 (Caching)':>18} {'v4 (Routing)':>18} {'Change':>12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "has_v3_metrics = v3_total_cost > 0 and v3_avg_latency > 0\n",
    "\n",
    "# Cost comparison\n",
    "if v3_total_cost > 0:\n",
    "    cost_change = ((v4_total_cost - v3_total_cost) / v3_total_cost) * 100\n",
    "    cost_str = f\"{cost_change:+.1f}%\"\n",
    "else:\n",
    "    cost_str = \"N/A\"\n",
    "print(f\"{'Total Cost':<20} ${v3_total_cost:>17.4f} ${v4_total_cost:>17.4f} {cost_str:>12}\")\n",
    "\n",
    "# Latency comparison\n",
    "if v3_avg_latency > 0:\n",
    "    latency_change = ((v4_avg_latency - v3_avg_latency) / v3_avg_latency) * 100\n",
    "    latency_str = f\"{latency_change:+.1f}%\"\n",
    "else:\n",
    "    latency_str = \"N/A\"\n",
    "print(f\"{'Avg Latency (s)':<20} {v3_avg_latency:>18.2f} {v4_avg_latency:>18.2f} {latency_str:>12}\")\n",
    "\n",
    "# Input tokens comparison\n",
    "if v3_total_input_tokens > 0:\n",
    "    input_change = ((v4_total_input_tokens - v3_total_input_tokens) / v3_total_input_tokens) * 100\n",
    "    input_str = f\"{input_change:+.1f}%\"\n",
    "else:\n",
    "    input_str = \"N/A\"\n",
    "print(f\"{'Input Tokens':<20} {v3_total_input_tokens:>18,} {v4_total_input_tokens:>18,} {input_str:>12}\")\n",
    "\n",
    "# Output tokens comparison\n",
    "if v3_total_output_tokens > 0:\n",
    "    output_change = ((v4_total_output_tokens - v3_total_output_tokens) / v3_total_output_tokens) * 100\n",
    "    output_str = f\"{output_change:+.1f}%\"\n",
    "else:\n",
    "    output_str = \"N/A\"\n",
    "print(f\"{'Output Tokens':<20} {v3_total_output_tokens:>18,} {v4_total_output_tokens:>18,} {output_str:>12}\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "if has_v3_metrics:\n",
    "    print(f\"\\nRouting: {-cost_change:.1f}% cost reduction, {-latency_change:.1f}% latency change\")\n",
    "    print(\"Note: Latency may increase due to classification overhead, but cost savings are significant.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Enter your v3 metrics above to see the comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": "## Summary\n\nIn this notebook, we implemented intelligent model routing:\n\n1. **LLM-based classification** ‚Äî Haiku classifies queries as \"simple\" or \"complex\" using a single-word response\n2. **Cost-effective routing** ‚Äî Simple queries go to Haiku (~4x cheaper), complex to Sonnet\n3. **Prompt caching for Sonnet only** ‚Äî Haiku requires 4,096 tokens minimum (our prompt is ~1,030), so only Sonnet requests benefit from caching\n\n**Key insights:**\n\n- **LLM classification beats keyword matching** ‚Äî Handles semantic variations and edge cases\n- **Simple text parsing is reliable** ‚Äî Few-shot examples in the prompt ensure consistent \"simple\" or \"complex\" responses\n- **Real cost savings** ‚Äî Compare the v3 vs v4 metrics above to see actual savings from routing\n\n**Next:** In Lab 05, we'll add Bedrock Guardrails to filter off-topic queries before they reach the LLM."
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cleanup\n",
    "\n",
    "To delete the agent deployed in this notebook, uncomment and run the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Delete the agent\n",
    "# control_client.delete_agent_runtime(agentRuntimeId=agent_arn.split(\"/\")[-1])\n",
    "# print(f\"Agent deleted: {agent_arn}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "developer-journey (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}