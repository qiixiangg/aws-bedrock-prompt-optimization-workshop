{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 04: LLM Routing\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we implement **model routing** to use cheaper models for simple queries while preserving quality for complex ones. This can dramatically reduce costs without sacrificing user experience.\n",
    "\n",
    "**What you'll learn:**\n",
    "- How to classify query complexity\n",
    "- How to route queries to appropriate models\n",
    "- How to verify routing decisions in Langfuse\n",
    "- Cost savings from intelligent routing\n",
    "\n",
    "**Routing Strategy:**\n",
    "- Simple queries → Claude Haiku ($0.25/1M input, 12x cheaper)\n",
    "- Complex queries → Claude Sonnet ($3.00/1M input)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed Labs 01-03\n",
    "\n",
    "## Workshop Journey\n",
    "\n",
    "```\n",
    "01 Baseline → 02 Quick Wins → 03 Caching → [04 Routing] → 05 Guardrails → 06 Gateway → 07 Evaluations\n",
    "                                               ↑\n",
    "                                          You are here\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport json\nimport uuid\nfrom pathlib import Path\nfrom dotenv import load_dotenv\n\nload_dotenv(override=True)\n\nimport boto3\nfrom bedrock_agentcore_starter_toolkit import Runtime\n\nregion = os.environ.get(\"AWS_DEFAULT_REGION\", \"us-east-1\")\ncontrol_client = boto3.client(\"bedrock-agentcore-control\", region_name=region)\ndata_client = boto3.client(\"bedrock-agentcore\", region_name=region)\nagentcore_runtime = Runtime()\n\nprint(f\"Region: {region}\")\nprint(f\"Langfuse Host: {os.environ.get('LANGFUSE_HOST', 'Not set')}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 2: Understanding Model Routing\n\n### Model Pricing Comparison\n\n| Model | Input (per 1M) | Output (per 1M) | Best For |\n|-------|----------------|-----------------|----------|\n| Claude Sonnet | $3.00 | $15.00 | Complex reasoning, analysis |\n| Claude Haiku | $0.25 | $1.25 | Simple Q&A, lookups |\n\n**Haiku is 12x cheaper for input tokens!**\n\n### Routing Logic\n\nSimple queries (use Haiku):\n- \"What is your return policy?\"\n- \"Hello, what can you help me with?\"\n- \"Do you have X product?\"\n\nComplex queries (use Sonnet):\n- \"Compare these products and recommend...\"\n- \"Troubleshoot my device that...\"\n- Multi-step reasoning required"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the routing logic in v4 agent\n",
    "agent_file = Path(\"agents/v4_routing.py\")\n",
    "print(agent_file.read_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Deploy the Routing Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "agent_name = \"customer_support_v4_routing\"\nagent_file = str(Path(\"agents/v4_routing.py\").absolute())\nrequirements_file = str(Path(\"requirements-for-agentcore.txt\").absolute())\n\nprint(f\"Agent name: {agent_name}\")\nprint(f\"Agent file: {agent_file}\")\nprint(f\"Requirements: {requirements_file}\")\n\nprint(f\"Configuring agent: {agent_name}\")\nagentcore_runtime.configure(\n    entrypoint=agent_file,\n    auto_create_execution_role=True,\n    auto_create_ecr=True,\n    requirements_file=requirements_file,\n    region=region,\n    agent_name=agent_name,\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Modify Dockerfile for Langfuse\ndockerfile_path = Path(\"Dockerfile\")\nif dockerfile_path.exists():\n    content = dockerfile_path.read_text()\n    if \"opentelemetry-instrument\" in content:\n        import re\n        content = re.sub(\n            r'CMD \\[\"opentelemetry-instrument\", \"python\", \"-m\", \"([^\"]+)\"\\]',\n            r'CMD [\"python\", \"-m\", \"\\1\"]',\n            content\n        )\n        dockerfile_path.write_text(content)\n        print(\"Dockerfile modified for Langfuse\")\n    else:\n        print(\"Dockerfile already configured or using different format\")\nelse:\n    print(\"Dockerfile not found - will be created during deployment\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_vars = {\n",
    "    \"LANGFUSE_HOST\": os.environ.get(\"LANGFUSE_HOST\"),\n",
    "    \"LANGFUSE_PUBLIC_KEY\": os.environ.get(\"LANGFUSE_PUBLIC_KEY\"),\n",
    "    \"LANGFUSE_SECRET_KEY\": os.environ.get(\"LANGFUSE_SECRET_KEY\"),\n",
    "    \"PYTHONUNBUFFERED\": \"1\",\n",
    "}\n",
    "\n",
    "print(\"Deploying to AgentCore Runtime...\")\n",
    "launch_result = agentcore_runtime.launch(env_vars=env_vars, auto_update_on_conflict=True)\n",
    "agent_arn = launch_result.agent_arn\n",
    "print(f\"Agent deployed: {agent_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test Model Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_agent(prompt):\n",
    "    \"\"\"Invoke the agent via AgentCore API.\"\"\"\n",
    "    response = data_client.invoke_agent_runtime(\n",
    "        agentRuntimeArn=agent_arn,\n",
    "        runtimeSessionId=str(uuid.uuid4()),\n",
    "        payload=json.dumps({\"prompt\": prompt}).encode(),\n",
    "    )\n",
    "    return json.loads(response[\"response\"].read().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import Langfuse metrics helper\nfrom utils.langfuse_metrics import (\n    get_latest_trace_metrics,\n    print_metrics,\n    clear_metrics,\n    collect_metric,\n    print_metrics_table,\n    get_collected_metrics\n)\n\n# Clear any previously collected metrics\nclear_metrics()\n\n# Standard test prompts - same across all notebooks for consistent comparison\nTEST_PROMPTS = [\n    # Single tool: get_return_policy\n    (\"Return Policy\", \"What is your return policy for laptops?\"),\n\n    # Single tool: get_product_info\n    (\"Product Info\", \"Tell me about your smartphone options\"),\n\n    # Single tool: get_technical_support (Bedrock KB)\n    (\"Technical Support\", \"My laptop won't turn on, can you help me troubleshoot?\"),\n\n    # Multi-tool: get_product_info + get_return_policy\n    (\"Multi-part Question\", \"I want to buy a laptop. What are the specs and what's the return policy?\"),\n\n    # No tool: General greeting\n    (\"General Question\", \"Hello! What can you help me with today?\"),\n]\n\n# Run all tests and collect metrics\nfor test_name, prompt in TEST_PROMPTS:\n    print(\"=\" * 60)\n    print(f\"Test: {test_name}\")\n    print(\"=\" * 60)\n\n    result = invoke_agent(prompt)\n    \n    # Show routing decision if available\n    if isinstance(result, dict):\n        print(f\"Model used: {result.get('model_used', 'N/A')}\")\n        print(f\"Complexity: {result.get('complexity', 'N/A')}\")\n        print(f\"Response: {str(result.get('response', result))[:200]}...\")\n    else:\n        print(result)\n\n    # Fetch and collect metrics\n    metrics = get_latest_trace_metrics(\n        agent_name=\"customer-support-v4-routing\",\n        wait_seconds=5,\n        max_retries=5,\n        timeout_seconds=120,\n    )\n    print_metrics(metrics, test_name)\n    collect_metric(metrics, test_name)"
  },
  {
   "cell_type": "code",
   "source": "# Print summary table\nprint_metrics_table()\n\n# Compare with baseline metrics (from notebook 01)\nBASELINE_AVG_INPUT_TOKENS = 4251  # From v1-baseline\nBASELINE_AVG_LATENCY = 8.0  # From v1-baseline (seconds)\n\n# Calculate improvements\ncollected = get_collected_metrics()\nif collected:\n    valid_metrics = [m for m in collected if \"error\" not in m]\n    if valid_metrics:\n        avg_input = sum(m.get('input_tokens', 0) for m in valid_metrics) / len(valid_metrics)\n        avg_latency = sum(m.get('latency_seconds', 0) or 0 for m in valid_metrics) / len(valid_metrics)\n        total_cost = sum(m.get('cost_usd', 0) for m in valid_metrics)\n        total_cache_read = sum(m.get('cache_read_tokens', 0) for m in valid_metrics)\n\n        token_reduction = ((BASELINE_AVG_INPUT_TOKENS - avg_input) / BASELINE_AVG_INPUT_TOKENS) * 100\n        latency_change = ((BASELINE_AVG_LATENCY - avg_latency) / BASELINE_AVG_LATENCY) * 100\n\n        print(\"\\n\" + \"=\" * 60)\n        print(\"           COMPARISON VS BASELINE (v1)\")\n        print(\"=\" * 60)\n        print(f\"  Avg Input Tokens:  {avg_input:,.0f} (Baseline: {BASELINE_AVG_INPUT_TOKENS:,})\")\n        print(f\"  Token Reduction:   {token_reduction:+.1f}%\")\n        print(f\"  Avg Latency:       {avg_latency:.2f}s (Baseline: {BASELINE_AVG_LATENCY:.2f}s)\")\n        print(f\"  Latency Change:    {latency_change:+.1f}%\")\n        print(f\"  Total Cost:        ${total_cost:.4f}\")\n        print(f\"  Cache Read Tokens: {total_cache_read:,}\")\n        print(\"=\" * 60)\n        print(\"\\nNote: With routing, simple queries use Haiku (12x cheaper)!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Verify Routing in Langfuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langfuse_host = os.environ.get(\"LANGFUSE_HOST\", \"https://cloud.langfuse.com\")\n",
    "print(f\"View your traces at: {langfuse_host}\")\n",
    "print(\"\\nFilter by tags: 'routing', 'simple' or 'complex'\")\n",
    "print(\"\\nCheck the 'query_complexity' and 'model_used' attributes in each trace.\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Expected Routing:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\")\n",
    "print(\"| Query Type | Model | Cost per 1M input |\")\n",
    "print(\"|------------|-------|-------------------|\")\n",
    "print(\"| Simple     | Haiku | $0.25             |\")\n",
    "print(\"| Complex    | Sonnet| $3.00             |\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Calculate Routing Savings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_routing_savings(total_requests, simple_pct, tokens_per_request):\n",
    "    \"\"\"Calculate savings from model routing.\"\"\"\n",
    "    sonnet_input = 3.00  # per 1M\n",
    "    haiku_input = 0.25   # per 1M\n",
    "    \n",
    "    simple_requests = total_requests * (simple_pct / 100)\n",
    "    complex_requests = total_requests * (1 - simple_pct / 100)\n",
    "    \n",
    "    # Without routing (all Sonnet)\n",
    "    cost_no_routing = (tokens_per_request * total_requests / 1_000_000) * sonnet_input\n",
    "    \n",
    "    # With routing\n",
    "    cost_simple = (tokens_per_request * simple_requests / 1_000_000) * haiku_input\n",
    "    cost_complex = (tokens_per_request * complex_requests / 1_000_000) * sonnet_input\n",
    "    cost_with_routing = cost_simple + cost_complex\n",
    "    \n",
    "    savings = cost_no_routing - cost_with_routing\n",
    "    savings_pct = (savings / cost_no_routing) * 100\n",
    "    \n",
    "    return {\n",
    "        \"without_routing\": cost_no_routing,\n",
    "        \"with_routing\": cost_with_routing,\n",
    "        \"savings\": savings,\n",
    "        \"savings_pct\": savings_pct,\n",
    "    }\n",
    "\n",
    "# Example: 70% simple queries\n",
    "result = calculate_routing_savings(\n",
    "    total_requests=1000,\n",
    "    simple_pct=70,\n",
    "    tokens_per_request=1000\n",
    ")\n",
    "\n",
    "print(\"Routing Savings (1000 requests, 70% simple, 1000 tokens each):\")\n",
    "print(f\"  Without routing (all Sonnet): ${result['without_routing']:.4f}\")\n",
    "print(f\"  With routing:                 ${result['with_routing']:.4f}\")\n",
    "print(f\"  Savings:                      ${result['savings']:.4f} ({result['savings_pct']:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we implemented model routing:\n",
    "\n",
    "1. **Query classification**: Pattern-based complexity detection\n",
    "2. **Model selection**: Haiku for simple, Sonnet for complex\n",
    "3. **Significant cost savings**: Up to 60%+ with typical query distributions\n",
    "\n",
    "**Key Insight:** Most customer support queries are simple lookups that don't need the full power of Sonnet.\n",
    "\n",
    "**Next Steps:** In the next notebook, we'll add Bedrock Guardrails to filter out off-topic queries before they reach the LLM.\n",
    "\n",
    "**Next notebook:** [05-guardrails.ipynb](./05-guardrails.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}