{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 03: Prompt Caching\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we enable **prompt caching** to reduce costs on repeated requests. Prompt caching stores the processed system prompt and tool definitions, so subsequent requests can reuse them at a 90% discount.\n",
    "\n",
    "**What you'll learn:**\n",
    "- How to configure system prompt caching\n",
    "- How to enable tool definition caching\n",
    "- How to verify cache hits in Langfuse\n",
    "- How to calculate cost savings from caching\n",
    "\n",
    "**Optimizations in this notebook:**\n",
    "- `SystemContentBlock` with `cachePoint`\n",
    "- `cache_tools=\"default\"` on BedrockModel\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed Labs 01-02\n",
    "\n",
    "## Workshop Journey\n",
    "\n",
    "```\n",
    "01 Baseline → 02 Quick Wins → [03 Caching] → 04 Routing → 05 Guardrails → 06 Gateway → 07 Evaluations\n",
    "                                   ↑\n",
    "                              You are here\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport json\nimport uuid\nfrom pathlib import Path\nfrom dotenv import load_dotenv\n\nload_dotenv(override=True)\n\nimport boto3\nfrom bedrock_agentcore_starter_toolkit import Runtime\n\nregion = os.environ.get(\"AWS_DEFAULT_REGION\", \"us-east-1\")\ncontrol_client = boto3.client(\"bedrock-agentcore-control\", region_name=region)\ndata_client = boto3.client(\"bedrock-agentcore\", region_name=region)\nagentcore_runtime = Runtime()\n\nprint(f\"Region: {region}\")\nprint(f\"Langfuse Host: {os.environ.get('LANGFUSE_HOST', 'Not set')}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Understanding Prompt Caching\n",
    "\n",
    "### How Prompt Caching Works\n",
    "\n",
    "1. **First request**: System prompt and tools are processed and cached\n",
    "   - You pay full price + 25% cache write fee\n",
    "   - Tokens appear as `cacheWriteInputTokens`\n",
    "\n",
    "2. **Subsequent requests**: Cached content is reused\n",
    "   - You pay only 10% of the normal input token price\n",
    "   - Tokens appear as `cacheReadInputTokens`\n",
    "\n",
    "### Cost Savings Example\n",
    "\n",
    "For 1000 tokens of system prompt + tools:\n",
    "- **Without caching**: 1000 tokens × $3.00/1M = $0.003 per request\n",
    "- **With caching (first request)**: 1000 tokens × $3.75/1M = $0.00375\n",
    "- **With caching (subsequent)**: 1000 tokens × $0.30/1M = $0.0003\n",
    "\n",
    "After just 2 requests, caching pays for itself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the caching configuration in v3 agent\n",
    "agent_file = Path(\"agents/v3_caching.py\")\n",
    "print(agent_file.read_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Deploy the Caching Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "agent_name = \"customer_support_v3_caching\"\nagent_file = str(Path(\"agents/v3_caching.py\").absolute())\nrequirements_file = str(Path(\"requirements-for-agentcore.txt\").absolute())\n\nprint(f\"Agent name: {agent_name}\")\nprint(f\"Agent file: {agent_file}\")\nprint(f\"Requirements: {requirements_file}\")\n\nprint(f\"Configuring agent: {agent_name}\")\nagentcore_runtime.configure(\n    entrypoint=agent_file,\n    auto_create_execution_role=True,\n    auto_create_ecr=True,\n    requirements_file=requirements_file,\n    region=region,\n    agent_name=agent_name,\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Modify Dockerfile for Langfuse\ndockerfile_path = Path(\"Dockerfile\")\nif dockerfile_path.exists():\n    content = dockerfile_path.read_text()\n    # Replace opentelemetry-instrument wrapper with direct python call\n    # Keep the correct module path using regex\n    if \"opentelemetry-instrument\" in content:\n        import re\n        content = re.sub(\n            r'CMD \\[\"opentelemetry-instrument\", \"python\", \"-m\", \"([^\"]+)\"\\]',\n            r'CMD [\"python\", \"-m\", \"\\1\"]',\n            content\n        )\n        dockerfile_path.write_text(content)\n        print(\"Dockerfile modified for Langfuse\")\n    else:\n        print(\"Dockerfile already configured or using different format\")\nelse:\n    print(\"Dockerfile not found - will be created during deployment\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_vars = {\n",
    "    \"LANGFUSE_HOST\": os.environ.get(\"LANGFUSE_HOST\"),\n",
    "    \"LANGFUSE_PUBLIC_KEY\": os.environ.get(\"LANGFUSE_PUBLIC_KEY\"),\n",
    "    \"LANGFUSE_SECRET_KEY\": os.environ.get(\"LANGFUSE_SECRET_KEY\"),\n",
    "    \"PYTHONUNBUFFERED\": \"1\",\n",
    "}\n",
    "\n",
    "print(\"Deploying to AgentCore Runtime...\")\n",
    "launch_result = agentcore_runtime.launch(env_vars=env_vars, auto_update_on_conflict=True)\n",
    "agent_arn = launch_result.agent_arn\n",
    "print(f\"Agent deployed: {agent_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test Caching Behavior\n",
    "\n",
    "We'll run the same query multiple times to demonstrate cache hits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_agent(prompt):\n",
    "    \"\"\"Invoke the agent via AgentCore API.\"\"\"\n",
    "    response = data_client.invoke_agent_runtime(\n",
    "        agentRuntimeArn=agent_arn,\n",
    "        runtimeSessionId=str(uuid.uuid4()),\n",
    "        payload=json.dumps({\"prompt\": prompt}).encode(),\n",
    "    )\n",
    "    return json.loads(response[\"response\"].read().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import Langfuse metrics helper\nfrom utils.langfuse_metrics import (\n    get_latest_trace_metrics,\n    print_metrics,\n    clear_metrics,\n    collect_metric,\n    print_metrics_table,\n    get_collected_metrics\n)\n\n# Clear any previously collected metrics\nclear_metrics()\n\n# Standard test prompts - same across all notebooks for consistent comparison\nTEST_PROMPTS = [\n    # Single tool: get_return_policy\n    (\"Return Policy\", \"What is your return policy for laptops?\"),\n\n    # Single tool: get_product_info\n    (\"Product Info\", \"Tell me about your smartphone options\"),\n\n    # Single tool: get_technical_support (Bedrock KB)\n    (\"Technical Support\", \"My laptop won't turn on, can you help me troubleshoot?\"),\n\n    # Multi-tool: get_product_info + get_return_policy\n    (\"Multi-part Question\", \"I want to buy a laptop. What are the specs and what's the return policy?\"),\n\n    # No tool: General greeting\n    (\"General Question\", \"Hello! What can you help me with today?\"),\n]\n\n# Run all tests and collect metrics\n# First request should show cache WRITE, subsequent should show cache READ\nfor i, (test_name, prompt) in enumerate(TEST_PROMPTS):\n    print(\"=\" * 60)\n    print(f\"Test {i+1}: {test_name}\")\n    if i == 0:\n        print(\"(First request - Cache WRITE expected)\")\n    else:\n        print(\"(Subsequent request - Cache READ expected)\")\n    print(\"=\" * 60)\n\n    response = invoke_agent(prompt)\n    print(response)\n\n    # Fetch and collect metrics\n    metrics = get_latest_trace_metrics(\n        agent_name=\"customer-support-v3-caching\",\n        wait_seconds=5,\n        max_retries=5,\n        timeout_seconds=120,\n    )\n    print_metrics(metrics, test_name)\n    collect_metric(metrics, test_name)"
  },
  {
   "cell_type": "code",
   "source": "# Print summary table\nprint_metrics_table()\n\n# Compare with baseline metrics (from notebook 01)\nBASELINE_AVG_INPUT_TOKENS = 4251  # From v1-baseline\nBASELINE_AVG_LATENCY = 8.0  # From v1-baseline (seconds)\n\n# Calculate improvements\ncollected = get_collected_metrics()\nif collected:\n    valid_metrics = [m for m in collected if \"error\" not in m]\n    if valid_metrics:\n        avg_input = sum(m.get('input_tokens', 0) for m in valid_metrics) / len(valid_metrics)\n        avg_latency = sum(m.get('latency_seconds', 0) or 0 for m in valid_metrics) / len(valid_metrics)\n        total_cost = sum(m.get('cost_usd', 0) for m in valid_metrics)\n        total_cache_read = sum(m.get('cache_read_tokens', 0) for m in valid_metrics)\n        total_cache_write = sum(m.get('cache_write_tokens', 0) for m in valid_metrics)\n\n        token_reduction = ((BASELINE_AVG_INPUT_TOKENS - avg_input) / BASELINE_AVG_INPUT_TOKENS) * 100\n        latency_change = ((BASELINE_AVG_LATENCY - avg_latency) / BASELINE_AVG_LATENCY) * 100\n\n        print(\"\\n\" + \"=\" * 60)\n        print(\"           COMPARISON VS BASELINE (v1)\")\n        print(\"=\" * 60)\n        print(f\"  Avg Input Tokens:    {avg_input:,.0f} (Baseline: {BASELINE_AVG_INPUT_TOKENS:,})\")\n        print(f\"  Token Reduction:     {token_reduction:+.1f}%\")\n        print(f\"  Avg Latency:         {avg_latency:.2f}s (Baseline: {BASELINE_AVG_LATENCY:.2f}s)\")\n        print(f\"  Latency Change:      {latency_change:+.1f}%\")\n        print(f\"  Total Cost:          ${total_cost:.4f}\")\n        print(\"-\" * 60)\n        print(f\"  Cache Write Tokens:  {total_cache_write:,} (first request)\")\n        print(f\"  Cache Read Tokens:   {total_cache_read:,} (subsequent requests)\")\n        print(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 3: Third request - should also show cache READ\n",
    "print(\"=\" * 60)\n",
    "print(\"Run 3: Third Request (Cache Read Expected)\")\n",
    "print(\"=\" * 60)\n",
    "response = invoke_agent(\"Tell me about your tablets\")\n",
    "print(response)\n",
    "print(\"\\nCheck Langfuse: cacheReadInputTokens should be > 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Analyze Cache Metrics in Langfuse\n",
    "\n",
    "Open Langfuse and examine the token metrics for each request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Calculate Cost Savings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost calculation helper\n",
    "def calculate_caching_savings(cached_tokens, num_requests):\n",
    "    \"\"\"Calculate savings from prompt caching.\"\"\"\n",
    "    # Sonnet pricing per 1M tokens\n",
    "    input_price = 3.00\n",
    "    cache_write_price = 3.75  # 25% premium\n",
    "    cache_read_price = 0.30   # 90% discount\n",
    "    \n",
    "    # Without caching\n",
    "    cost_no_cache = (cached_tokens / 1_000_000) * input_price * num_requests\n",
    "    \n",
    "    # With caching\n",
    "    cost_first_request = (cached_tokens / 1_000_000) * cache_write_price\n",
    "    cost_subsequent = (cached_tokens / 1_000_000) * cache_read_price * (num_requests - 1)\n",
    "    cost_with_cache = cost_first_request + cost_subsequent\n",
    "    \n",
    "    savings = cost_no_cache - cost_with_cache\n",
    "    savings_pct = (savings / cost_no_cache) * 100\n",
    "    \n",
    "    return {\n",
    "        \"without_caching\": cost_no_cache,\n",
    "        \"with_caching\": cost_with_cache,\n",
    "        \"savings\": savings,\n",
    "        \"savings_pct\": savings_pct,\n",
    "    }\n",
    "\n",
    "# Example: 500 cached tokens, 10 requests\n",
    "result = calculate_caching_savings(cached_tokens=500, num_requests=10)\n",
    "print(\"Cost Comparison (500 cached tokens, 10 requests):\")\n",
    "print(f\"  Without caching: ${result['without_caching']:.6f}\")\n",
    "print(f\"  With caching:    ${result['with_caching']:.6f}\")\n",
    "print(f\"  Savings:         ${result['savings']:.6f} ({result['savings_pct']:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At scale: 1000 requests per day\n",
    "result = calculate_caching_savings(cached_tokens=500, num_requests=1000)\n",
    "print(\"Cost Comparison (500 cached tokens, 1000 requests/day):\")\n",
    "print(f\"  Without caching: ${result['without_caching']:.4f}/day\")\n",
    "print(f\"  With caching:    ${result['with_caching']:.4f}/day\")\n",
    "print(f\"  Daily savings:   ${result['savings']:.4f} ({result['savings_pct']:.1f}%)\")\n",
    "print(f\"  Monthly savings: ${result['savings'] * 30:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we enabled prompt caching:\n",
    "\n",
    "1. **System prompt caching**: Using `SystemContentBlock` with `cachePoint`\n",
    "2. **Tool caching**: Using `cache_tools=\"default\"`\n",
    "\n",
    "**Key Observations:**\n",
    "- First request: Cache write (25% premium)\n",
    "- Subsequent requests: Cache read (90% discount)\n",
    "- Break-even after just 2 requests\n",
    "\n",
    "**Next Steps:** In the next notebook, we'll add model routing to use cheaper models for simple queries.\n",
    "\n",
    "**Next notebook:** [04-llm-routing.ipynb](./04-llm-routing.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}