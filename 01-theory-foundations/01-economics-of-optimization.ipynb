{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 The Economics of Prompt Optimization\n",
    "\n",
    "**Duration**: 20 minutes\n",
    "\n",
    "**Learning Objectives**:\n",
    "- Understand why prompt optimization matters (cost, latency, scale)\n",
    "- Calculate the ROI of optimization at scale\n",
    "- Learn the paradigm shift from ad-hoc to systematic optimization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's load environment variables and import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment loaded (AWS CLI credentials will be used)\n",
      "‚ÑπÔ∏è  boto3 will automatically use AWS CLI credentials from ~/.aws/credentials\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Verify AWS credentials are loaded (without exposing sensitive values)\n",
    "# Credentials can come from .env, AWS CLI (~/.aws/credentials), or environment variables\n",
    "if os.getenv('AWS_ACCESS_KEY_ID') and os.getenv('AWS_SECRET_ACCESS_KEY'):\n",
    "    print(\"‚úÖ AWS credentials loaded successfully\")\n",
    "    print(f\"‚úÖ Region: {os.getenv('AWS_DEFAULT_REGION', 'us-east-1')}\")\n",
    "else:\n",
    "    print(\"‚úÖ Environment loaded (AWS CLI credentials will be used)\")\n",
    "    print(\"‚ÑπÔ∏è  boto3 will automatically use AWS CLI credentials from ~/.aws/credentials\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Prompt Optimization Matters\n",
    "\n",
    "### 1. Cost Efficiency\n",
    "\n",
    "Input tokens typically represent **40-60% of total inference costs**. At scale (millions of requests), small optimizations compound dramatically.\n",
    "\n",
    "**Example**: Reducing a 5,000-token prompt to 3,000 tokens = **40% cost savings** on input tokens.\n",
    "\n",
    "### 2. Latency Impact\n",
    "\n",
    "Each token adds processing time (time-to-first-token and overall latency). Optimized prompts = faster response times = better user experience.\n",
    "\n",
    "Critical for real-time applications: chatbots, customer support, interactive agents.\n",
    "\n",
    "### 3. Scale Considerations\n",
    "\n",
    "Small inefficiencies become expensive at high volumes:\n",
    "- **1,000 tokens √ó 1 million requests = 1 billion tokens processed**\n",
    "- Optimization ROI increases exponentially with usage\n",
    "\n",
    "### 4. User Experience\n",
    "\n",
    "- Faster responses improve user satisfaction\n",
    "- Better quality outputs from well-crafted prompts\n",
    "- Reduced timeout errors and failures\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Cost Calculator\n",
    "\n",
    "Let's calculate the cost impact of token optimization at scale.\n",
    "\n",
    "**Scenario**: You're building a customer support chatbot.\n",
    "\n",
    "**Assumptions** (example pricing - always check current Bedrock pricing):\n",
    "- Input tokens: $3.00 per million tokens\n",
    "- Output tokens: $15.00 per million tokens\n",
    "- Average output: 150 tokens per response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COST COMPARISON: Unoptimized vs. Optimized Prompt\n",
      "============================================================\n",
      "\n",
      "üìä Scenario: 1 million customer support requests\n",
      "\n",
      "Unoptimized (5,000 input tokens per request):\n",
      "  - Input cost:  $15,000.00\n",
      "  - Output cost: $2,250.00\n",
      "  - Total cost:  $17,250.00\n",
      "\n",
      "Optimized (3,000 input tokens per request):\n",
      "  - Input cost:  $9,000.00\n",
      "  - Output cost: $2,250.00\n",
      "  - Total cost:  $11,250.00\n",
      "\n",
      "üí∞ Savings: $6,000.00 (34.8% reduction)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def calculate_cost(input_tokens, output_tokens, num_requests, input_price_per_mtok=3.0, output_price_per_mtok=15.0):\n",
    "    \"\"\"\n",
    "    Calculate total cost for LLM inference.\n",
    "    \n",
    "    Args:\n",
    "        input_tokens: Number of input tokens per request\n",
    "        output_tokens: Number of output tokens per request\n",
    "        num_requests: Total number of requests\n",
    "        input_price_per_mtok: Price per million input tokens (default: $3.00)\n",
    "        output_price_per_mtok: Price per million output tokens (default: $15.00)\n",
    "    \n",
    "    Returns:\n",
    "        dict with cost breakdown\n",
    "    \"\"\"\n",
    "    total_input_tokens = input_tokens * num_requests\n",
    "    total_output_tokens = output_tokens * num_requests\n",
    "    \n",
    "    input_cost = (total_input_tokens / 1_000_000) * input_price_per_mtok\n",
    "    output_cost = (total_output_tokens / 1_000_000) * output_price_per_mtok\n",
    "    total_cost = input_cost + output_cost\n",
    "    \n",
    "    return {\n",
    "        'total_input_tokens': total_input_tokens,\n",
    "        'total_output_tokens': total_output_tokens,\n",
    "        'input_cost': input_cost,\n",
    "        'output_cost': output_cost,\n",
    "        'total_cost': total_cost\n",
    "    }\n",
    "\n",
    "# Scenario: Unoptimized prompt (5,000 tokens)\n",
    "unoptimized = calculate_cost(\n",
    "    input_tokens=5000,\n",
    "    output_tokens=150,\n",
    "    num_requests=1_000_000  # 1 million requests\n",
    ")\n",
    "\n",
    "# Scenario: Optimized prompt (3,000 tokens) - 40% reduction\n",
    "optimized = calculate_cost(\n",
    "    input_tokens=3000,\n",
    "    output_tokens=150,\n",
    "    num_requests=1_000_000\n",
    ")\n",
    "\n",
    "# Calculate savings\n",
    "savings = unoptimized['total_cost'] - optimized['total_cost']\n",
    "savings_percentage = (savings / unoptimized['total_cost']) * 100\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"COST COMPARISON: Unoptimized vs. Optimized Prompt\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nüìä Scenario: 1 million customer support requests\\n\")\n",
    "\n",
    "print(\"Unoptimized (5,000 input tokens per request):\")\n",
    "print(f\"  - Input cost:  ${unoptimized['input_cost']:,.2f}\")\n",
    "print(f\"  - Output cost: ${unoptimized['output_cost']:,.2f}\")\n",
    "print(f\"  - Total cost:  ${unoptimized['total_cost']:,.2f}\")\n",
    "\n",
    "print(\"\\nOptimized (3,000 input tokens per request):\")\n",
    "print(f\"  - Input cost:  ${optimized['input_cost']:,.2f}\")\n",
    "print(f\"  - Output cost: ${optimized['output_cost']:,.2f}\")\n",
    "print(f\"  - Total cost:  ${optimized['total_cost']:,.2f}\")\n",
    "\n",
    "print(f\"\\nüí∞ Savings: ${savings:,.2f} ({savings_percentage:.1f}% reduction)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insight\n",
    "\n",
    "By reducing prompt size from 5,000 to 3,000 tokens (40% reduction):\n",
    "- **Input tokens reduced from $15,000 to $9,000** - saving $6,000 at 1M requests\n",
    "- **Input cost represents 87% of total cost** in this scenario ($15,000 input vs. $2,250 output)\n",
    "- **34.8% total cost reduction** through token optimization alone\n",
    "- This is before applying prompt caching - which can save another 75-90%!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latency Impact Simulator\n",
    "\n",
    "Let's simulate how token count affects response latency.\n",
    "\n",
    "**Approximate processing rates**:\n",
    "- Input tokens: ~0.5ms per token (model-dependent)\n",
    "- Output tokens: ~50ms per token (generation is slower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LATENCY COMPARISON: Unoptimized vs. Optimized Prompt\n",
      "============================================================\n",
      "\n",
      "üìä Scenario: Customer support response generation\n",
      "\n",
      "Unoptimized (5,000 input tokens):\n",
      "  - Input processing:  2500ms\n",
      "  - Output generation: 7500ms\n",
      "  - Total latency:     10.00s\n",
      "\n",
      "Optimized (3,000 input tokens):\n",
      "  - Input processing:  1500ms\n",
      "  - Output generation: 7500ms\n",
      "  - Total latency:     9.00s\n",
      "\n",
      "‚ö° Latency improvement: 1000ms (10.0% faster)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def calculate_latency(input_tokens, output_tokens, input_ms_per_token=0.5, output_ms_per_token=50):\n",
    "    \"\"\"\n",
    "    Estimate latency for LLM inference.\n",
    "    \n",
    "    Args:\n",
    "        input_tokens: Number of input tokens\n",
    "        output_tokens: Number of output tokens\n",
    "        input_ms_per_token: Processing time per input token in ms (default: 0.5ms)\n",
    "        output_ms_per_token: Generation time per output token in ms (default: 50ms)\n",
    "    \n",
    "    Returns:\n",
    "        dict with latency breakdown\n",
    "    \"\"\"\n",
    "    input_latency_ms = input_tokens * input_ms_per_token\n",
    "    output_latency_ms = output_tokens * output_ms_per_token\n",
    "    total_latency_ms = input_latency_ms + output_latency_ms\n",
    "    \n",
    "    return {\n",
    "        'input_latency_ms': input_latency_ms,\n",
    "        'output_latency_ms': output_latency_ms,\n",
    "        'total_latency_ms': total_latency_ms,\n",
    "        'total_latency_sec': total_latency_ms / 1000\n",
    "    }\n",
    "\n",
    "# Compare latency: Unoptimized vs. Optimized\n",
    "latency_unoptimized = calculate_latency(input_tokens=5000, output_tokens=150)\n",
    "latency_optimized = calculate_latency(input_tokens=3000, output_tokens=150)\n",
    "\n",
    "latency_savings_ms = latency_unoptimized['total_latency_ms'] - latency_optimized['total_latency_ms']\n",
    "latency_savings_pct = (latency_savings_ms / latency_unoptimized['total_latency_ms']) * 100\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LATENCY COMPARISON: Unoptimized vs. Optimized Prompt\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nüìä Scenario: Customer support response generation\\n\")\n",
    "\n",
    "print(\"Unoptimized (5,000 input tokens):\")\n",
    "print(f\"  - Input processing:  {latency_unoptimized['input_latency_ms']:.0f}ms\")\n",
    "print(f\"  - Output generation: {latency_unoptimized['output_latency_ms']:.0f}ms\")\n",
    "print(f\"  - Total latency:     {latency_unoptimized['total_latency_sec']:.2f}s\")\n",
    "\n",
    "print(\"\\nOptimized (3,000 input tokens):\")\n",
    "print(f\"  - Input processing:  {latency_optimized['input_latency_ms']:.0f}ms\")\n",
    "print(f\"  - Output generation: {latency_optimized['output_latency_ms']:.0f}ms\")\n",
    "print(f\"  - Total latency:     {latency_optimized['total_latency_sec']:.2f}s\")\n",
    "\n",
    "print(f\"\\n‚ö° Latency improvement: {latency_savings_ms:.0f}ms ({latency_savings_pct:.1f}% faster)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insight\n",
    "\n",
    "By reducing prompt size from 5,000 to 3,000 tokens:\n",
    "- **Input processing time reduced by 1 second** (2,000 fewer tokens √ó 0.5ms/token)\n",
    "- At scale, faster responses improve user experience and system throughput\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale Impact: ROI Calculator\n",
    "\n",
    "Let's calculate the ROI of optimization across different usage scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ROI OF OPTIMIZATION: 5000 ‚Üí 3000 tokens\n",
      "================================================================================\n",
      "\n",
      "Scale                          Original Cost   Optimized Cost  Savings        \n",
      "--------------------------------------------------------------------------------\n",
      "1K requests (pilot)            $       17.25  $       11.25  $        6.00\n",
      "10K requests (small scale)     $      172.50  $      112.50  $       60.00\n",
      "100K requests (medium scale)   $    1,725.00  $    1,125.00  $      600.00\n",
      "1M requests (large scale)      $   17,250.00  $   11,250.00  $    6,000.00\n",
      "10M requests (enterprise scale) $  172,500.00  $  112,500.00  $   60,000.00\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def roi_at_scale(original_tokens, optimized_tokens, output_tokens=150):\n",
    "    \"\"\"\n",
    "    Calculate ROI of prompt optimization at different scales.\n",
    "    \"\"\"\n",
    "    scales = [\n",
    "        (1_000, \"1K requests (pilot)\"),\n",
    "        (10_000, \"10K requests (small scale)\"),\n",
    "        (100_000, \"100K requests (medium scale)\"),\n",
    "        (1_000_000, \"1M requests (large scale)\"),\n",
    "        (10_000_000, \"10M requests (enterprise scale)\")\n",
    "    ]\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(f\"ROI OF OPTIMIZATION: {original_tokens} ‚Üí {optimized_tokens} tokens\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\n{'Scale':<30} {'Original Cost':<15} {'Optimized Cost':<15} {'Savings':<15}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for num_requests, label in scales:\n",
    "        original_cost = calculate_cost(original_tokens, output_tokens, num_requests)['total_cost']\n",
    "        optimized_cost = calculate_cost(optimized_tokens, output_tokens, num_requests)['total_cost']\n",
    "        savings = original_cost - optimized_cost\n",
    "        \n",
    "        print(f\"{label:<30} ${original_cost:>12,.2f}  ${optimized_cost:>12,.2f}  ${savings:>12,.2f}\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "\n",
    "# Run ROI analysis\n",
    "roi_at_scale(original_tokens=5000, optimized_tokens=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insight\n",
    "\n",
    "By reducing prompt size from 5,000 to 3,000 tokens:\n",
    "- At 1K requests: **$6 savings** (pilot testing)\n",
    "- At 100K requests: **$600 savings** (hundreds of dollars)\n",
    "- At 1M requests: **$6,000 savings** (thousands of dollars)\n",
    "- At 10M requests: **$60,000 savings** (tens of thousands of dollars)\n",
    "\n",
    "**Optimization ROI scales linearly with request volume. At production scale (1M+ requests), token optimization delivers substantial savings.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Optimization Paradigm Shift\n",
    "\n",
    "Now that you understand the economics (cost, latency, ROI), let's examine how to approach optimization systematically.\n",
    "\n",
    "### Traditional Approach ‚ùå\n",
    "\n",
    "```\n",
    "Write Prompt ‚Üí Test Manually ‚Üí Deploy to Production ‚Üí Hope for the Best\n",
    "```\n",
    "\n",
    "**Problems**:\n",
    "- Ad-hoc testing (\"it works for me\")\n",
    "- No systematic evaluation\n",
    "- No visibility into costs or performance\n",
    "- Difficult to iterate and improve\n",
    "- Regressions go unnoticed\n",
    "- **No strategy for caching or token optimization**\n",
    "\n",
    "### Modern Optimized Approach ‚úÖ\n",
    "\n",
    "```\n",
    "Design Prompt ‚Üí Evaluate Systematically ‚Üí Apply Caching Strategy ‚Üí \n",
    "Monitor Metrics ‚Üí Iterate Based on Data\n",
    "```\n",
    "\n",
    "**Benefits**:\n",
    "- **Data-driven decisions**: Use evaluation datasets, not anecdotes\n",
    "- **Continuous improvement loop**: Measure, optimize, measure again\n",
    "- **Cost and performance visibility**: Track metrics in production\n",
    "- **Systematic evaluation and testing**: Catch regressions before deployment\n",
    "- **Version control**: Track prompt changes like code\n",
    "- **Strategic caching**: Cache static content for 75-90% additional savings\n",
    "\n",
    "---\n",
    "\n",
    "### What This Workshop Teaches You\n",
    "\n",
    "This paradigm shift requires three foundational pillars that are covered in subsequent sections:\n",
    "\n",
    "**1. Understanding Caching Mechanics**\n",
    "- How prompt caching works (cache hits, misses, TTL)\n",
    "- When caching provides ROI (break-even analysis)\n",
    "- Cost structure: cache write vs. cache read\n",
    "- **Goal**: Amplify savings by another 75-90% through strategic caching\n",
    "\n",
    "**2. Optimization Techniques**\n",
    "- Manual & automated optimization\n",
    "- Decision framework: Choosing the right approach\n",
    "- **Goal**: Improve prompt quality while reducing tokens\n",
    "\n",
    "**3. Production Integration**\n",
    "- Caching patterns for different use cases\n",
    "- Observability and monitoring\n",
    "- Evaluation frameworks for systematic testing\n",
    "- CI/CD for prompt lifecycle management\n",
    "- **Goal**: Build production-grade GenAI systems\n",
    "\n",
    "### The Compounding Effect\n",
    "\n",
    "When you combine:\n",
    "- **Token optimization** (30-40% cost savings) \n",
    "- **Strategic caching** (75-90% additional savings on cached tokens)\n",
    "- **Systematic evaluation** (prevent regressions, improve quality)\n",
    "- **Production monitoring** (catch issues early, optimize continuously)\n",
    "\n",
    "**Result**: Cost reductions of 80-95% with improved quality and reliability.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned the economic fundamentals of prompt optimization:\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. ‚úÖ **Cost Efficiency**: At 1M requests, reducing input tokens from 5,000‚Üí3,000 saves **$6,000 (34.8% cost reduction)**. Input tokens can represent up to 87% of total inference costs.\n",
    "\n",
    "2. ‚úÖ **Latency Impact**: Reducing 2,000 input tokens saves **1 second of processing time**, improving user experience and system throughput.\n",
    "\n",
    "3. ‚úÖ **Scale Amplification**: Optimization ROI scales linearly with volume:\n",
    "   - 1K requests: $6 savings\n",
    "   - 100K requests: $600 savings\n",
    "   - 1M requests: $6,000 savings\n",
    "   - 10M requests: $60,000 savings\n",
    "\n",
    "4. ‚úÖ **Paradigm Shift**: Move from ad-hoc testing to systematic, data-driven optimization with:\n",
    "   - Strategic caching (75-90% additional savings)\n",
    "   - Systematic evaluation (prevent regressions)\n",
    "   - Production monitoring (continuous improvement)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
