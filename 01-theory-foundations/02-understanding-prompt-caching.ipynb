{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Understanding Prompt Caching\n",
    "\n",
    "**Duration**: 30 minutes\n",
    "\n",
    "**Learning Objectives**:\n",
    "- Understand cache mechanics (TTL, cache hits/misses, cache write/read)\n",
    "- Learn the cost structure of prompt caching\n",
    "- Calculate break-even points for caching ROI\n",
    "- Know which models support caching and their token requirements\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Environment loaded (AWS CLI credentials will be used)\n",
      "â„¹ï¸  boto3 will automatically use AWS CLI credentials from ~/.aws/credentials\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Verify AWS credentials are loaded (without exposing sensitive values)\n",
    "# Credentials can come from .env, AWS CLI (~/.aws/credentials), or environment variables\n",
    "if os.getenv('AWS_ACCESS_KEY_ID') and os.getenv('AWS_SECRET_ACCESS_KEY'):\n",
    "    print(\"âœ… AWS credentials loaded successfully\")\n",
    "    print(f\"âœ… Region: {os.getenv('AWS_DEFAULT_REGION', 'us-east-1')}\")\n",
    "else:\n",
    "    print(\"âœ… Environment loaded (AWS CLI credentials will be used)\")\n",
    "    print(\"â„¹ï¸  boto3 will automatically use AWS CLI credentials from ~/.aws/credentials\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Prompt Caching?\n",
    "\n",
    "**Prompt caching** is an Amazon Bedrock feature that stores frequently used portions of prompts (such as context documents, system instructions, tool definitions) to avoid recomputation across multiple API calls.\n",
    "\n",
    "### Key Concept\n",
    "\n",
    "**Cache the static, reuse it repeatedly.**\n",
    "\n",
    "Instead of processing the same 5,000-token document on every request, cache it once and only process the dynamic user query (e.g., 50 tokens) on subsequent requests.\n",
    "\n",
    "### Benefits\n",
    "\n",
    "1. **Cost Savings**: Cache reads cost ~90% less than regular input tokens\n",
    "2. **Latency Reduction**: Skip recomputation of cached content (50-85% faster)\n",
    "3. **Scale Efficiency**: Benefits compound with high request volumes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache Mechanics\n",
    "\n",
    "### Time-To-Live (TTL)\n",
    "\n",
    "- **Cache duration**: 5 minutes\n",
    "- **TTL resets** on each cache hit (extending cache lifetime)\n",
    "- **After 5 minutes** of no hits, cache expires and must be rewritten\n",
    "\n",
    "### Cache Hit\n",
    "\n",
    "Occurs when the prompt prefix matches exactly with cached content.\n",
    "\n",
    "**Result**:\n",
    "- Significantly reduced cost (~90% savings on cached portion)\n",
    "- Reduced latency (skip processing of cached tokens)\n",
    "- Only the uncached portion (dynamic content) is processed\n",
    "\n",
    "### Cache Miss\n",
    "\n",
    "Occurs when prompt prefix differs from cached content.\n",
    "\n",
    "**Result**:\n",
    "- Full recomputation required\n",
    "- Results in cache write (higher cost than regular input)\n",
    "\n",
    "**Common causes**:\n",
    "- Content changes (modified document, updated system prompt)\n",
    "- Cache expiration (5 minutes elapsed with no hits)\n",
    "- First request (no cache exists yet)\n",
    "\n",
    "### Cache Write\n",
    "\n",
    "First occurrence of a cacheable prompt prefix.\n",
    "\n",
    "**Cost**: Higher than regular input tokens (typically 1.25x - check current pricing)\n",
    "\n",
    "**Think of it as**: An investment that pays dividends with subsequent cache reads.\n",
    "\n",
    "### Cache Read\n",
    "\n",
    "Subsequent uses of cached content (cache hit scenario).\n",
    "\n",
    "**Cost**: ~90% less than regular input tokens (typically 0.1x)\n",
    "\n",
    "**Latency**: Significantly faster (skip token processing)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache TTL Simulation\n",
    "\n",
    "Let's simulate how cache TTL works with requests over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CACHE TTL SIMULATION (5-minute TTL)\n",
      "================================================================================\n",
      "\n",
      "Time     Cache Status              Description                             \n",
      "--------------------------------------------------------------------------------\n",
      "0m       ðŸ“ cache_write            First request with document A\n",
      "1m       âœ… cache_hit              Query 2 (same document)\n",
      "2m       âœ… cache_hit              Query 3 (same document)\n",
      "3m       âœ… cache_hit              Query 4 (same document)\n",
      "4m       âœ… cache_hit              Query 5 (same document)\n",
      "6m       âœ… cache_hit              Query 6 (6 mins later - TTL reset by previous hit)\n",
      "12m       âŒ cache_miss (expired)   Query 7 (12 mins - cache expired!)\n",
      "13m       âŒ cache_miss (content changed) Query 8 (content changed!)\n",
      "================================================================================\n",
      "\n",
      "ðŸ’¡ Key Observations:\n",
      "  - First request = cache write (investment)\n",
      "  - Subsequent requests within 5 min = cache hit (savings!)\n",
      "  - Each cache hit RESETS the 5-minute TTL\n",
      "  - After 5 min of no hits = cache expires\n",
      "  - Content changes = cache miss (new write required)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "class CacheSimulator:\n",
    "    def __init__(self, ttl_minutes=5):\n",
    "        self.ttl_minutes = ttl_minutes\n",
    "        self.cache = None\n",
    "        self.cache_expiry = None\n",
    "    \n",
    "    def request(self, timestamp, content_hash):\n",
    "        \"\"\"\n",
    "        Simulate a request with cache checking.\n",
    "        \n",
    "        Args:\n",
    "            timestamp: datetime object for request time\n",
    "            content_hash: hash of the cacheable content\n",
    "        \n",
    "        Returns:\n",
    "            'cache_write', 'cache_hit', or 'cache_miss'\n",
    "        \"\"\"\n",
    "        # No cache exists yet\n",
    "        if self.cache is None:\n",
    "            self.cache = content_hash\n",
    "            self.cache_expiry = timestamp + timedelta(minutes=self.ttl_minutes)\n",
    "            return 'cache_write'\n",
    "        \n",
    "        # Cache expired\n",
    "        if timestamp >= self.cache_expiry:\n",
    "            self.cache = content_hash\n",
    "            self.cache_expiry = timestamp + timedelta(minutes=self.ttl_minutes)\n",
    "            return 'cache_miss (expired)'\n",
    "        \n",
    "        # Content changed\n",
    "        if content_hash != self.cache:\n",
    "            self.cache = content_hash\n",
    "            self.cache_expiry = timestamp + timedelta(minutes=self.ttl_minutes)\n",
    "            return 'cache_miss (content changed)'\n",
    "        \n",
    "        # Cache hit! Reset TTL\n",
    "        self.cache_expiry = timestamp + timedelta(minutes=self.ttl_minutes)\n",
    "        return 'cache_hit'\n",
    "\n",
    "# Simulate a series of requests\n",
    "cache = CacheSimulator(ttl_minutes=5)\n",
    "start_time = datetime.now()\n",
    "\n",
    "requests = [\n",
    "    (0, 'content_v1', 'First request with document A'),\n",
    "    (1, 'content_v1', 'Query 2 (same document)'),\n",
    "    (2, 'content_v1', 'Query 3 (same document)'),\n",
    "    (3, 'content_v1', 'Query 4 (same document)'),\n",
    "    (4, 'content_v1', 'Query 5 (same document)'),\n",
    "    (6, 'content_v1', 'Query 6 (6 mins later - TTL reset by previous hit)'),\n",
    "    (12, 'content_v1', 'Query 7 (12 mins - cache expired!)'),\n",
    "    (13, 'content_v2', 'Query 8 (content changed!)'),\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CACHE TTL SIMULATION (5-minute TTL)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n{'Time':<8} {'Cache Status':<25} {'Description':<40}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for minutes, content, description in requests:\n",
    "    timestamp = start_time + timedelta(minutes=minutes)\n",
    "    result = cache.request(timestamp, content)\n",
    "    \n",
    "    # Format with emoji\n",
    "    if 'hit' in result:\n",
    "        status_icon = 'âœ…'\n",
    "    elif 'write' in result:\n",
    "        status_icon = 'ðŸ“'\n",
    "    else:\n",
    "        status_icon = 'âŒ'\n",
    "    \n",
    "    print(f\"{minutes}m       {status_icon} {result:<22} {description}\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nðŸ’¡ Key Observations:\")\n",
    "print(\"  - First request = cache write (investment)\")\n",
    "print(\"  - Subsequent requests within 5 min = cache hit (savings!)\")\n",
    "print(\"  - Each cache hit RESETS the 5-minute TTL\")\n",
    "print(\"  - After 5 min of no hits = cache expires\")\n",
    "print(\"  - Content changes = cache miss (new write required)\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Supported Models and Token Requirements\n",
    "\n",
    "Not all models support caching, and each has different token minimums.\n",
    "\n",
    "### Current Supported Models (as of Dec 2025)\n",
    "\n",
    "| Model Family | Min Tokens per Checkpoint | Max Checkpoints | Cacheable Fields |\n",
    "|--------------|---------------------------|-----------------|------------------|\n",
    "| Claude Opus 4.1, Sonnet 4.5, 3.7 Sonnet, Opus 4, Sonnet 4 | 1,024 | 4 | system, messages, tools |\n",
    "| Claude Opus 4.5, Haiku 4.5 | 4,096 | 4 | system, messages, tools |\n",
    "| Claude 3.5 Haiku | 2,048 | 4 | system, messages, tools |\n",
    "| Amazon Nova (Premier, Pro, Lite, Micro) | ~1,024 (max 20K total) | 4 | system, messages |\n",
    "\n",
    "**Important Notes**:\n",
    "- Token requirements are **minimums per checkpoint** - content must meet this threshold to be cached\n",
    "- Claude models support caching in `system`, `messages`, and `tools` fields (we'll explore these in Part 2 labs)\n",
    "- Nova models support caching in `system` and `messages` fields (tools not cached)\n",
    "- For current pricing details, check [Amazon Bedrock Pricing](https://aws.amazon.com/bedrock/pricing/)\n",
    "- For caching implementation details, see [Prompt Caching Documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-caching.html)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Structure: Cache Write vs. Cache Read\n",
    "\n",
    "Let's calculate the economics of caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COST COMPARISON: Caching vs. No Caching\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Scenario: 100 requests with 5,000-token document + 100-token query\n",
      "\n",
      "Without caching:\n",
      "  - All requests: $1.5300\n",
      "\n",
      "With caching:\n",
      "  - First request (cache write):   $0.019050\n",
      "  - Subsequent requests (cache read): $0.001800 each\n",
      "  - Total cost: $0.1973\n",
      "\n",
      "ðŸ’° Savings: $1.3328 (87.1% reduction)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def calculate_caching_cost(cacheable_tokens, dynamic_tokens, num_requests, \n",
    "                           input_price=3.0, cache_write_multiplier=1.25, cache_read_multiplier=0.1):\n",
    "    \"\"\"\n",
    "    Calculate cost comparison: with vs. without caching.\n",
    "    \n",
    "    Args:\n",
    "        cacheable_tokens: Tokens that can be cached (static content)\n",
    "        dynamic_tokens: Tokens that cannot be cached (user queries)\n",
    "        num_requests: Total number of requests\n",
    "        input_price: Price per million input tokens (default: $3.00)\n",
    "        cache_write_multiplier: Cache write cost multiplier (default: 1.25x)\n",
    "        cache_read_multiplier: Cache read cost multiplier (default: 0.1x)\n",
    "    \n",
    "    Returns:\n",
    "        dict with cost comparison\n",
    "    \"\"\"\n",
    "    total_tokens = cacheable_tokens + dynamic_tokens\n",
    "    \n",
    "    # Without caching: All tokens charged at standard input rate\n",
    "    cost_no_cache = (total_tokens * num_requests / 1_000_000) * input_price\n",
    "    \n",
    "    # With caching:\n",
    "    # - First request: cache write (1.25x) + dynamic tokens (1x)\n",
    "    # - Subsequent requests: cache read (0.1x) + dynamic tokens (1x)\n",
    "    first_request_cost = (\n",
    "        (cacheable_tokens * cache_write_multiplier / 1_000_000) * input_price +\n",
    "        (dynamic_tokens / 1_000_000) * input_price\n",
    "    )\n",
    "    \n",
    "    subsequent_request_cost = (\n",
    "        (cacheable_tokens * cache_read_multiplier / 1_000_000) * input_price +\n",
    "        (dynamic_tokens / 1_000_000) * input_price\n",
    "    )\n",
    "    \n",
    "    cost_with_cache = first_request_cost + (subsequent_request_cost * (num_requests - 1))\n",
    "    \n",
    "    savings = cost_no_cache - cost_with_cache\n",
    "    savings_pct = (savings / cost_no_cache) * 100\n",
    "    \n",
    "    return {\n",
    "        'cost_no_cache': cost_no_cache,\n",
    "        'cost_with_cache': cost_with_cache,\n",
    "        'savings': savings,\n",
    "        'savings_pct': savings_pct,\n",
    "        'first_request_cost': first_request_cost,\n",
    "        'subsequent_request_cost': subsequent_request_cost\n",
    "    }\n",
    "\n",
    "# Example: Document Q&A chatbot\n",
    "# - 5,000 token document (cacheable)\n",
    "# - 100 token user query (dynamic)\n",
    "result = calculate_caching_cost(\n",
    "    cacheable_tokens=5000,\n",
    "    dynamic_tokens=100,\n",
    "    num_requests=100\n",
    ")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COST COMPARISON: Caching vs. No Caching\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nðŸ“Š Scenario: 100 requests with 5,000-token document + 100-token query\\n\")\n",
    "\n",
    "print(f\"Without caching:\")\n",
    "print(f\"  - All requests: ${result['cost_no_cache']:.4f}\")\n",
    "\n",
    "print(f\"\\nWith caching:\")\n",
    "print(f\"  - First request (cache write):   ${result['first_request_cost']:.6f}\")\n",
    "print(f\"  - Subsequent requests (cache read): ${result['subsequent_request_cost']:.6f} each\")\n",
    "print(f\"  - Total cost: ${result['cost_with_cache']:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ’° Savings: ${result['savings']:.4f} ({result['savings_pct']:.1f}% reduction)\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Break-Even Analysis\n",
    "\n",
    "How many requests do you need before caching pays off?\n",
    "\n",
    "**Formula**: Cache write cost > regular input cost, but cache reads cost << regular input\n",
    "\n",
    "**Break-even point**: When savings from cache reads offset the initial cache write investment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BREAK-EVEN ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Scenario                                      Break-Even     \n",
      "--------------------------------------------------------------------------------\n",
      "Large document (5K tokens) + short query      2 requests\n",
      "Medium document (3K tokens) + medium query    2 requests\n",
      "Small document (1.5K tokens) + long query     2 requests\n",
      "\n",
      "================================================================================\n",
      "ðŸ’¡ Key Insight: Break-even is typically 2-3 requests.\n",
      "   After that, every request saves ~75-90% on cached tokens!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def find_breakeven_point(cacheable_tokens, dynamic_tokens, input_price=3.0):\n",
    "    \"\"\"\n",
    "    Calculate break-even point for prompt caching.\n",
    "    \n",
    "    Returns:\n",
    "        Number of requests needed to break even\n",
    "    \"\"\"\n",
    "    # Cost without caching per request\n",
    "    cost_per_request_no_cache = ((cacheable_tokens + dynamic_tokens) / 1_000_000) * input_price\n",
    "    \n",
    "    # Cost with caching (first request)\n",
    "    first_request_cost = (\n",
    "        (cacheable_tokens * 1.25 / 1_000_000) * input_price +\n",
    "        (dynamic_tokens / 1_000_000) * input_price\n",
    "    )\n",
    "    \n",
    "    # Cost with caching (subsequent requests)\n",
    "    subsequent_request_cost = (\n",
    "        (cacheable_tokens * 0.1 / 1_000_000) * input_price +\n",
    "        (dynamic_tokens / 1_000_000) * input_price\n",
    "    )\n",
    "    \n",
    "    # Find break-even point\n",
    "    # No cache: N * cost_per_request_no_cache\n",
    "    # With cache: first_request_cost + (N-1) * subsequent_request_cost\n",
    "    # Solve: N * cost_per_request_no_cache = first_request_cost + (N-1) * subsequent_request_cost\n",
    "    \n",
    "    breakeven = (first_request_cost - subsequent_request_cost) / (cost_per_request_no_cache - subsequent_request_cost)\n",
    "    \n",
    "    return {\n",
    "        'breakeven_requests': breakeven,\n",
    "        'cost_per_request_no_cache': cost_per_request_no_cache,\n",
    "        'first_request_cost': first_request_cost,\n",
    "        'subsequent_request_cost': subsequent_request_cost\n",
    "    }\n",
    "\n",
    "# Calculate break-even for different scenarios\n",
    "scenarios = [\n",
    "    (5000, 100, \"Large document (5K tokens) + short query\"),\n",
    "    (3000, 200, \"Medium document (3K tokens) + medium query\"),\n",
    "    (1500, 500, \"Small document (1.5K tokens) + long query\"),\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"BREAK-EVEN ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n{'Scenario':<45} {'Break-Even':<15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for cacheable, dynamic, description in scenarios:\n",
    "    result = find_breakeven_point(cacheable, dynamic)\n",
    "    breakeven_rounded = int(result['breakeven_requests']) + 1\n",
    "    \n",
    "    print(f\"{description:<45} {breakeven_rounded} requests\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸ’¡ Key Insight: Break-even is typically 2-3 requests.\")\n",
    "print(\"   After that, every request saves ~75-90% on cached tokens!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## When Caching Makes Economic Sense\n",
    "\n",
    "Use this checklist to determine if caching is beneficial:\n",
    "\n",
    "### âœ… Good Use Cases for Caching\n",
    "\n",
    "1. **Document Q&A**: Users ask multiple questions about the same document\n",
    "2. **Chatbots with context**: Static company policies + dynamic user queries\n",
    "3. **Agents with tools**: Tool definitions rarely change, but queries vary\n",
    "4. **Batch processing**: Same prompt template across many inputs\n",
    "5. **Few-shot learning**: Static examples + dynamic test cases\n",
    "\n",
    "**Key criteria**:\n",
    "- Static content â‰¥ 1,024 tokens (or model minimum)\n",
    "- Expected 2+ requests within 5 minutes\n",
    "- Static content reused frequently\n",
    "\n",
    "### âŒ Poor Use Cases for Caching\n",
    "\n",
    "1. **One-off requests**: No reuse = no benefit\n",
    "2. **Highly dynamic content**: Content changes every request = constant cache misses\n",
    "3. **Small prompts**: Below minimum token threshold (< 1,024 tokens)\n",
    "4. **Low request volume**: Break-even never reached\n",
    "5. **Long gaps between requests**: Cache expires (> 5 minutes)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned the mechanics and economics of prompt caching:\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. âœ… **Cache Mechanics**: \n",
    "   - 5-minute TTL (resets on each cache hit)\n",
    "   - Cache hits save ~90% on cached tokens\n",
    "   - Cache misses trigger cache writes (1.25x cost investment)\n",
    "\n",
    "2. âœ… **Cost Structure**: \n",
    "   - Cache write: 1.25x regular input (one-time investment)\n",
    "   - Cache read: 0.1x regular input (90% savings)\n",
    "   - Break-even: Typically 2-3 requests\n",
    "   - Example: 100 requests with 5K-token document â†’ **87.1% cost reduction**\n",
    "\n",
    "3. âœ… **Model Support**: \n",
    "   - Claude models: 1,024+ tokens minimum (Opus/Sonnet), 4,096+ (Haiku 4.5)\n",
    "   - Nova models: ~1,024 tokens minimum (max 20K total)\n",
    "   - Maximum 4 cache checkpoints per request\n",
    "   - Cacheable fields: system, messages, tools (we'll explore these in Part 2)\n",
    "\n",
    "4. âœ… **When to Cache**: \n",
    "   - Static content â‰¥ model minimum token threshold\n",
    "   - Expected 2+ requests within 5 minutes\n",
    "   - Frequent reuse of static content (documents, policies, tool definitions)\n",
    "   \n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws-bedrock-prompt-optimization-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
