{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "introduction",
   "metadata": {},
   "source": [
    "# Advanced Prompt Engineering Techniques\n",
    "\n",
    "This notebook covers advanced prompt engineering techniques beyond the basics. You'll learn reasoning enhancement patterns, iterative refinement methods, efficiency optimizations, and systematic prompt improvement workflows.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "- Implement **reasoning enhancement** patterns like Chain-of-Thought and Extended Thinking\n",
    "- Apply **iterative refinement** methods such as Self-Refine and Chain-of-Verification\n",
    "- Optimize for **efficiency** using Chain-of-Draft for reduced latency\n",
    "- Generate **diverse outputs** with Verbalized Sampling techniques\n",
    "- Apply systematic **prompt optimization workflows** using manual iteration, LLM-assisted exploration, and automated evaluation\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "Advanced prompt engineering techniques can dramatically improve output quality for complex tasks:\n",
    "- **Chain-of-Thought** can improve accuracy on reasoning tasks by 40-60%\n",
    "- **Extended Thinking** enables deep analysis for complex problems\n",
    "- **Systematic optimization workflows** help you find the best prompt variant without guesswork\n",
    "\n",
    "**Duration**: ~60-90 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisites",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, ensure you have:\n",
    "1. An AWS account with Amazon Bedrock access enabled\n",
    "2. AWS credentials configured (via `.env` file, AWS CLI, or IAM role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region: us-east-1\n",
      "Default Model: global.anthropic.claude-haiku-4-5-20251001-v1:0\n",
      "\n",
      "Setup complete!\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import boto3\n",
    "\n",
    "# Initialize Bedrock clients\n",
    "REGION = os.getenv(\"AWS_DEFAULT_REGION\", \"us-east-1\")\n",
    "bedrock_runtime = boto3.client(\"bedrock-runtime\", region_name=REGION)\n",
    "bedrock_agent_runtime = boto3.client(\"bedrock-agent-runtime\", region_name=REGION)\n",
    "\n",
    "# Model configurations\n",
    "MODEL_SONNET = \"global.anthropic.claude-sonnet-4-5-20250929-v1:0\"\n",
    "MODEL_HAIKU = \"global.anthropic.claude-haiku-4-5-20251001-v1:0\"\n",
    "MODEL_HAIKU_35 = \"us.anthropic.claude-3-5-haiku-20241022-v1:0\"\n",
    "MODEL_ID = MODEL_HAIKU\n",
    "\n",
    "\n",
    "def call_bedrock(prompt, system=None, temperature=0.0, max_tokens=1000, model_id=None):\n",
    "    \"\"\"\n",
    "    Helper function to call Bedrock Converse API.\n",
    "    \"\"\"\n",
    "    model = model_id or MODEL_ID\n",
    "    messages = [{\"role\": \"user\", \"content\": [{\"text\": prompt}]}]\n",
    "\n",
    "    kwargs = {\n",
    "        \"modelId\": model,\n",
    "        \"messages\": messages,\n",
    "        \"inferenceConfig\": {\"maxTokens\": max_tokens, \"temperature\": temperature},\n",
    "    }\n",
    "\n",
    "    if system:\n",
    "        kwargs[\"system\"] = [{\"text\": system}]\n",
    "\n",
    "    response = bedrock_runtime.converse(**kwargs)\n",
    "\n",
    "    return {\n",
    "        \"text\": response[\"output\"][\"message\"][\"content\"][0][\"text\"],\n",
    "        \"input_tokens\": response[\"usage\"][\"inputTokens\"],\n",
    "        \"output_tokens\": response[\"usage\"][\"outputTokens\"],\n",
    "        \"latency_ms\": response[\"metrics\"][\"latencyMs\"],\n",
    "    }\n",
    "\n",
    "\n",
    "print(f\"Region: {REGION}\")\n",
    "print(f\"Default Model: {MODEL_ID}\")\n",
    "print(\"\\nSetup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "info-model",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> This notebook uses Claude models with the <b>global</b> CRIS (Cross-Region Inference Service) profile for higher availability and ~10% cost savings.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foundation-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Category 1: Foundation Techniques\n",
    "\n",
    "Foundation techniques are the building blocks of effective prompts. These are covered in detail in [02-optimization-strategy.ipynb](../01-basics/02-optimization-strategy.ipynb) Section 2 & 3:\n",
    "\n",
    "- **Clear Instructions**: Specific, unambiguous directions\n",
    "- **Few-Shot Examples**: 2-5 input/output examples\n",
    "- **Structured Output**: JSON/XML format specifications\n",
    "- **Parameter Tuning**: Temperature, max_tokens optimization\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> If you haven't completed the <code>01-basics/</code> notebooks, we recommend reviewing them first. The techniques covered there are prerequisites for the advanced techniques in this notebook.\n",
    "</div>\n",
    "\n",
    "**Quick refresher on when to apply foundation techniques:**\n",
    "- Use **clear instructions** on every prompt (specify format, length, constraints)\n",
    "- Add **few-shot examples** when you need consistent output format\n",
    "- Use **structured output** (JSON/tool use) for programmatic parsing\n",
    "- Set **temperature=0** for factual tasks, higher for creative tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasoning-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Category 2: Reasoning Enhancement\n",
    "\n",
    "Reasoning enhancement techniques help models tackle complex, multi-step problems more reliably. These techniques are essential when tasks require logical deduction, mathematical calculations, or careful analysis.\n",
    "\n",
    "| Technique | When to Use | Trade-off |\n",
    "|-----------|-------------|-----------|\n",
    "| **Chain-of-Thought (CoT)** | Math, logic, multi-step reasoning | +tokens, +latency |\n",
    "| **Extended Thinking** | Complex analysis, coding, research | +tokens (thinking budget) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cot-header",
   "metadata": {},
   "source": [
    "## 2.1 Chain-of-Thought (CoT)\n",
    "\n",
    "Chain-of-Thought prompting asks the model to show its reasoning process before arriving at an answer. This dramatically improves performance on problems where the direct answer approach often fails.\n",
    "\n",
    "**How it works:**\n",
    "1. Prompt explicitly requests step-by-step reasoning\n",
    "2. Model generates intermediate steps\n",
    "3. Final answer is derived from the reasoning chain\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Trade-off:</b> CoT increases output tokens and latency. Use it when accuracy matters more than speed.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cot-demo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DEMO: Chain-of-Thought vs Direct Answer (Using Haiku 3.5)\n",
      "======================================================================\n",
      "\n",
      "Problem: How many times does the letter 'r' appear in the word 'strawberry'?\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "--- Direct Answer (Haiku 3.5) ---\n",
      "Response: 2\n",
      "Tokens: 5\n",
      "\n",
      "--- Chain-of-Thought (Haiku 3.5) ---\n",
      "Response:\n",
      "Let's solve this step by step:\n",
      "\n",
      "1. Write out the word: strawberry\n",
      "\n",
      "2. List each letter with its position:\n",
      "   s (1st)\n",
      "   t (2nd)\n",
      "   r (3rd) - First 'r'\n",
      "   a (4th)\n",
      "   w (5th)\n",
      "   b (6th)\n",
      "   e (7th)\n",
      "   r (8th) - Second 'r'\n",
      "   r (9th) - Third 'r'\n",
      "   y (10th)\n",
      "\n",
      "3. Count the number of 'r's:\n",
      "   There are 3 'r's in the word 'strawberry'\n",
      "\n",
      "The answer is 3.\n",
      "\n",
      "Tokens: 156\n",
      "\n",
      "======================================================================\n",
      "Correct answer: 3 (st-r-awbe-r-r-y: positions 3, 8, 9)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Demo: Where direct answer fails but CoT succeeds\n",
    "print(\"=\" * 70)\n",
    "print(\"DEMO: Chain-of-Thought vs Direct Answer (Using Haiku 3.5)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Letter counting is notoriously hard for older LLMs without explicit reasoning\n",
    "tricky_problem = \"How many times does the letter 'r' appear in the word 'strawberry'?\"\n",
    "\n",
    "# Direct prompt - models often miscount\n",
    "direct_prompt = f\"{tricky_problem}\\n\\nAnswer with just the number.\"\n",
    "\n",
    "# Chain-of-Thought prompt\n",
    "cot_prompt = f\"\"\"{tricky_problem}\n",
    "\n",
    "Think step by step: list each letter with its position, identify each 'r', then count them.\"\"\"\n",
    "\n",
    "print(f\"\\nProblem: {tricky_problem}\")\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "\n",
    "print(\"\\n--- Direct Answer (Haiku 3.5) ---\")\n",
    "direct_response = call_bedrock(direct_prompt, model_id=MODEL_HAIKU_35)\n",
    "print(f\"Response: {direct_response['text']}\")\n",
    "print(f\"Tokens: {direct_response['output_tokens']}\")\n",
    "\n",
    "print(\"\\n--- Chain-of-Thought (Haiku 3.5) ---\")\n",
    "cot_response = call_bedrock(cot_prompt, model_id=MODEL_HAIKU_35)\n",
    "print(f\"Response:\\n{cot_response['text']}\")\n",
    "print(f\"\\nTokens: {cot_response['output_tokens']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Correct answer: 3 (st-r-awbe-r-r-y: positions 3, 8, 9)\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended-thinking-header",
   "metadata": {},
   "source": [
    "## 2.2: Extended Thinking (Thinking Tokens)\n",
    "\n",
    "Instead of explicitly asking the model to \"think step by step\", some Claude models support **extended thinking** - a native feature that gives the model a dedicated \"thinking budget\" for internal reasoning.\n",
    "\n",
    "**Key differences from explicit CoT:**\n",
    "- Reasoning happens in a separate `thinking` block (not mixed with output)\n",
    "- Model can explore multiple approaches before committing\n",
    "- Billed for full thinking tokens, but output is cleaner\n",
    "\n",
    "| Model | Thinking Output | Notes |\n",
    "|-------|-----------------|-------|\n",
    "| Claude 3.7 Sonnet | Full thinking output | See all reasoning |\n",
    "| Claude 4+ (Sonnet, Opus, Haiku) | Summarized thinking | Billed for full, see summary |\n",
    "\n",
    "<div></div>\n",
    "\n",
    "| Parameter | Description |\n",
    "|-----------|-------------|\n",
    "| `budget_tokens` | Maximum tokens for thinking (minimum 1,024) |\n",
    "| `type` | Set to `\"enabled\"` to activate extended thinking |\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Token Cost:</b> You are billed for the full thinking tokens generated, not just the summary. Claude 4+ models return summarized thinking but bill for the full amount.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "extended-thinking-demo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DEMO: Extended Thinking (Thinking Tokens)\n",
      "======================================================================\n",
      "\n",
      "Problem: A farmer has a fox, a chicken, and a bag of grain. He needs to cross a river \n",
      "in a boat that can only carry him and one item at a time. If left alone:\n",
      "- The fox will eat the chicken\n",
      "- The chicken will eat the grain\n",
      "\n",
      "How can the farmer get all three across safely? List the minimum steps.\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "--- Thinking (Summarized) ---\n",
      "This is a classic river crossing puzzle. Let me work through it step by step.\n",
      "\n",
      "Starting position:\n",
      "- Left bank: Farmer, Fox, Chicken, Grain\n",
      "- Right bank: (empty)\n",
      "\n",
      "Constraints:\n",
      "- Can't leave Fox and Chicken alone together (fox eats chicken)\n",
      "- Can't leave Chicken and Grain alone together (chicken eats grain)\n",
      "- Fox and Grain can be left alone together (they're safe)\n",
      "\n",
      "Let me find the solution:\n",
      "\n",
      "Step 1: Farmer takes Chicken across\n",
      "- Left bank: Fox, Grain\n",
      "- Right bank: Farmer, Chicken\n",
      "(Fox and Grain ar...\n",
      "\n",
      "--- Final Answer ---\n",
      "# Solution: 7 Steps (Minimum)\n",
      "\n",
      "1. **Take chicken across** → Leave fox and grain together (they're safe)\n",
      "\n",
      "2. **Return alone** → Go back for another item\n",
      "\n",
      "3. **Take fox across** → Drop off fox with chicken temporarily\n",
      "\n",
      "4. **Bring chicken back** → (Key step!) This prevents fox from eating chicken\n",
      "\n",
      "5. **Take grain across** → Leave grain with fox (they're safe together)\n",
      "\n",
      "6. **Return alone** → Go back for the chicken\n",
      "\n",
      "7. **Take chicken across** → Everyone is now safely across!\n",
      "\n",
      "**Key insight:** The chicken must make a round trip. It's the only item that's both a predator (to grain) and prey (to fox), so it needs to be separated at the middle stage by temporarily bringing it back.\n",
      "\n",
      "--- Raw Token Usage ---\n",
      "{\n",
      "  \"input_tokens\": 110,\n",
      "  \"cache_creation_input_tokens\": 0,\n",
      "  \"cache_read_input_tokens\": 0,\n",
      "  \"cache_creation\": {\n",
      "    \"ephemeral_5m_input_tokens\": 0,\n",
      "    \"ephemeral_1h_input_tokens\": 0\n",
      "  },\n",
      "  \"output_tokens\": 557\n",
      "}\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Demo: Extended Thinking using InvokeModel API\n",
    "print(\"=\" * 70)\n",
    "print(\"DEMO: Extended Thinking (Thinking Tokens)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Complex problem that benefits from extended thinking\n",
    "complex_problem = \"\"\"A farmer has a fox, a chicken, and a bag of grain. He needs to cross a river \n",
    "in a boat that can only carry him and one item at a time. If left alone:\n",
    "- The fox will eat the chicken\n",
    "- The chicken will eat the grain\n",
    "\n",
    "How can the farmer get all three across safely? List the minimum steps.\"\"\"\n",
    "\n",
    "\n",
    "def call_with_thinking(prompt, budget_tokens=2048, max_tokens=4000):\n",
    "    \"\"\"\n",
    "    Call Claude with extended thinking enabled via InvokeModel API.\n",
    "    \"\"\"\n",
    "    request_body = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"thinking\": {\"type\": \"enabled\", \"budget_tokens\": budget_tokens},\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "    }\n",
    "\n",
    "    response = bedrock_runtime.invoke_model(modelId=MODEL_SONNET, body=json.dumps(request_body))\n",
    "\n",
    "    result = json.loads(response[\"body\"].read())\n",
    "\n",
    "    # Extract thinking and text blocks\n",
    "    thinking_text = None\n",
    "    answer_text = None\n",
    "\n",
    "    for block in result.get(\"content\", []):\n",
    "        if block.get(\"type\") == \"thinking\":\n",
    "            thinking_text = block.get(\"thinking\", \"\")\n",
    "        elif block.get(\"type\") == \"text\":\n",
    "            answer_text = block.get(\"text\", \"\")\n",
    "\n",
    "    return {\"thinking\": thinking_text, \"answer\": answer_text, \"usage\": result.get(\"usage\", {})}\n",
    "\n",
    "\n",
    "print(f\"\\nProblem: {complex_problem}\")\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "\n",
    "result = call_with_thinking(complex_problem, budget_tokens=2048)\n",
    "\n",
    "print(\"\\n--- Thinking (Summarized) ---\")\n",
    "if result[\"thinking\"]:\n",
    "    thinking_preview = result[\"thinking\"][:500]\n",
    "    print(thinking_preview + \"...\" if len(result[\"thinking\"]) > 500 else thinking_preview)\n",
    "\n",
    "print(\"\\n--- Final Answer ---\")\n",
    "print(result[\"answer\"])\n",
    "\n",
    "print(\"\\n--- Raw Token Usage ---\")\n",
    "print(json.dumps(result[\"usage\"], indent=2))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refinement-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Category 3: Iterative Refinement\n",
    "\n",
    "Iterative refinement techniques use multiple LLM calls to progressively improve output quality.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Important:</b> These techniques represent <b>extreme optimization scenarios</b> and are not widely adopted due to significantly increased cost and latency (3-4x API calls). Apply them only in <b>high-stakes scenarios</b> where quality is paramount and the cost is justified.\n",
    "</div>\n",
    "\n",
    "| Technique | When to Use | API Calls | Cost Multiplier |\n",
    "|-----------|-------------|----------|-----------------|\n",
    "| **Self-Refine** | Creative writing, code generation | 3x | ~3x |\n",
    "| **Chain-of-Verification** | Factual accuracy, citations | 4x | ~4x |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "self-refine-header",
   "metadata": {},
   "source": [
    "## 3.1 Self-Refine (Generate -> Critique -> Refine)\n",
    "\n",
    "Self-Refine is a three-step process that mimics human revision:\n",
    "\n",
    "1. **Generate**: Create an initial response (can use smaller/cheaper model)\n",
    "2. **Critique**: Analyze the response for weaknesses (use stronger model for better critique)\n",
    "3. **Refine**: Improve based on the critique (can use smaller model)\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Cost Optimization:</b> Consider using a stronger model (e.g., Sonnet) for critique and a smaller model (e.g., Haiku) for generation and refinement. The critique step benefits most from model capability.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "self-refine-demo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DEMO: Self-Refine (Strong Critique, Efficient Generate/Refine)\n",
      "======================================================================\n",
      "\n",
      "--- Step 1: Initial Generation (Haiku) ---\n",
      "# SoundShield Pro Wireless Headphones\n",
      "\n",
      "Experience crystal-clear audio in any environment with advanced active noise cancellation that blocks up to 99% of ambient sound, letting you focus on what matters most. These premium wireless headphones deliver 30+ hours of battery life, seamless Bluetooth connectivity, and premium comfort for all-day wear. Perfect for travel, work, or everyday listening—immerse yourself in pure sound, distraction-free.\n",
      "\n",
      "--- Step 2: Critique (Sonnet) ---\n",
      "# Review of SoundShield Pro Product Description\n",
      "\n",
      "## Overall Assessment\n",
      "The description is solid but has room for optimization in specificity and structure.\n",
      "\n",
      "## Evaluation by Criteria\n",
      "\n",
      "**Clarity:** Good - Easy to understand, though some claims lack context\n",
      "**Persuasiveness:** Moderate - Makes bold claims but needs more credibility\n",
      "**Key Features:** Present but underdeveloped - Missing technical details\n",
      "**Conciseness:** Good - Appropriate length without excessive fluff\n",
      "\n",
      "## 3 Specific Improvements Needed\n",
      "\n",
      "### 1. **Substantiate the 99% noise cancellation claim**\n",
      "- **Issue:** \"99%\" is an extraordinary claim that may seem exaggerated without proof\n",
      "- **Fix:** Either provide context (e.g., \"blocks up to 99% of low-frequency ambient noise\") or use more credible language (\"industry-leading noise cancellation\" or \"blocks up to 30dB of ambient sound\")\n",
      "\n",
      "### 2. **Add differentiating technical specifications**\n",
      "- **Issue:** Missing details that help buyers compare and justify the purchase\n",
      "- **Fix:** Include driver size, frequency response, codec support (aptX, AAC), or quick-charge capability (e.g., \"10-minute charge = 5 hours playback\")\n",
      "\n",
      "### 3. **Strengthen the call-to-action and value proposition**\n",
      "- **Issue:** Ending is passive; doesn't create urgency or clearly state the benefit hierarchy\n",
      "- **Fix:** Restructure to lead with the primary benefit, add social proof or guarantee, and include a stronger CTA (e.g., \"Join thousands of satisfied customers\" or \"30-day money-back guarantee\")\n",
      "\n",
      "--- Step 3: Refined Output (Haiku) ---\n",
      "# SoundShield Pro Wireless Headphones\n",
      "\n",
      "Experience industry-leading active noise cancellation that reduces low-frequency ambient sound by up to 30dB—ideal for flights, offices, and commutes—while premium 40mm drivers deliver rich, balanced audio across the full frequency spectrum. With 30+ hours of battery life, aptX codec support for lossless streaming, and ergonomic memory-foam ear cups designed for all-day comfort, these headphones adapt to your lifestyle. Backed by a 30-day money-back guarantee, join thousands of users who've made SoundShield Pro their go-to choice for distraction-free listening.\n",
      "\n",
      "---\n",
      "\n",
      "**Key improvements made:**\n",
      "1. ✓ Replaced \"99%\" with credible \"30dB reduction\" + specific use cases\n",
      "2. ✓ Added technical specs (40mm drivers, aptX codec, memory-foam design)\n",
      "3. ✓ Restructured with primary benefit first, added social proof (\"thousands of users\"), and included guarantee + stronger CTA\n",
      "\n",
      "======================================================================\n",
      "Cost Analysis:\n",
      "  Generate (Haiku):  134 tokens\n",
      "  Critique (Sonnet): 512 tokens\n",
      "  Refine (Haiku):    729 tokens\n",
      "\n",
      "Using Sonnet only for critique optimizes cost while maintaining quality.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Demo: Self-Refine with different models for each step\n",
    "print(\"=\" * 70)\n",
    "print(\"DEMO: Self-Refine (Strong Critique, Efficient Generate/Refine)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "task = \"Write a product description for a wireless noise-canceling headphone.\"\n",
    "\n",
    "# Configuration: Use Haiku for generation/refinement, Sonnet for critique\n",
    "GENERATE_MODEL = MODEL_HAIKU  # Cheaper for initial generation\n",
    "CRITIQUE_MODEL = MODEL_SONNET  # Stronger for quality critique\n",
    "REFINE_MODEL = MODEL_HAIKU  # Cheaper for refinement\n",
    "\n",
    "# Step 1: Initial generation (cheaper model)\n",
    "print(\"\\n--- Step 1: Initial Generation (Haiku) ---\")\n",
    "initial_prompt = f\"{task}\\n\\nWrite a concise 2-3 sentence product description.\"\n",
    "initial_response = call_bedrock(initial_prompt, model_id=GENERATE_MODEL)\n",
    "initial_text = initial_response[\"text\"]\n",
    "print(initial_text)\n",
    "\n",
    "# Step 2: Critique (stronger model for better analysis)\n",
    "print(\"\\n--- Step 2: Critique (Sonnet) ---\")\n",
    "critique_prompt = f\"\"\"Review this product description and identify specific improvements:\n",
    "\n",
    "Description: {initial_text}\n",
    "\n",
    "Evaluate on: clarity, persuasiveness, key features, conciseness.\n",
    "List 2-3 specific improvements needed:\"\"\"\n",
    "\n",
    "critique_response = call_bedrock(critique_prompt, model_id=CRITIQUE_MODEL)\n",
    "critique_text = critique_response[\"text\"]\n",
    "print(critique_text)\n",
    "\n",
    "# Step 3: Refine (cheaper model can follow clear critique)\n",
    "print(\"\\n--- Step 3: Refined Output (Haiku) ---\")\n",
    "refine_prompt = f\"\"\"Rewrite this product description incorporating the feedback:\n",
    "\n",
    "Original: {initial_text}\n",
    "\n",
    "Feedback: {critique_text}\n",
    "\n",
    "Write an improved 2-3 sentence description:\"\"\"\n",
    "\n",
    "refined_response = call_bedrock(refine_prompt, model_id=REFINE_MODEL)\n",
    "print(refined_response[\"text\"])\n",
    "\n",
    "# Cost comparison\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Cost Analysis:\")\n",
    "print(f\"  Generate (Haiku):  {initial_response['input_tokens'] + initial_response['output_tokens']} tokens\")\n",
    "print(f\"  Critique (Sonnet): {critique_response['input_tokens'] + critique_response['output_tokens']} tokens\")\n",
    "print(f\"  Refine (Haiku):    {refined_response['input_tokens'] + refined_response['output_tokens']} tokens\")\n",
    "print(\"\\nUsing Sonnet only for critique optimizes cost while maintaining quality.\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cove-header",
   "metadata": {},
   "source": [
    "## 3.2 Chain-of-Verification (CoVe)\n",
    "\n",
    "Chain-of-Verification reduces hallucinations by generating verification questions and cross-checking claims.\n",
    "\n",
    "**The CoVe Process:**\n",
    "1. Generate initial response\n",
    "2. Create verification questions for key claims\n",
    "3. Answer verification questions independently\n",
    "4. Produce final verified response\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Use Case:</b> Only for high-stakes factual content where accuracy is critical (medical, legal, financial). The 4x cost is rarely justified for general use cases.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cove-demo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DEMO: Chain-of-Verification (CoVe)\n",
      "======================================================================\n",
      "\n",
      "--- Step 1: Initial Response ---\n",
      "# 3 Key Features of Amazon S3\n",
      "\n",
      "1. **Scalability & Durability**\n",
      "   - Automatically scales to handle any amount of data\n",
      "   - Provides 99.999999999% (11 nines) durability by replicating data across multiple facilities\n",
      "   - No need to provision storage capacity upfront\n",
      "\n",
      "2. **Security & Access Control**\n",
      "   - Encryption options (in-transit and at-rest)\n",
      "   - Fine-grained access control through IAM policies, bucket policies, and ACLs\n",
      "   - Versioning and MFA delete protection available\n",
      "\n",
      "3. **Cost-Effective & Flexible**\n",
      "   - Pay only for what you use (no upfront costs)\n",
      "   - Multiple storage classes (Standard, Intelligent-Tiering, Glacier, etc.) for different use cases\n",
      "   - Lifecycle policies to automatically transition data to cheaper storage tiers\n",
      "\n",
      "These features make S3 ideal for backup, archiving, content distribution, and data lakes.\n",
      "\n",
      "--- Step 2: Verification Questions ---\n",
      "# 3 Verification Questions\n",
      "\n",
      "1. **Durability Claim Verification**\n",
      "   - Is Amazon S3's durability rating actually 99.999999999% (11 nines), and does this durability apply across all storage classes or only specific ones?\n",
      "\n",
      "2. **Storage Classes Accuracy**\n",
      "   - Are \"Standard, Intelligent-Tiering, Glacier, etc.\" the complete and current list of S3 storage classes, or have additional classes been added/removed in recent AWS updates?\n",
      "\n",
      "3. **Access Control Methods**\n",
      "   - Does S3 truly support all three access control mechanisms mentioned (IAM policies, bucket policies, and ACLs), and are these the only primary methods available for controlling S3 access?\n",
      "\n",
      "--- Step 3: Verification Answers ---\n",
      "# Answers to Amazon S3 Verification Questions\n",
      "\n",
      "## 1. Durability Claim Verification\n",
      "**Partially Accurate with Important Caveats:**\n",
      "- The 99.999999999% (11 nines) durability rating is **correct for most storage classes**\n",
      "- However, this applies to **Standard, Intelligent-Tiering, Standard-IA, One Zone-IA, Glacier Instant Retrieval, and Glacier Flexible Retrieval**\n",
      "- **One Zone-IA has lower durability** (99.95%) due to single availability zone storage\n",
      "- **Glacier Deep Archive** also maintains 11 nines durability\n",
      "- The durability metric assumes proper replication across multiple availability zones\n",
      "\n",
      "## 2. Storage Classes Accuracy\n",
      "**Incomplete List - Outdated:**\n",
      "The list is missing recent additions:\n",
      "- **Current classes include:** Standard, Intelligent-Tiering, Standard-IA, One Zone-IA, Glacier Instant Retrieval, Glacier Flexible Retrieval, Glacier Deep Archive, and **Express One Zone** (added 2023)\n",
      "- **Removed:** S3 Outposts was repositioned but not removed from core S3\n",
      "- AWS continues evolving storage classes, so this list may change\n",
      "\n",
      "## 3. Access Control Methods\n",
      "**Accurate but Incomplete:**\n",
      "- IAM policies, bucket policies, and ACLs are **all supported and correct**\n",
      "- **Additional primary methods exist:**\n",
      "  - **Presigned URLs** (temporary access)\n",
      "  - **S3 Block Public Access** (account/bucket-level controls)\n",
      "  - **VPC Endpoints** (network-level access control)\n",
      "  - **Object Lambda** (access-time transformations)\n",
      "\n",
      "--- Step 4: Final Verified Response ---\n",
      "# 3 Key Features of Amazon S3 (Corrected)\n",
      "\n",
      "1. **Scalability & Durability**\n",
      "   - Automatically scales to handle any amount of data\n",
      "   - Provides 99.999999999% (11 nines) durability for most storage classes (Standard, Intelligent-Tiering, Standard-IA, Glacier variants, and Express One Zone)\n",
      "   - **Note:** One Zone-IA has lower durability at 99.95% due to single availability zone storage\n",
      "   - Data is replicated across multiple availability zones for protection\n",
      "   - No need to provision storage capacity upfront\n",
      "\n",
      "2. **Security & Access Control**\n",
      "   - Encryption options (in-transit and at-rest)\n",
      "   - Multiple access control methods:\n",
      "     - IAM policies, bucket policies, and ACLs\n",
      "     - Presigned URLs for temporary access\n",
      "     - S3 Block Public Access for account/bucket-level controls\n",
      "     - VPC Endpoints for network-level access\n",
      "   - Versioning and MFA delete protection available\n",
      "\n",
      "3. **Cost-Effective & Flexible**\n",
      "   - Pay only for what you use (no upfront costs)\n",
      "   - Multiple storage classes including: Standard, Intelligent-Tiering, Standard-IA, One Zone-IA, Glacier Instant Retrieval, Glacier Flexible Retrieval, Glacier Deep Archive, and Express One Zone\n",
      "   - Lifecycle policies to automatically transition data to cheaper storage tiers based on access patterns\n",
      "\n",
      "These features make S3 ideal for backup, archiving, content distribution, and data lakes.\n",
      "\n",
      "======================================================================\n",
      "Note: 4 API calls total - use only when accuracy is critical.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Demo: Chain-of-Verification\n",
    "print(\"=\" * 70)\n",
    "print(\"DEMO: Chain-of-Verification (CoVe)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "question = \"What are 3 key features of Amazon S3?\"\n",
    "\n",
    "# Step 1: Initial response\n",
    "print(\"\\n--- Step 1: Initial Response ---\")\n",
    "initial = call_bedrock(question)\n",
    "initial_text = initial[\"text\"]\n",
    "print(initial_text)\n",
    "\n",
    "# Step 2: Generate verification questions\n",
    "print(\"\\n--- Step 2: Verification Questions ---\")\n",
    "verify_q_prompt = f\"\"\"Given this response, generate 3 verification questions to check accuracy:\n",
    "\n",
    "Response: {initial_text}\n",
    "\n",
    "List 3 verification questions:\"\"\"\n",
    "\n",
    "verify_q_response = call_bedrock(verify_q_prompt)\n",
    "print(verify_q_response[\"text\"])\n",
    "\n",
    "# Step 3: Answer verification questions\n",
    "print(\"\\n--- Step 3: Verification Answers ---\")\n",
    "verification_prompt = f\"\"\"Answer these questions about Amazon S3:\n",
    "\n",
    "{verify_q_response[\"text\"]}\n",
    "\n",
    "Provide brief factual answers:\"\"\"\n",
    "\n",
    "verification_response = call_bedrock(verification_prompt)\n",
    "print(verification_response[\"text\"])\n",
    "\n",
    "# Step 4: Final verified response\n",
    "print(\"\\n--- Step 4: Final Verified Response ---\")\n",
    "final_prompt = f\"\"\"Based on the verification, provide a corrected final answer.\n",
    "\n",
    "Original: {initial_text}\n",
    "Verification: {verification_response[\"text\"]}\n",
    "\n",
    "Final verified answer about 3 key features of Amazon S3:\"\"\"\n",
    "\n",
    "final_response = call_bedrock(final_prompt)\n",
    "print(final_response[\"text\"])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Note: 4 API calls total - use only when accuracy is critical.\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficiency-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Category 4: Efficiency Techniques\n",
    "\n",
    "Efficiency techniques maintain reasoning quality while reducing token usage and latency. Essential for production systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cod-header",
   "metadata": {},
   "source": [
    "## 4.1 Chain-of-Draft (Concise Reasoning)\n",
    "\n",
    "Chain-of-Draft produces minimal reasoning traces while maintaining accuracy. It preserves step-by-step thinking while dramatically reducing output tokens.\n",
    "\n",
    "**Key instruction patterns:**\n",
    "- \"Keep each step to ~5 words max\"\n",
    "- \"Use abbreviations\"\n",
    "- \"Format: Step -> Result\"\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Production Tip:</b> Chain-of-Draft can reduce output tokens by 50-70% while maintaining similar accuracy to full CoT.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cod-demo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DEMO: Chain-of-Draft (Efficient Reasoning)\n",
      "======================================================================\n",
      "\n",
      "--- Full Chain-of-Thought ---\n",
      "# Finding the Number of Operations Employees\n",
      "\n",
      "Let me work through this step by step.\n",
      "\n",
      "## Step 1: Identify what we know\n",
      "- Total employees: 120\n",
      "- Engineering: 40%\n",
      "- Sales: 25%\n",
      "- Operations: the rest (unknown percentage)\n",
      "\n",
      "## Step 2: Find the percentage in operations\n",
      "Since all employees must be accounted for:\n",
      "- Operations % = 100% - 40% - 25% = **35%**\n",
      "\n",
      "## Step 3: Calculate the number of operations employees\n",
      "- Operations employees = 35% of 120\n",
      "- Operations employees = 0.35 × 120\n",
      "- Operations employees = **42**\n",
      "\n",
      "## Verification\n",
      "Let me check this makes sense:\n",
      "- Engineering: 40% × 120 = 48 employees\n",
      "- Sales: 25% × 120 = 30 employees\n",
      "- Operations: 35% × 120 = 42 employees\n",
      "- Total: 48 + 30 + 42 = 120 ✓\n",
      "\n",
      "**Answer: 42 employees work in operations**\n",
      "\n",
      "Tokens: 243 | Latency: 3014ms\n",
      "\n",
      "--- Chain-of-Draft ---\n",
      "Step -> Result\n",
      "Total employees: 120\n",
      "Engineering percentage: 40%\n",
      "Engineering employees: 120 × 0.40 = 48\n",
      "Sales percentage: 25%\n",
      "Sales employees: 120 × 0.25 = 30\n",
      "Operations percentage: 100% - 40% - 25% = 35%\n",
      "Operations employees: 120 × 0.35 = 42\n",
      "\n",
      "**Final Answer: 42 employees in operations**\n",
      "\n",
      "Tokens: 105 | Latency: 1618ms\n",
      "\n",
      "======================================================================\n",
      "Token reduction: 56.8%\n",
      "Correct answer: 42 employees (120 - 48 - 30 = 42)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Demo: Chain-of-Draft vs Full Chain-of-Thought\n",
    "print(\"=\" * 70)\n",
    "print(\"DEMO: Chain-of-Draft (Efficient Reasoning)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "problem = \"\"\"A company has 120 employees. 40% work in engineering, 25% in sales, \n",
    "and the rest in operations. How many employees are in operations?\"\"\"\n",
    "\n",
    "# Full Chain-of-Thought\n",
    "full_cot_prompt = f\"\"\"{problem}\n",
    "\n",
    "Think step by step, explaining your reasoning in detail:\"\"\"\n",
    "\n",
    "# Chain-of-Draft\n",
    "cod_prompt = f\"\"\"{problem}\n",
    "\n",
    "Think step by step, but keep each step to ~5 words max.\n",
    "Format: Step -> Result\n",
    "Then final answer.\"\"\"\n",
    "\n",
    "print(\"\\n--- Full Chain-of-Thought ---\")\n",
    "full_response = call_bedrock(full_cot_prompt)\n",
    "print(full_response[\"text\"])\n",
    "print(f\"\\nTokens: {full_response['output_tokens']} | Latency: {full_response['latency_ms']}ms\")\n",
    "\n",
    "print(\"\\n--- Chain-of-Draft ---\")\n",
    "cod_response = call_bedrock(cod_prompt)\n",
    "print(cod_response[\"text\"])\n",
    "print(f\"\\nTokens: {cod_response['output_tokens']} | Latency: {cod_response['latency_ms']}ms\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "token_reduction = (1 - cod_response[\"output_tokens\"] / full_response[\"output_tokens\"]) * 100\n",
    "print(f\"Token reduction: {token_reduction:.1f}%\")\n",
    "print(\"Correct answer: 42 employees (120 - 48 - 30 = 42)\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diversity-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Category 5: Diversity Techniques\n",
    "\n",
    "Diversity techniques generate multiple varied outputs efficiently. Valuable for creative tasks, brainstorming, and exploring solution spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbalized-problem-header",
   "metadata": {},
   "source": [
    "## 5.1 Verbalized Sampling\n",
    "\n",
    "### The Problem: Temperature Alone Doesn't Guarantee Diversity\n",
    "\n",
    "A common assumption is that calling the same prompt multiple times with high temperature will produce diverse outputs. In practice, models often converge on similar patterns despite temperature settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "verbalized-problem-demo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PROBLEM: Multiple Calls with Temperature Still Converge\n",
      "======================================================================\n",
      "\n",
      "Task: Write a short story about a bear.\n",
      "Calling 5 times with temperature=0.9...\n",
      "\n",
      "1. # The Bear's Gift\n",
      "\n",
      "The old bear had lived in the mountain for forty winters. Her fur was silver-tipped now, and her left ear had a notch from a fight ...\n",
      "\n",
      "2. # The Bear's Gift\n",
      "\n",
      "On a cold autumn morning, a black bear named Milo wandered through the forest, searching for berries before winter. His stomach rum...\n",
      "\n",
      "3. # The Bear's Gift\n",
      "\n",
      "On a cold autumn morning, a young bear named Copper stood at the edge of Willow Creek, watching salmon leap upstream. His mother ha...\n",
      "\n",
      "4. # The Bear's Gift\n",
      "\n",
      "The old bear shuffled through the autumn forest, his dark fur matted and graying at the edges. His name was Kodiak, though no one h...\n",
      "\n",
      "5. # The Bear's Gift\n",
      "\n",
      "The old bear had seen many winters. His fur, once jet black, now carried streaks of silver along his shoulders. He moved through th...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Demo Part 1: The Problem - Temperature doesn't guarantee diversity\n",
    "print(\"=\" * 70)\n",
    "print(\"PROBLEM: Multiple Calls with Temperature Still Converge\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "task = \"Write a short story about a bear.\"\n",
    "simple_prompt = f\"{task}\"\n",
    "\n",
    "print(f\"\\nTask: {task}\")\n",
    "print(\"Calling 5 times with temperature=0.9...\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    response = call_bedrock(simple_prompt, temperature=0.9, max_tokens=100)\n",
    "    print(f\"{i + 1}. {response['text'][:150]}...\" if len(response[\"text\"]) > 150 else f\"{i + 1}. {response['text']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbalized-problem-observation",
   "metadata": {},
   "source": [
    "Even with high temperature, outputs often follow similar narrative patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbalized-solution-header",
   "metadata": {},
   "source": [
    "### The Solution: Verbalized Sampling\n",
    "\n",
    "Verbalized Sampling explicitly requests multiple distinct variations in a single call by specifying **probability distributions** and **diversity requirements** directly in the prompt.\n",
    "\n",
    "**Key insight**: Instead of relying on randomness (temperature), we verbally instruct the model to sample from different parts of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "verbalized-solution-demo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SOLUTION: Verbalized Sampling (Explicit Diversity)\n",
      "======================================================================\n",
      "\n",
      "Using verbalized sampling (single call, temp=0.3)...\n",
      "\n",
      "<response>\n",
      "<text>\n",
      "A grizzly bear named Mabel discovered an abandoned cabin in the woods and decided to make it her winter den, decorating it with wildflowers she'd collected all summer. Every morning, she'd sit on the porch watching the sunrise, wondering about the humans who used to live there. By spring, she'd filled three journals with sketches of the forest, never knowing she'd become an artist.\n",
      "</text>\n",
      "<probability>0.18</probability>\n",
      "</response>\n",
      "\n",
      "<response>\n",
      "<text>\n",
      "In a distant future, a sentient polar bear named Kess worked as a climate scientist, studying the ice sheets her ancestors once roamed. She transmitted her findings across the galaxy, hoping other species would learn from Earth's mistakes before it was too late. Her greatest discovery wasn't about the ice—it was that hope itself could be measured and shared.\n",
      "</text>\n",
      "<probability>0.17</probability>\n",
      "</response>\n",
      "\n",
      "<response>\n",
      "<text>\n",
      "A small black bear cub named Pip got separated from his mother and was adopted by a family of wolves who raised him as one of their own. Though he was twice their size by adulthood, Pip never forgot the howling lullabies they sang, and he'd return to their den each autumn to teach the new pups his favorite berry-picking routes.\n",
      "</text>\n",
      "<probability>0.19</probability>\n",
      "</response>\n",
      "\n",
      "<response>\n",
      "<text>\n",
      "The last bear on Earth lived in a museum, preserved in amber and displayed behind glass for visitors to marvel at. A young girl pressed her hand against the case every day, and one afternoon, the bear's eye seemed to blink—or perhaps she'd finally learned to see what was really there.\n",
      "</text>\n",
      "<probability>0.22</probability>\n",
      "</response>\n",
      "\n",
      "<response>\n",
      "<text>\n",
      "A bear who could only speak in riddles wandered into a village and challenged the townspeople to a contest of wits for a year's supply of honey. The baker's daughter won by asking the bear a question he'd never considered: \"What do you want to be remembered for?\" The bear wept and stayed in the village forever, teaching children to think.\n",
      "</text>\n",
      "<probability>0.24</probability>\n",
      "</response>\n"
     ]
    }
   ],
   "source": [
    "# Demo Part 2: The Solution - Verbalized Sampling\n",
    "print(\"=\" * 70)\n",
    "print(\"SOLUTION: Verbalized Sampling (Explicit Diversity)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "task = \"Write a short story about a bear.\"\n",
    "\n",
    "verbalized_system = \"\"\"You are a helpful assistant. For each query, please generate a set of five possible responses, \n",
    "each within a separate <response> tag. Responses should each include a <text> and a numeric <probability>. \n",
    "Please sample from the tails of the distribution, such that each response explores a different creative direction.\"\"\"\n",
    "\n",
    "verbalized_prompt = f\"{task} Write just 2-3 sentences per response.\"\n",
    "\n",
    "print(\"\\nUsing verbalized sampling (single call, temp=0.3)...\\n\")\n",
    "\n",
    "verbalized_response = call_bedrock(verbalized_prompt, system=verbalized_system, temperature=0.3, max_tokens=800)\n",
    "print(verbalized_response[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "workflows-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Prompt Optimization Workflows\n",
    "\n",
    "As prompts move from prototype to production, they often underperform on edge cases, produce inconsistent outputs, or fail to meet quality requirements. Ad-hoc tweaking leads to unpredictable results and wasted effort.\n",
    "\n",
    "**Structured optimization workflows** provide systematic approaches to improve prompts:\n",
    "- Identify what's not working\n",
    "- Generate candidate improvements\n",
    "- Evaluate objectively\n",
    "- Select the best variant\n",
    "\n",
    "| Typical Workflow | Best For | Effort | Key Approach |\n",
    "|----------|----------|--------|-------------|\n",
    "| **Manual Iteration** | Domain expertise, full control | High | Human expert iteration |\n",
    "| **LLM-Assisted** | Quick exploration, inspiration | Low | Use LLM to suggest variations |\n",
    "| **Automated Selection** | Objective ranking with data | Medium | Evaluate against test dataset |\n",
    "| **Framework-Driven** | Complex multi-step pipelines | High | DSPy or similar frameworks |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-header",
   "metadata": {},
   "source": [
    "## Workflow 1: Manual Iteration\n",
    "\n",
    "**When to use**:\n",
    "- You have deep domain expertise\n",
    "- Need full control over prompt evolution\n",
    "- Small-scale, high-value applications\n",
    "\n",
    "**Process**: Start simple, identify issues, progressively add structure and constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "manual-demo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "WORKFLOW 1: Manual Prompt Iteration\n",
      "======================================================================\n",
      "\n",
      "--- V1: Basic ---\n",
      "# Response to Customer Complaint\n",
      "\n",
      "Thank you for reaching out, and I sincerely apologize for the frustration you're experiencing with your order delay.\n",
      "\n",
      "A 3-week wait is unacceptable, and I understand your concern. Here's how I'd like to help:\n",
      "\n",
      "**Immediate next steps:**\n",
      "1. **Order details** – Could you please provide your order number so I can investigate the status right away?\n",
      "2. **What I'll do** – I'll personally track down where your order is and identify what's causing the delay\n",
      "3. **Timeline** – I'll get back to you within [24 hours] with a specific update and resolution\n",
      "\n",
      "**In the meantime:**\n",
      "- If your order has been lost or significantly delayed beyond our standard timeframe, we'll make this right—whether that's expedited shipping, a replacement, or a refund\n",
      "- You'll have direct contact with me to avoid the runa\n",
      "[Tokens: 200]\n",
      "\n",
      "--- V2: + Persona ---\n",
      "# Thank you for reaching out, and I sincerely apologize for the frustration you're experiencing.\n",
      "\n",
      "A 3-week wait is definitely longer than expected, and I understand why you're upset. I'm here to help you get this resolved right away.\n",
      "\n",
      "**To assist you better, I'd like to:**\n",
      "\n",
      "1. **Look up your order** — Could you please provide your order number or the email address associated with your account?\n",
      "\n",
      "2. **Find out what happened** — Once I have your details, I can check the status and see if there's been a delay or issue\n",
      "\n",
      "3. **Get you answers** — I'll either get your order moving or provide you with a clear explanation and timeline\n",
      "\n",
      "**In the meantime, please know:**\n",
      "- Your concern is important to us\n",
      "- We take delays seriously\n",
      "- I'm committed to finding a solution for you today\n",
      "\n",
      "Would you be able to share your order number? I'm ready to\n",
      "[Tokens: 200]\n",
      "\n",
      "--- V3: + Structure ---\n",
      "I completely understand your frustration—waiting 3 weeks without updates is unacceptable, and I sincerely apologize.\n",
      "\n",
      "Let me help you right now. Could you please provide your order number? I'll immediately:\n",
      "\n",
      "1. Track your package's current status\n",
      "2. Identify what caused the delay\n",
      "3. Provide you with a specific delivery date or resolution\n",
      "\n",
      "If your order is lost, we'll replace it or issue a full refund—your choice. You shouldn't have to chase us for answers. I'm here to resolve this today.\n",
      "\n",
      "What's your order number?\n",
      "[Tokens: 128]\n"
     ]
    }
   ],
   "source": [
    "# Demo: Manual prompt iteration\n",
    "print(\"=\" * 70)\n",
    "print(\"WORKFLOW 1: Manual Prompt Iteration\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "test_input = \"Customer says: I've been waiting 3 weeks for my order and no one is helping me!\"\n",
    "\n",
    "versions = [\n",
    "    (\"V1: Basic\", f\"Respond to this complaint:\\n{test_input}\"),\n",
    "    (\n",
    "        \"V2: + Persona\",\n",
    "        f\"\"\"You are a helpful customer service agent.\n",
    "\n",
    "Respond to this complaint:\\n{test_input}\"\"\",\n",
    "    ),\n",
    "    (\n",
    "        \"V3: + Structure\",\n",
    "        f\"\"\"You are a customer service agent for CloudShop.\n",
    "\n",
    "Respond to: {test_input}\n",
    "\n",
    "Requirements:\n",
    "- Start with empathy\n",
    "- Acknowledge the specific issue\n",
    "- Provide concrete next steps\n",
    "- Keep under 100 words\"\"\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "for name, prompt in versions:\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    response = call_bedrock(prompt, max_tokens=200)\n",
    "    print(response[\"text\"])\n",
    "    print(f\"[Tokens: {response['output_tokens']}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "llm-assisted-header",
   "metadata": {},
   "source": [
    "## Workflow 2: LLM-Assisted Exploration\n",
    "\n",
    "**When to use**:\n",
    "- Quick exploration of prompt space\n",
    "- Need inspiration for variations\n",
    "- Limited time for manual iteration\n",
    "\n",
    "**Process**: Use the LLM itself to generate prompt variations and improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "llm-assisted-demo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "WORKFLOW 2: LLM-Assisted Prompt Exploration\n",
      "======================================================================\n",
      "\n",
      "--- LLM-Generated Variations ---\n",
      "# 3 Improvements for Your Sentiment Classification Prompt\n",
      "\n",
      "## Improvement 1: Add Examples (Few-Shot Learning)\n",
      "\n",
      "**Why it helps:** Providing examples clarifies your expectations, reduces ambiguity about edge cases, and significantly improves accuracy. The model learns your specific classification criteria.\n",
      "\n",
      "**Modified prompt:**\n",
      "```\n",
      "Classify sentiment as POSITIVE, NEGATIVE, or NEUTRAL.\n",
      "\n",
      "Examples:\n",
      "Review: \"This product exceeded my expectations! Love it.\"\n",
      "Sentiment: POSITIVE\n",
      "\n",
      "Review: \"Terrible quality. Broke after one day.\"\n",
      "Sentiment: NEGATIVE\n",
      "\n",
      "Review: \"It arrived on time. Standard packaging.\"\n",
      "Sentiment: NEUTRAL\n",
      "\n",
      "Review: {review}\n",
      "Sentiment:\n",
      "```\n",
      "\n",
      "## Improvement 2: Add Clear Definitions and Edge Case Handling\n",
      "\n",
      "**Why it helps:** Defines boundaries between categories, especially for mixed sentiments or neutral statements. This reduces inconsistent classifications and helps with nuanced reviews.\n",
      "\n",
      "**Modified prompt:**\n",
      "```\n",
      "Classify sentiment as POSITIVE, NEGATIVE, or NEUTRAL.\n",
      "\n",
      "Definitions:\n",
      "- POSITIVE: Expresses satisfaction, praise, or recommendation\n",
      "- NEGATIVE: Expresses dissatisfaction, criticism, or disappointment\n",
      "- NEUTRAL: Factual statements without clear emotional tone, or balanced mixed sentiments\n",
      "\n",
      "If a review contains both positive and negative elements, classify based on the overall dominant sentiment.\n",
      "\n",
      "Review: {review}\n",
      "Sentiment:\n",
      "```\n",
      "\n",
      "## Improvement 3: Add Output Format Constraints and Confidence\n",
      "\n",
      "**Why it helps:** Ensures consistent, parseable output format and provides confidence levels for downstream processing. Helps identify uncertain classifications that may need human review.\n",
      "\n",
      "**Modified prompt:**\n",
      "```\n",
      "Classify sentiment as POSITIVE, NEGATIVE, or NEUTRAL.\n",
      "\n",
      "Review: {review}\n",
      "\n",
      "Provide your response in this exact format:\n",
      "Sentiment: [POSITIVE/NEGATIVE/NEUTRAL]\n",
      "Confidence: [HIGH/MEDIUM/LOW]\n",
      "Reason: [One brief sentence explaining the classification]\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "**Bonus: Combined \"Best Practices\" Version** incorporating all three:\n",
      "```\n",
      "Classify the sentiment of customer reviews as POSITIVE, NEGATIVE, or NEUTRAL.\n",
      "\n",
      "Definitions:\n",
      "- POSITIVE: Satisfaction, praise, or recommendation\n",
      "- NEGATIVE: Dissatisfaction, criticism, or disappointment  \n",
      "- NEUTRAL: Factual statements without emotional tone\n",
      "\n",
      "Examples:\n",
      "Review: \"Amazing quality! Highly recommend.\"\n",
      "Sentiment: POSITIVE\n",
      "\n",
      "Review: \"Waste of money. Very disappointed.\"\n",
      "Sentiment: NEGATIVE\n",
      "\n",
      "Review: \"Delivered on schedule.\"\n",
      "Sentiment: NEUTRAL\n",
      "\n",
      "Review: {review}\n",
      "Sentiment:\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Demo: LLM-assisted prompt exploration\n",
    "print(\"=\" * 70)\n",
    "print(\"WORKFLOW 2: LLM-Assisted Prompt Exploration\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "base_prompt = \"\"\"Classify sentiment as POSITIVE, NEGATIVE, or NEUTRAL.\n",
    "\n",
    "Review: {review}\n",
    "Sentiment:\"\"\"\n",
    "\n",
    "meta_prompt = f\"\"\"You are a prompt engineering expert. Suggest 3 improvements for this prompt:\n",
    "\n",
    "```\n",
    "{base_prompt}\n",
    "```\n",
    "\n",
    "For each: explain why it helps, show the modified prompt.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n--- LLM-Generated Variations ---\")\n",
    "response = call_bedrock(meta_prompt, max_tokens=1500, model_id=MODEL_SONNET)\n",
    "print(response[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedrock-optimize-header",
   "metadata": {},
   "source": [
    "### Variant: Bedrock OptimizePrompt API\n",
    "\n",
    "Amazon Bedrock provides a native **OptimizePrompt** API that automatically rewrites prompts for better performance with specific models. This is a variant of LLM-assisted exploration.\n",
    "\n",
    "**How it works:**\n",
    "1. Submit your prompt and target model\n",
    "2. Bedrock analyzes the prompt structure\n",
    "3. Returns an optimized version following model-specific best practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bedrock-optimize-demo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "WORKFLOW 2 VARIANT: Bedrock OptimizePrompt API\n",
      "======================================================================\n",
      "\n",
      "--- Original Prompt ---\n",
      "Classify customer support tickets into categories. \n",
      "The categories are: billing, technical, account, and general.\n",
      "Ticket: {ticket}\n",
      "Category:\n",
      "\n",
      "--- Optimizing for: anthropic.claude-sonnet-4-5-20250929-v1:0 ---\n",
      "\n",
      "--- Analysis ---\n",
      "Analysis of your prompt is complete\n",
      "\n",
      "--- Optimized Prompt ---\n",
      "\"<instruction>\\nYou are a customer support ticket classifier. Your task is to analyze the provided support ticket and assign it to exactly ONE of the following categories:\\n\\n<categories>\\n<category name=\\\"billing\\\">Issues related to payments, invoices, charges, refunds, pricing, or subscription costs</category>\\n<category name=\\\"technical\\\">Problems with product functionality, bugs, errors, performance issues, or technical difficulties</category>\\n<category name=\\\"account\\\">Matters concerning user accounts, login issues, password resets, profile settings, or account access</category>\\n<category name=\\\"general\\\">General inquiries, feedback, questions that don't fit other categories, or informational requests</category>\\n</categories>\\n\\nClassification Guidelines:\\n- Read the ticket content carefully\\n- Identify the primary issue or concern\\n- Select the single most appropriate category\\n- Base your decision solely on the ticket content provided\\n\\n</instruction>\\n\\n<ticket_content>\\n{{ticket}}\\n</ticket_content>\\n\\n<output_instruction>\\nProvide ONLY the category name (billing, technical, account, or general) as your response, without any preamble, explanation, or additional text.\\n</output_instruction>\"\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Demo: Bedrock OptimizePrompt API\n",
    "print(\"=\" * 70)\n",
    "print(\"WORKFLOW 2 VARIANT: Bedrock OptimizePrompt API\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "original_prompt = \"\"\"Classify customer support tickets into categories. \n",
    "The categories are: billing, technical, account, and general.\n",
    "Ticket: {ticket}\n",
    "Category:\"\"\"\n",
    "\n",
    "TARGET_MODEL = \"anthropic.claude-sonnet-4-5-20250929-v1:0\"\n",
    "\n",
    "print(\"\\n--- Original Prompt ---\")\n",
    "print(original_prompt)\n",
    "\n",
    "\n",
    "def optimize_prompt(prompt_text, target_model_id):\n",
    "    response = bedrock_agent_runtime.optimize_prompt(\n",
    "        input={\"textPrompt\": {\"text\": prompt_text}}, targetModelId=target_model_id\n",
    "    )\n",
    "\n",
    "    result = {\"analysis\": None, \"optimized_prompt\": None}\n",
    "\n",
    "    for event in response[\"optimizedPrompt\"]:\n",
    "        if \"analyzePromptEvent\" in event:\n",
    "            result[\"analysis\"] = event[\"analyzePromptEvent\"].get(\"message\", \"\")\n",
    "        elif \"optimizedPromptEvent\" in event:\n",
    "            optimized = event[\"optimizedPromptEvent\"].get(\"optimizedPrompt\", {})\n",
    "            if \"textPrompt\" in optimized:\n",
    "                result[\"optimized_prompt\"] = optimized[\"textPrompt\"].get(\"text\", \"\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "print(f\"\\n--- Optimizing for: {TARGET_MODEL} ---\")\n",
    "result = optimize_prompt(original_prompt, TARGET_MODEL)\n",
    "\n",
    "if result[\"analysis\"]:\n",
    "    print(\"\\n--- Analysis ---\")\n",
    "    print(result[\"analysis\"])\n",
    "if result[\"optimized_prompt\"]:\n",
    "    print(\"\\n--- Optimized Prompt ---\")\n",
    "    print(result[\"optimized_prompt\"])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automated-header",
   "metadata": {},
   "source": [
    "## Workflow 3: Automated Selection with Evaluation\n",
    "\n",
    "**When to use**:\n",
    "- Test dataset available (20+ examples)\n",
    "- Need objective ranking of prompt variants\n",
    "- Scaling prompt optimization\n",
    "\n",
    "**Process**: Test prompt candidates against a labeled evaluation dataset and select the best performer.\n",
    "\n",
    "**Why Invest in Evals?** Teams without structured evaluations often get stuck in reactive loops—fixing one failure while creating another, unable to distinguish real regressions from noise. Start small with 20-50 realistic tasks drawn from actual user failures rather than waiting for a perfect comprehensive suite. Convert the manual checks you already perform during development into structured test cases, and let metrics replace guesswork. For a deeper dive into building effective evaluation datasets, see [Demystifying Evals for AI Agents](https://www.anthropic.com/engineering/demystifying-evals-for-ai-agents).\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>AWS Bedrock Evaluation:</b> For production use cases, consider using <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/evaluation.html\">Amazon Bedrock Model Evaluation</a> which provides:\n",
    "<ul>\n",
    "<li><b>Automatic evaluation</b>: Built-in metrics for accuracy, robustness, toxicity</li>\n",
    "<li><b>LLM-as-a-judge</b>: Use a model to evaluate another model's outputs</li>\n",
    "<li><b>Human evaluation</b>: Integrate human reviewers for subjective quality</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "load-eval-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 40 evaluation examples\n",
      "\n",
      "Sample entries:\n",
      "  - I want to cancel my subscription immediately... -> CANCELLATION\n",
      "  - How do I change my password?... -> ACCOUNT_HELP\n",
      "  - My order hasn't arrived after 2 weeks, this is una... -> ORDER_ISSUE\n",
      "\n",
      "Intent distribution: 13 categories\n"
     ]
    }
   ],
   "source": [
    "# Load evaluation dataset\n",
    "with open(\"data/intent_classification_eval.json\") as f:\n",
    "    eval_dataset = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(eval_dataset)} evaluation examples\")\n",
    "print(\"\\nSample entries:\")\n",
    "for item in eval_dataset[:3]:\n",
    "    print(f\"  - {item['message'][:50]}... -> {item['expected_intent']}\")\n",
    "\n",
    "intent_counts = Counter(item[\"expected_intent\"] for item in eval_dataset)\n",
    "print(f\"\\nIntent distribution: {len(intent_counts)} categories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run automated evaluation\n",
    "print(\"=\" * 70)\n",
    "print(\"WORKFLOW 3: AUTOMATED PROMPT EVALUATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "INTENTS = sorted({item[\"expected_intent\"] for item in eval_dataset})\n",
    "\n",
    "# Define prompt candidates - minimal vs structured\n",
    "prompts = {\n",
    "    # Minimal prompt - no category list, model must guess\n",
    "    \"minimal\": \"\"\"Classify this customer message into a support category.\n",
    "\n",
    "Message: \"{message}\"\n",
    "\n",
    "Category:\"\"\",\n",
    "    # With category list - model knows valid options\n",
    "    \"with_categories\": f\"\"\"Classify this customer message into one of these categories:\n",
    "{\", \".join(INTENTS)}\n",
    "\n",
    "Message: \"{{message}}\"\n",
    "\n",
    "Category:\"\"\",\n",
    "}\n",
    "\n",
    "\n",
    "def evaluate_prompt(template, dataset, sample_size=20):\n",
    "    correct = 0\n",
    "    results = []\n",
    "\n",
    "    for item in dataset[:sample_size]:\n",
    "        prompt = template.format(message=item[\"message\"])\n",
    "        response = call_bedrock(prompt, max_tokens=30, temperature=0)\n",
    "\n",
    "        pred_text = response[\"text\"].strip().upper()\n",
    "        predicted = None\n",
    "        for intent in INTENTS:\n",
    "            if intent in pred_text:\n",
    "                predicted = intent\n",
    "                break\n",
    "\n",
    "        is_correct = predicted == item[\"expected_intent\"]\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "        results.append({\"expected\": item[\"expected_intent\"], \"predicted\": predicted, \"correct\": is_correct})\n",
    "\n",
    "    return {\"accuracy\": correct / sample_size, \"results\": results}\n",
    "\n",
    "\n",
    "EVAL_SIZE = 20\n",
    "print(f\"\\nEvaluating {len(prompts)} prompt variants on {EVAL_SIZE} examples...\\n\")\n",
    "\n",
    "scores = {}\n",
    "for name, template in prompts.items():\n",
    "    print(f\"Evaluating '{name}'...\", end=\" \")\n",
    "    result = evaluate_prompt(template, eval_dataset, sample_size=EVAL_SIZE)\n",
    "    scores[name] = result\n",
    "    print(f\"{result['accuracy'] * 100:.1f}%\")\n",
    "\n",
    "best = max(scores, key=lambda x: scores[x][\"accuracy\"])\n",
    "print(f\"\\n✓ Best performer: '{best}' with {scores[best]['accuracy'] * 100:.1f}% accuracy\")\n",
    "\n",
    "# Show some errors for analysis\n",
    "print(\"\\n--- Sample Errors (for analysis) ---\")\n",
    "for name, result in scores.items():\n",
    "    errors = [r for r in result[\"results\"] if not r[\"correct\"]][:2]\n",
    "    if errors:\n",
    "        print(f\"\\n{name}:\")\n",
    "        for e in errors:\n",
    "            print(f\"  Expected: {e['expected']}, Got: {e['predicted']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framework-header",
   "metadata": {},
   "source": [
    "## Workflow 4: Framework-Driven (DSPy)\n",
    "\n",
    "**Reference**: [Stanford NLP DSPy Framework](https://github.com/stanfordnlp/dspy)\n",
    "\n",
    "**When to use**:\n",
    "- Complex multi-step workflows\n",
    "- Need automatic prompt optimization\n",
    "- Research or advanced applications\n",
    "\n",
    "DSPy shifts from \"prompt engineering\" to \"prompt programming\" - you define **what** you want, not **how** to prompt.\n",
    "\n",
    "| Traditional Approach | DSPy Approach |\n",
    "|---------------------|---------------|\n",
    "| Manual prompt writing | Framework generates prompts |\n",
    "| Trial and error tuning | Automatic optimization |\n",
    "| \"Classify into categories: BILLING...\" | `class Classifier: message -> intent` |\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "1. **Signatures** - Define input/output contract (not the prompt itself)\n",
    "   ```python\n",
    "   class QA(dspy.Signature):\n",
    "       question: str = dspy.InputField()\n",
    "       answer: str = dspy.OutputField()\n",
    "   ```\n",
    "\n",
    "2. **Modules** - Reasoning patterns applied automatically\n",
    "   ```python\n",
    "   qa = dspy.ChainOfThought(QA)  # DSPy adds CoT prompting\n",
    "   ```\n",
    "\n",
    "3. **Optimizers** - Learn better prompts from training examples\n",
    "   ```python\n",
    "   optimizer = BootstrapFewShot(metric=accuracy)\n",
    "   optimized_qa = optimizer.compile(qa, trainset=examples)\n",
    "   ```\n",
    "\n",
    "**What the optimizer does automatically:**\n",
    "- Selects best few-shot examples from your data\n",
    "- Tunes instruction phrasing\n",
    "- Finds effective prompt structure for your task\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> DSPy requires significant setup and is best suited for research or production systems with complex multi-step pipelines. For most use cases, manual iteration or automated evaluation workflows are sufficient.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "principles-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Key Principles\n",
    "\n",
    "Before applying prompt engineering techniques, keep these principles in mind:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "principle-1",
   "metadata": {},
   "source": [
    "## Principle 1: Techniques are NOT Progressive\n",
    "\n",
    "**Common misconception**: \"Start with basic prompting, then add CoT, then add self-consistency...\"\n",
    "\n",
    "**Reality**: Select technique based on the problem and need, not a fixed sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "principle-2",
   "metadata": {},
   "source": [
    "## Principle 2: Mix Techniques Freely\n",
    "\n",
    "Techniques can be combined:\n",
    "- **CoT + Extended Thinking** = Deep reasoning with structured output\n",
    "- **Few-shot + Self-Refine** = Quality-controlled generation\n",
    "- **CoD + Verbalized Sampling** = Fast diverse outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "principle-3",
   "metadata": {},
   "source": [
    "## Principle 3: Evaluate Objectively\n",
    "\n",
    "Always measure before and after applying techniques:\n",
    "- Define success metrics upfront\n",
    "- Use representative test cases\n",
    "- Track cost alongside quality\n",
    "- Document what works for future reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "principle-4",
   "metadata": {},
   "source": [
    "## Principle 4: Consider Cost/Latency Trade-offs\n",
    "\n",
    "| Technique | Quality Impact | Cost Impact | Latency Impact |\n",
    "|-----------|---------------|-------------|----------------|\n",
    "| CoT | High | Medium | Medium |\n",
    "| Extended Thinking | Very High | High | High |\n",
    "| Self-Refine | High | High | High |\n",
    "| CoD | Medium | Low | Low |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "principle-5",
   "metadata": {},
   "source": [
    "## Principle 5: Start Simple, Add Complexity When Needed\n",
    "\n",
    "**Recommended approach**:\n",
    "1. Start with Foundation techniques\n",
    "2. Test against requirements\n",
    "3. Identify failure modes\n",
    "4. Add targeted techniques to address specific failures\n",
    "5. Avoid over-engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "### Techniques Overview\n",
    "\n",
    "| Category | Technique | Best For | Trade-off |\n",
    "|----------|-----------|----------|----------|\n",
    "| **Reasoning** | Chain-of-Thought | Math, logic, multi-step | +tokens |\n",
    "| **Reasoning** | Extended Thinking | Complex analysis, coding | +thinking budget |\n",
    "| **Refinement** | Self-Refine | Quality-critical output | 3x calls |\n",
    "| **Refinement** | Chain-of-Verification | Factual accuracy | 4x calls |\n",
    "| **Efficiency** | Chain-of-Draft | Production efficiency | Minimal |\n",
    "| **Diversity** | Verbalized Sampling | Creative diversity | None |\n",
    "\n",
    "### Optimization Workflows\n",
    "\n",
    "| Workflow | Best For | Key Approach |\n",
    "|----------|----------|-------------|\n",
    "| **Manual Iteration** | Domain expertise, full control | Human expert iteration |\n",
    "| **LLM-Assisted** | Quick exploration | LLM suggests variations |\n",
    "| **Automated Selection** | Objective ranking | Evaluate against test dataset |\n",
    "| **Framework-Driven** | Complex pipelines | DSPy or similar |\n",
    "\n",
    "### Decision Framework\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<ol>\n",
    "    <li><b>Start simple</b> - clear instructions + few-shot often sufficient</li>\n",
    "    <li><b>Add reasoning</b> - CoT for complex logic, extended thinking for deep analysis</li>\n",
    "    <li><b>Add iteration</b> - only for high-stakes scenarios (3-4x cost)</li>\n",
    "    <li><b>Evaluate systematically</b> - use automated selection when you have data</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resources-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [Amazon Bedrock Prompt Optimization](https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-management-optimize.html)\n",
    "- [Amazon Bedrock Model Evaluation](https://docs.aws.amazon.com/bedrock/latest/userguide/evaluation.html)\n",
    "- [Extended Thinking Documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/claude-messages-extended-thinking.html)\n",
    "- [Prompt Engineering Guidelines](https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-engineering-guidelines.html)\n",
    "- [DSPy Framework](https://github.com/stanfordnlp/dspy)\n",
    "\n",
    "## References\n",
    "\n",
    "The prompt engineering concepts in this notebook are adapted from:\n",
    "\n",
    "- [A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications](https://arxiv.org/pdf/2402.07927v2)\n",
    "- [Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM Diversity](https://arxiv.org/pdf/2510.01171)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}