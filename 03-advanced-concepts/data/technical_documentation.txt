AnyCompany DataPipeline API Reference Guide
Version 4.1 | Enterprise Data Pipeline Platform

================================================================================
CHAPTER 1: INTRODUCTION
================================================================================

AnyCompany DataPipeline is an enterprise-grade data pipeline platform designed
for real-time and batch data processing at scale. This guide provides
comprehensive documentation for integrating with the DataPipeline API.

1.1 Key Capabilities
- Real-time streaming data ingestion (up to 1M events/second)
- Batch processing for large-scale ETL workloads
- Built-in data transformation and enrichment
- Multi-cloud deployment (AWS, Azure, GCP)
- Enterprise security with encryption at rest and in transit

1.2 Architecture Overview
DataPipeline operates on a distributed architecture with three main components:
- Ingestion Layer: Receives data from various sources
- Processing Layer: Transforms and enriches data
- Storage Layer: Persists processed data to destinations

================================================================================
CHAPTER 2: AUTHENTICATION
================================================================================

2.1 API Key Authentication
All API requests require authentication via API keys. Include your API key
in the request header:

    Authorization: Bearer <your-api-key>

API keys can be generated from the DataPipeline Console under Settings > API Keys.
Each key has configurable permissions and rate limits.

2.2 OAuth 2.0 (Enterprise)
Enterprise customers can use OAuth 2.0 for service-to-service authentication:

    POST /oauth/token
    Content-Type: application/x-www-form-urlencoded

    grant_type=client_credentials
    &client_id=<your-client-id>
    &client_secret=<your-client-secret>

Access tokens expire after 1 hour. Refresh tokens are valid for 30 days.

2.3 IP Whitelisting
For additional security, you can restrict API access to specific IP addresses
or CIDR ranges. Configure this in Console > Security > IP Allowlist.

================================================================================
CHAPTER 3: CORE API ENDPOINTS
================================================================================

3.1 Pipelines

GET /v1/pipelines
List all pipelines in your account.

Query Parameters:
- status (string): Filter by status (active, paused, failed)
- limit (integer): Max results per page (default: 20, max: 100)
- offset (integer): Pagination offset

Response:
{
  "pipelines": [
    {
      "id": "pipe_abc123",
      "name": "User Events Pipeline",
      "status": "active",
      "created_at": "2024-01-15T10:30:00Z",
      "throughput": "125000 events/min"
    }
  ],
  "total": 42,
  "has_more": true
}

POST /v1/pipelines
Create a new pipeline.

Request Body:
{
  "name": "My Pipeline",
  "source": {
    "type": "kafka",
    "config": {
      "brokers": ["kafka1:9092", "kafka2:9092"],
      "topic": "user-events",
      "consumer_group": "datapipeline-prod"
    }
  },
  "transformations": [...],
  "destination": {...}
}

GET /v1/pipelines/{id}
Retrieve details for a specific pipeline.

PUT /v1/pipelines/{id}
Update pipeline configuration. Pipeline must be paused before updates.

DELETE /v1/pipelines/{id}
Delete a pipeline. This action is irreversible.

3.2 Sources

Supported source types:
- kafka: Apache Kafka and Confluent Cloud
- kinesis: AWS Kinesis Data Streams
- pubsub: Google Cloud Pub/Sub
- eventhub: Azure Event Hubs
- http: HTTP/Webhook ingestion
- s3: AWS S3 bucket (batch)
- gcs: Google Cloud Storage (batch)
- database: JDBC-compatible databases

POST /v1/sources/test
Test connectivity to a source before creating a pipeline.

Request Body:
{
  "type": "kafka",
  "config": {
    "brokers": ["kafka1:9092"],
    "topic": "test-topic"
  }
}

Response:
{
  "success": true,
  "latency_ms": 45,
  "message": "Successfully connected to Kafka cluster"
}

3.3 Transformations

DataPipeline supports declarative transformations using SQL or JavaScript.

SQL Transformation Example:
{
  "type": "sql",
  "query": "SELECT user_id, event_type, timestamp FROM events WHERE event_type != 'heartbeat'"
}

JavaScript Transformation Example:
{
  "type": "javascript",
  "code": "function transform(event) { event.processed_at = Date.now(); return event; }"
}

Built-in transformation functions:
- PARSE_JSON(field): Parse JSON string into object
- FLATTEN(field): Flatten nested objects
- MASK(field, pattern): Mask sensitive data
- HASH(field, algorithm): Hash field values (SHA256, MD5)
- TIMESTAMP_CONVERT(field, from_format, to_format): Convert timestamps
- GEOCODE(lat, lon): Enrich with geographic data
- LOOKUP(field, table): Join with lookup table

3.4 Destinations

Supported destination types:
- snowflake: Snowflake Data Warehouse
- bigquery: Google BigQuery
- redshift: Amazon Redshift
- databricks: Databricks Lakehouse
- elasticsearch: Elasticsearch/OpenSearch
- s3: AWS S3 (Parquet, JSON, CSV)
- postgres: PostgreSQL databases
- kafka: Apache Kafka (for chaining pipelines)

POST /v1/destinations/test
Test connectivity to a destination.

================================================================================
CHAPTER 4: MONITORING AND OBSERVABILITY
================================================================================

4.1 Metrics API

GET /v1/metrics/pipelines/{id}
Retrieve metrics for a specific pipeline.

Query Parameters:
- start_time (ISO 8601): Start of time range
- end_time (ISO 8601): End of time range
- granularity (string): 1m, 5m, 15m, 1h, 1d

Response:
{
  "pipeline_id": "pipe_abc123",
  "metrics": {
    "events_processed": 1250000,
    "events_failed": 125,
    "avg_latency_ms": 45,
    "p99_latency_ms": 120,
    "throughput_per_second": 2500
  }
}

4.2 Alerts

POST /v1/alerts
Create an alert rule.

Request Body:
{
  "name": "High Error Rate",
  "pipeline_id": "pipe_abc123",
  "condition": {
    "metric": "error_rate",
    "operator": "gt",
    "threshold": 0.01
  },
  "notification": {
    "type": "slack",
    "channel": "#datapipeline-alerts"
  }
}

GET /v1/alerts/{id}/history
Retrieve alert trigger history.

4.3 Logs

GET /v1/logs/pipelines/{id}
Stream pipeline execution logs.

Query Parameters:
- level (string): debug, info, warn, error
- since (ISO 8601): Logs after this timestamp
- tail (integer): Last N log entries

================================================================================
CHAPTER 5: ERROR HANDLING
================================================================================

5.1 Error Response Format
All API errors follow a consistent format:

{
  "error": {
    "code": "PIPELINE_NOT_FOUND",
    "message": "Pipeline with ID 'pipe_xyz' does not exist",
    "details": {
      "pipeline_id": "pipe_xyz"
    },
    "request_id": "req_abc123"
  }
}

5.2 Common Error Codes
- AUTHENTICATION_FAILED: Invalid or expired API key
- AUTHORIZATION_DENIED: Insufficient permissions
- RATE_LIMIT_EXCEEDED: Too many requests
- PIPELINE_NOT_FOUND: Pipeline does not exist
- INVALID_CONFIGURATION: Invalid pipeline configuration
- SOURCE_CONNECTION_FAILED: Cannot connect to source
- DESTINATION_CONNECTION_FAILED: Cannot connect to destination
- TRANSFORMATION_ERROR: Error in transformation logic
- QUOTA_EXCEEDED: Account quota exceeded

5.3 Rate Limits
- Standard tier: 100 requests/minute
- Professional tier: 1,000 requests/minute
- Enterprise tier: 10,000 requests/minute

Rate limit headers are included in all responses:
- X-RateLimit-Limit: Your rate limit
- X-RateLimit-Remaining: Requests remaining
- X-RateLimit-Reset: Unix timestamp when limit resets

================================================================================
CHAPTER 6: BEST PRACTICES
================================================================================

6.1 Performance Optimization
- Use batch endpoints for bulk operations
- Implement exponential backoff for retries
- Cache pipeline configurations locally
- Use webhooks instead of polling for status updates

6.2 Security Recommendations
- Rotate API keys every 90 days
- Use least-privilege permissions
- Enable IP whitelisting for production environments
- Encrypt sensitive configuration values

6.3 Reliability Patterns
- Implement idempotent event processing
- Use dead letter queues for failed events
- Set up alerts for anomaly detection
- Maintain pipeline version history for rollbacks

================================================================================
CHAPTER 7: SDK REFERENCE
================================================================================

Official SDKs are available for:
- Python: pip install anycompany-datapipeline-sdk
- JavaScript/Node.js: npm install @anycompany/datapipeline-sdk
- Java: Maven artifact com.anycompany:datapipeline-sdk
- Go: go get github.com/anycompany/datapipeline-sdk-go

Python SDK Example:
from anycompany_datapipeline import DataPipelineClient

client = DataPipelineClient(api_key="your-api-key")

# List pipelines
pipelines = client.pipelines.list(status="active")

# Create a pipeline
new_pipeline = client.pipelines.create(
    name="My Pipeline",
    source={"type": "kafka", "config": {...}},
    destination={"type": "snowflake", "config": {...}}
)

# Get metrics
metrics = client.metrics.get(
    pipeline_id="pipe_abc123",
    start_time="2024-01-01T00:00:00Z",
    end_time="2024-01-02T00:00:00Z"
)

================================================================================

For additional support, contact api-support@anycompany.example.com or visit our
documentation portal at https://docs.anycompany.example.com

API Status Page: https://status.anycompany.example.com
