{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "introduction",
   "metadata": {},
   "source": [
    "# Advanced Prompt Caching\n",
    "\n",
    "This notebook covers advanced prompt caching strategies for Amazon Bedrock. You'll learn what can be cached, how caching works under the hood, and best practices to maximize cache efficiency for cost savings.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "- Understand the three types of cacheable content (tools, system, messages)\n",
    "- Implement multi-checkpoint caching patterns\n",
    "- Apply static/dynamic separation principles\n",
    "- Choose appropriate TTL strategies (5-minute vs 1-hour)\n",
    "- Monitor and optimize cache hit rates\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "At production scale, caching decisions significantly impact costs:\n",
    "- **Cache reads cost up to 90% less than regular input** - each cache hit saves substantial money\n",
    "- **Wrong checkpoint placement causes cache thrashing** - repeated cache misses waste the 1.25x write investment\n",
    "- **Proper TTL selection prevents unnecessary rewrites** - 1-hour TTL saves money on stable content\n",
    "\n",
    "**Duration**: ~60 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisites",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, ensure you have:\n",
    "1. An AWS account with Amazon Bedrock access enabled\n",
    "2. AWS credentials configured (via `.env` file, AWS CLI, or IAM role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1p8ge58d1mui",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup-imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region: us-east-1\n",
      "Model: global.anthropic.claude-sonnet-4-5-20250929-v1:0\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from uuid import uuid4\n",
    "\n",
    "import boto3\n",
    "\n",
    "# Third-party imports\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Local imports - utility functions for cache metrics\n",
    "from utils import (\n",
    "    analyze_caching_roi,\n",
    "    calculate_savings,\n",
    "    extract_cache_metrics,\n",
    ")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize AWS client\n",
    "REGION = os.getenv(\"AWS_DEFAULT_REGION\", \"us-east-1\")\n",
    "bedrock_runtime = boto3.client(\"bedrock-runtime\", region_name=REGION)\n",
    "\n",
    "# Model configuration\n",
    "MODEL_ID = \"global.anthropic.claude-sonnet-4-5-20250929-v1:0\"\n",
    "\n",
    "# Data directory path\n",
    "DATA_DIR = Path(\"data\")\n",
    "\n",
    "print(f\"Region: {REGION}\")\n",
    "print(f\"Model: {MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l611ub1qnks",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> This notebook uses Claude Sonnet 4.5 with the <b>global</b> CRIS (Cross-Region Inference Service) profile. The global profile offers ~10% cost savings and higher availability by automatically routing requests to regions with capacity.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ej19lgbbiik",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded sample data:\n",
      "  - Technical docs: 9,696 chars\n",
      "  - Company policies: 8,750 chars\n",
      "  - Analyst system v1: 4,701 chars\n",
      "  - Analyst system v2: 4,387 chars\n",
      "  - Weather tools: 2 tools\n",
      "  - Analyst tools: 5 tools\n"
     ]
    }
   ],
   "source": [
    "# Load sample data files for demos\n",
    "# These files contain realistic content to demonstrate caching patterns\n",
    "\n",
    "# Technical documentation for document Q&A demos\n",
    "TECHNICAL_DOCS = (DATA_DIR / \"technical_documentation.txt\").read_text()\n",
    "\n",
    "# Company policies for system prompt demos\n",
    "COMPANY_POLICIES = (DATA_DIR / \"company_policies.txt\").read_text()\n",
    "\n",
    "# Data analyst system prompts (two versions to demo invalidation)\n",
    "ANALYST_SYSTEM_V1 = (DATA_DIR / \"analyst_system_prompt_v1.txt\").read_text()\n",
    "ANALYST_SYSTEM_V2 = (DATA_DIR / \"analyst_system_prompt_v2.txt\").read_text()\n",
    "\n",
    "# Tool definitions\n",
    "WEATHER_TOOLS = json.loads((DATA_DIR / \"weather_tools.json\").read_text())\n",
    "ANALYST_TOOLS = json.loads((DATA_DIR / \"analyst_tools.json\").read_text())\n",
    "\n",
    "print(\"Loaded sample data:\")\n",
    "print(f\"  - Technical docs: {len(TECHNICAL_DOCS):,} chars\")\n",
    "print(f\"  - Company policies: {len(COMPANY_POLICIES):,} chars\")\n",
    "print(f\"  - Analyst system v1: {len(ANALYST_SYSTEM_V1):,} chars\")\n",
    "print(f\"  - Analyst system v2: {len(ANALYST_SYSTEM_V2):,} chars\")\n",
    "print(f\"  - Weather tools: {len(WEATHER_TOOLS)} tools\")\n",
    "print(f\"  - Analyst tools: {len(ANALYST_TOOLS)} tools\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part1-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Key Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1-1",
   "metadata": {},
   "source": [
    "## 1.1 What Can Be Cached\n",
    "\n",
    "Amazon Bedrock supports caching **three types of content** in your requests:\n",
    "\n",
    "| Content Type | Description | Change Frequency |\n",
    "|-------------|-------------|------------------|\n",
    "| **Tools** | Function definitions for tool use | Rarely (schema changes) |\n",
    "| **System** | System instructions and context | Occasionally (policy updates) |\n",
    "| **Messages** | Conversation history | Frequently (every turn) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1-2",
   "metadata": {},
   "source": [
    "## 1.2 Cache Checkpoints\n",
    "\n",
    "Amazon Bedrock supports **up to 4 cache checkpoints per request**. A checkpoint marks: \"Cache everything from the beginning up to this point.\"\n",
    "\n",
    "### Checkpoint Syntax\n",
    "\n",
    "```python\n",
    "# Converse API (5-minute TTL only)\n",
    "{\"cachePoint\": {\"type\": \"default\"}}\n",
    "\n",
    "# InvokeModel API (Anthropic models - supports both TTLs)\n",
    "{\"cache_control\": {\"type\": \"ephemeral\"}}      # 5-minute TTL (1.25x write)\n",
    "{\"cache_control\": {\"type\": \"ephemeral_1h\"}}   # 1-hour TTL (2x write)\n",
    "```\n",
    "\n",
    "### Supported Models and Limits\n",
    "\n",
    "| Model | Min Tokens | Max Checkpoints | Cacheable Fields |\n",
    "|-------|-----------|-----------------|------------------|\n",
    "| Claude Sonnet 4.5 | 1,024 | 4 | system, messages, tools |\n",
    "| Claude Opus 4.1 | 1,024 | 4 | system, messages, tools |\n",
    "| Claude Haiku 4.5 | 4,096 | 4 | system, messages, tools |\n",
    "| Amazon Nova Pro | 1,000 | 4 | system, messages |\n",
    "\n",
    "> **Reference**: See [AWS Bedrock Prompt Caching Documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-caching.html) for the latest supported models and limits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1-3",
   "metadata": {},
   "source": [
    "## 1.3 Bedrock Prompt Assembly Order\n",
    "\n",
    "**Critical**: When calling Bedrock for inference, prompts are assembled in this specific order:\n",
    "\n",
    "```\n",
    "┌─────────────┐      ┌─────────────┐      ┌─────────────┐\n",
    "│   Tools     │  →   │   System    │  →   │  Messages   │\n",
    "│(if provided)│      │(if provided)│      │  (history + │\n",
    "│             │      │             │      │   current)  │\n",
    "└─────────────┘      └─────────────┘      └─────────────┘\n",
    "```\n",
    "\n",
    "This sequential assembly determines cache behavior:\n",
    "- Caches are matched from the **beginning** of the assembled prompt\n",
    "- Each checkpoint caches **everything from the start up to that checkpoint** (cumulative)\n",
    "- If content at any position changes, **that cache + all subsequent caches** are invalidated\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Key Insight:</b> Tools are first in assembly order - changing tools invalidates the <b>entire</b> cache chain (tools → system → messages).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1-3-patterns",
   "metadata": {},
   "source": [
    "### Common Caching Patterns\n",
    "\n",
    "| Pattern | Structure | Best For |\n",
    "|---------|-----------|----------|\n",
    "| **Single Checkpoint** | `[Document] → [CP] → [Query]` | FAQ/support, document Q&A |\n",
    "| **Two Checkpoints** | `[Tools] → [CP1] → [System] → [CP2] → [Messages]` | Stable agent with fixed tools |\n",
    "| **Three Checkpoints or more** | `[Tools] → [CP1] → [System] → [CP2] → [History] → [CP3] → [Current]` | Conversational agent |\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> You can place multiple checkpoints within a single section. The 4-checkpoint limit is per request, not per section.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1-4",
   "metadata": {},
   "source": [
    "## 1.4 Cumulative Token Counting\n",
    "\n",
    "**Important**: The minimum token requirement is **cumulative**, not per-section.\n",
    "\n",
    "This means:\n",
    "- You DON'T need 1,024 tokens in tools AND 1,024 in system AND 1,024 in messages\n",
    "- You need 1,024 tokens **total** from the start to your first checkpoint\n",
    "- Each subsequent checkpoint also uses cumulative counting from the start\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Key Insight:</b> If your tools/system individually don't meet the minimum, that's okay! The cumulative count from the start is what matters.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demo-1-4-cumulative",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Cumulative Token Counting\n",
    "# Uses weather tools (small) + company policies as system (large)\n",
    "# Together they exceed the 1,024 token minimum\n",
    "\n",
    "# Weather tools - below minimum token count alone\n",
    "tools_with_checkpoint = [*WEATHER_TOOLS, {\"cachePoint\": {\"type\": \"default\"}}]\n",
    "\n",
    "# Company policies as system prompt - combined with tools exceeds minimum\n",
    "system_with_checkpoint = [{\"text\": COMPANY_POLICIES}, {\"cachePoint\": {\"type\": \"default\"}}]\n",
    "\n",
    "# Request 1: Cache WRITE\n",
    "r1 = bedrock_runtime.converse(\n",
    "    modelId=MODEL_ID,\n",
    "    toolConfig={\"tools\": tools_with_checkpoint},\n",
    "    system=system_with_checkpoint,\n",
    "    messages=[{\"role\": \"user\", \"content\": [{\"text\": \"What's the weather in Seattle?\"}]}],\n",
    "    inferenceConfig={\"maxTokens\": 100},\n",
    ")\n",
    "m1 = extract_cache_metrics(r1)\n",
    "\n",
    "# Request 2: Cache READ\n",
    "r2 = bedrock_runtime.converse(\n",
    "    modelId=MODEL_ID,\n",
    "    toolConfig={\"tools\": tools_with_checkpoint},\n",
    "    system=system_with_checkpoint,\n",
    "    messages=[{\"role\": \"user\", \"content\": [{\"text\": \"What's the weather in New York?\"}]}],\n",
    "    inferenceConfig={\"maxTokens\": 100},\n",
    ")\n",
    "m2 = extract_cache_metrics(r2)\n",
    "\n",
    "print(\"DEMO: Cumulative Token Counting\")\n",
    "print(\"Request 1: \")\n",
    "print(m1)\n",
    "print(\"---\")\n",
    "print(\"Request 2: \")\n",
    "print(m2)\n",
    "print(\"\\nKey: Even though tools alone < 1,024, cumulative total met the requirement.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1-5",
   "metadata": {},
   "source": [
    "## 1.5 Cache Invalidation Scenarios\n",
    "\n",
    "Understanding what invalidates cache is critical for maintaining high hit rates.\n",
    "\n",
    "### Invalidation Impact by Change Type\n",
    "\n",
    "| What Changes | Tools Cache | System Cache | Messages Cache |\n",
    "|--------------|-------------|--------------|----------------|\n",
    "| Only messages | ✅ HIT | ✅ HIT | Fresh (not cached) |\n",
    "| System prompt | ✅ HIT | ❌ MISS (rewrite) | ❌ MISS |\n",
    "| Tools | ❌ MISS | ❌ MISS | ❌ MISS |\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> This assumes cache checkpoints are explicitly defined at each section and meet the minimum token requirement.\n",
    "</div>\n",
    "\n",
    "### Common Invalidation Triggers\n",
    "\n",
    "| Trigger | Impact | Notes |\n",
    "|---------|--------|-------|\n",
    "| Tool definition changed | Full invalidation | Tools are assembled first |\n",
    "| System prompt changed | System + Messages miss | Tools cache preserved |\n",
    "| Message history changed | Messages only | Tools + System preserved |\n",
    "| Image added/removed | Full invalidation | Even in uncached sections |\n",
    "| `tool_choice` changed | Full invalidation | Affects request structure |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demo-1-5-invalidation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Cache Invalidation Scenarios\n",
    "# Uses analyst tools and two versions of system prompt to show invalidation patterns\n",
    "\n",
    "\n",
    "def run_analyst(user_msg, system_text, tools_list):\n",
    "    \"\"\"Helper function to run analyst queries with caching.\"\"\"\n",
    "    response = bedrock_runtime.converse(\n",
    "        modelId=MODEL_ID,\n",
    "        toolConfig={\"tools\": [*tools_list, {\"cachePoint\": {\"type\": \"default\"}}]},\n",
    "        system=[{\"text\": system_text}, {\"cachePoint\": {\"type\": \"default\"}}],\n",
    "        messages=[{\"role\": \"user\", \"content\": [{\"text\": user_msg}]}],\n",
    "        inferenceConfig={\"maxTokens\": 100},\n",
    "    )\n",
    "    return extract_cache_metrics(response)\n",
    "\n",
    "\n",
    "print(\"DEMO: Cache Invalidation Scenarios\\n\")\n",
    "\n",
    "# Scenario 1: Only message changes (IDEAL)\n",
    "print(\"--- Scenario 1: Only Message Changes ---\")\n",
    "m1 = run_analyst(\"Show sales Q1\", ANALYST_SYSTEM_V1, ANALYST_TOOLS)\n",
    "print(f\"R1: write={m1['cache_write']:,}, read={m1['cache_read']:,}\")\n",
    "m2 = run_analyst(\"Show sales Q2\", ANALYST_SYSTEM_V1, ANALYST_TOOLS)\n",
    "print(f\"R2: write={m2['cache_write']:,}, read={m2['cache_read']:,} ← Cache HIT!\")\n",
    "\n",
    "# Scenario 2: System changes (PARTIAL MISS)\n",
    "print(\"\\n--- Scenario 2: System Prompt Changes ---\")\n",
    "m3 = run_analyst(\"Show sales Q3\", ANALYST_SYSTEM_V2, ANALYST_TOOLS)\n",
    "print(f\"R3: write={m3['cache_write']:,}, read={m3['cache_read']:,} ← System changed, partial miss\")\n",
    "\n",
    "# Scenario 3: Tools change (FULL INVALIDATION)\n",
    "print(\"\\n--- Scenario 3: Tools Change ---\")\n",
    "# Add a new tool to the existing tools\n",
    "NEW_TOOL = {\n",
    "    \"toolSpec\": {\n",
    "        \"name\": \"send_notification\",\n",
    "        \"description\": \"Send notifications to users via email or Slack.\",\n",
    "        \"inputSchema\": {\n",
    "            \"json\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\"channel\": {\"type\": \"string\"}, \"message\": {\"type\": \"string\"}},\n",
    "                \"required\": [\"channel\", \"message\"],\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "}\n",
    "m4 = run_analyst(\"Show sales Q4\", ANALYST_SYSTEM_V2, [NEW_TOOL, *ANALYST_TOOLS])\n",
    "print(f\"R4: write={m4['cache_write']:,}, read={m4['cache_read']:,} ← Tools changed, FULL invalidation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part2-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2-1",
   "metadata": {},
   "source": [
    "## 2.1 Static First, Dynamic Last\n",
    "\n",
    "**Principle**: Place static (cacheable) content before your cache checkpoint, dynamic content after.\n",
    "\n",
    "### Good Pattern: Static Document with Checkpoint\n",
    "\n",
    "```python\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\"text\": f\"Document:\\n{STATIC_DOCUMENT}\"},  # Static (cached)\n",
    "        {\"cachePoint\": {\"type\": \"default\"}},\n",
    "        {\"text\": f\"Question: {user_query}\"}  # Dynamic (not cached)\n",
    "    ]\n",
    "}]\n",
    "```\n",
    "\n",
    "### Good Pattern: System Prompt with Static/Dynamic Split\n",
    "\n",
    "```python\n",
    "system = [\n",
    "    {\"text\": f\"You are a helpful assistant.\\n\\n{COMPANY_POLICIES}\"},  # Static\n",
    "    {\"cachePoint\": {\"type\": \"default\"}},\n",
    "    {\"text\": f\"Session: {uuid4()}\\nDate: {datetime.now()}\"}  # Dynamic\n",
    "]\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Common Mistakes:</b>\n",
    "<ul>\n",
    "<li>Putting user queries inside the cached section (before checkpoint)</li>\n",
    "<li>Including session IDs or timestamps before the checkpoint</li>\n",
    "<li>Adding dynamic data anywhere in the cached content</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "demo-2-1-static-dynamic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOOD Pattern: Static before checkpoint, dynamic after\n",
      "\n",
      "Query 1: cache_read=0, cache_write=2,627\n",
      "Query 2: cache_read=2,627, cache_write=0\n",
      "Query 3: cache_read=2,627, cache_write=0\n",
      "\n",
      "Cache hit rate: 66.7%\n"
     ]
    }
   ],
   "source": [
    "# Demo: Static/Dynamic Separation - GOOD Pattern\n",
    "# Uses technical documentation as static content\n",
    "\n",
    "print(\"GOOD Pattern: Static before checkpoint, dynamic after\\n\")\n",
    "queries = [\"What is this about?\", \"Summarize the content\", \"Any key points?\"]\n",
    "metrics_good = []\n",
    "\n",
    "for i, query in enumerate(queries, 1):\n",
    "    response = bedrock_runtime.converse(\n",
    "        modelId=MODEL_ID,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"text\": f\"Document:\\n{TECHNICAL_DOCS}\"},\n",
    "                    {\"cachePoint\": {\"type\": \"default\"}},  # Checkpoint AFTER static\n",
    "                    {\"text\": f\"Question: {query}\"},  # Dynamic AFTER checkpoint\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        inferenceConfig={\"maxTokens\": 50},\n",
    "    )\n",
    "    metrics_good.append(extract_cache_metrics(response))\n",
    "    print(f\"Query {i}: cache_read={metrics_good[-1]['cache_read']:,}, cache_write={metrics_good[-1]['cache_write']:,}\")\n",
    "\n",
    "savings = calculate_savings(metrics_good)\n",
    "print(f\"\\nCache hit rate: {savings['hit_rate']:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "demo-2-1-bad-pattern",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAD Pattern: Dynamic content inside cached section (cache thrashing)\n",
      "\n",
      "Query 1: cache_read=0, cache_write=2,634\n",
      "Query 2: cache_read=0, cache_write=2,634\n",
      "Query 3: cache_read=0, cache_write=2,633\n",
      "\n",
      "Cache hit rate: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Demo: BAD Pattern - Dynamic content inside cached section\n",
    "\n",
    "print(\"BAD Pattern: Dynamic content inside cached section (cache thrashing)\\n\")\n",
    "queries = [\"What is this about?\", \"Summarize the content\", \"Any key points?\"]\n",
    "metrics_bad = []\n",
    "\n",
    "for i, query in enumerate(queries, 1):\n",
    "    response = bedrock_runtime.converse(\n",
    "        modelId=MODEL_ID,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    # BAD: Query is INSIDE cached content!\n",
    "                    {\"text\": f\"Document:\\n{TECHNICAL_DOCS}\\n\\nQuestion: {query}\"},\n",
    "                    {\"cachePoint\": {\"type\": \"default\"}},\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        inferenceConfig={\"maxTokens\": 50},\n",
    "    )\n",
    "    metrics_bad.append(extract_cache_metrics(response))\n",
    "    print(f\"Query {i}: cache_read={metrics_bad[-1]['cache_read']:,}, cache_write={metrics_bad[-1]['cache_write']:,}\")\n",
    "\n",
    "savings = calculate_savings(metrics_bad)\n",
    "print(f\"\\nCache hit rate: {savings['hit_rate']:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "demo-2-1-session-ids",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAD Pattern: Dynamic session data in system prompt\n",
      "\n",
      "Request 1: cache_read=0, cache_write=1,930\n",
      "Request 2: cache_read=0, cache_write=1,930\n",
      "Request 3: cache_read=0, cache_write=1,932\n",
      "\n",
      "Cache hit rate: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Demo: BAD Pattern - Session IDs in system prompt before cache checkpoint\n",
    "\n",
    "print(\"BAD Pattern: Dynamic session data in system prompt\\n\")\n",
    "metrics_session_bad = []\n",
    "for i in range(3):\n",
    "    # BAD: New session ID on every request invalidates cache!\n",
    "    system_bad = [\n",
    "        {\n",
    "            \"text\": f\"You are a helpful assistant.\\nSession ID: {uuid4()}\\nDate: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n{COMPANY_POLICIES}\"\n",
    "        },\n",
    "        {\"cachePoint\": {\"type\": \"default\"}},\n",
    "    ]\n",
    "\n",
    "    response = bedrock_runtime.converse(\n",
    "        modelId=MODEL_ID,\n",
    "        system=system_bad,\n",
    "        messages=[{\"role\": \"user\", \"content\": [{\"text\": \"Hello\"}]}],\n",
    "        inferenceConfig={\"maxTokens\": 20},\n",
    "    )\n",
    "    metrics_session_bad.append(extract_cache_metrics(response))\n",
    "    print(\n",
    "        f\"Request {i + 1}: cache_read={metrics_session_bad[-1]['cache_read']:,}, cache_write={metrics_session_bad[-1]['cache_write']:,}\"\n",
    "    )\n",
    "\n",
    "print(f\"\\nCache hit rate: {calculate_savings(metrics_session_bad)['hit_rate']:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "demo-2-1-session-ids-good",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOOD Pattern: Static system with checkpoint, dynamic context after\n",
      "\n",
      "Request 1: cache_read=0, cache_write=1,887\n",
      "Request 2: cache_read=1,887, cache_write=0\n",
      "Request 3: cache_read=1,887, cache_write=0\n",
      "\n",
      "Cache hit rate: 66.7%\n"
     ]
    }
   ],
   "source": [
    "# Demo: GOOD Pattern - Static system with checkpoint, then dynamic context\n",
    "\n",
    "print(\"GOOD Pattern: Static system with checkpoint, dynamic context after\\n\")\n",
    "metrics_session_good = []\n",
    "for i in range(3):\n",
    "    # GOOD: Checkpoint BETWEEN static and dynamic parts\n",
    "    system_good = [\n",
    "        {\"text\": f\"You are a helpful assistant.\\n\\n{COMPANY_POLICIES}\"},  # Static (cached)\n",
    "        {\"cachePoint\": {\"type\": \"default\"}},\n",
    "        {\"text\": f\"Session ID: {uuid4()}\\nDate: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"},  # Dynamic\n",
    "    ]\n",
    "\n",
    "    response = bedrock_runtime.converse(\n",
    "        modelId=MODEL_ID,\n",
    "        system=system_good,\n",
    "        messages=[{\"role\": \"user\", \"content\": [{\"text\": \"Hello\"}]}],\n",
    "        inferenceConfig={\"maxTokens\": 20},\n",
    "    )\n",
    "    metrics_session_good.append(extract_cache_metrics(response))\n",
    "    print(\n",
    "        f\"Request {i + 1}: cache_read={metrics_session_good[-1]['cache_read']:,}, cache_write={metrics_session_good[-1]['cache_write']:,}\"\n",
    "    )\n",
    "\n",
    "print(f\"\\nCache hit rate: {calculate_savings(metrics_session_good)['hit_rate']:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2-2",
   "metadata": {},
   "source": [
    "## 2.2 Cache TTL Strategy: 5-Minute vs 1-Hour\n",
    "\n",
    "### TTL Syntax (InvokeModel API)\n",
    "\n",
    "```python\n",
    "# 5-minute TTL (default)\n",
    "\"cache_control\": {\"type\": \"ephemeral\", \"ttl\": \"5m\"}\n",
    "\n",
    "# 1-hour TTL (extended)\n",
    "\"cache_control\": {\"type\": \"ephemeral\", \"ttl\": \"1h\"}\n",
    "```\n",
    "\n",
    "| TTL | Syntax | Duration | Write Cost | Read Cost |\n",
    "|-----|--------|----------|------------|-----------|\n",
    "| **5-minute** | `{\"type\": \"ephemeral\", \"ttl\": \"5m\"}` | 5 min | 1.25x | 0.1x |\n",
    "| **1-hour** | `{\"type\": \"ephemeral\", \"ttl\": \"1h\"}` | 1 hour | 2x | 0.1x |\n",
    "\n",
    "### When to Use Each TTL\n",
    "\n",
    "**Continue using 5-minute TTL when:**\n",
    "- Prompts are used at a regular cadence (more frequently than every 5 minutes)\n",
    "- System prompts that are continuously refreshed at no additional charge\n",
    "\n",
    "**Use 1-hour TTL when:**\n",
    "- Prompts used less frequently than every 5 minutes, but more than every hour\n",
    "- Agentic side-agents that take longer than 5 minutes to complete\n",
    "- Long chat conversations where user may not respond within 5 minutes\n",
    "- Latency is important and follow-up prompts may be sent beyond 5 minutes\n",
    "- You want to improve rate limit utilization (cache hits don't count against rate limits)\n",
    "\n",
    "### Mixing TTLs\n",
    "\n",
    "You can use both TTLs in the same request with an **important constraint**:\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Important:</b> Cache entries with longer TTL must appear <b>before</b> shorter TTLs (1-hour cache must appear before any 5-minute cache entries).\n",
    "</div>\n",
    "\n",
    "> **API Support**: 1-hour TTL is supported via **InvokeModel API** only. Converse API supports 5-minute TTL (`cachePoint`) only.\n",
    ">\n",
    "> **Reference**: [Amazon Bedrock Pricing](https://aws.amazon.com/bedrock/pricing/) for supported models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "demo-2-2-warmup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEMO: Mixed TTL Strategy (InvokeModel API)\n",
      "\n",
      "Request 1: \n",
      "{'input_tokens': 11, 'cache_creation_input_tokens': 4533, 'cache_read_input_tokens': 0, 'cache_creation': {'ephemeral_5m_input_tokens': 2627, 'ephemeral_1h_input_tokens': 1906}, 'output_tokens': 100}\n",
      "---\n",
      "Request 2: \n",
      "{'input_tokens': 12, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 4533, 'cache_creation': {'ephemeral_5m_input_tokens': 0, 'ephemeral_1h_input_tokens': 0}, 'output_tokens': 100}\n"
     ]
    }
   ],
   "source": [
    "# Demo: Mixed TTL Strategy with InvokeModel API\n",
    "# Uses company policies (stable) with 1h TTL and technical docs as context with 5m TTL\n",
    "\n",
    "\n",
    "def test_mixed_ttl(query: str) -> dict:\n",
    "    \"\"\"Test mixed TTL strategy using InvokeModel API.\"\"\"\n",
    "    request_body = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 100,\n",
    "        \"system\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": f\"You are an assistant for the company. You have to follow exactly with the company policies when the employee is asking any questions.\\n\\n{COMPANY_POLICIES}\",\n",
    "                \"cache_control\": {\"type\": \"ephemeral\", \"ttl\": \"1h\"},  # 1-hour TTL (must come first)\n",
    "            }\n",
    "        ],\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": f\"Reference documentation:\\n{TECHNICAL_DOCS}\",\n",
    "                        \"cache_control\": {\"type\": \"ephemeral\", \"ttl\": \"5m\"},  # 5-minute TTL (after 1h)\n",
    "                    },\n",
    "                    {\"type\": \"text\", \"text\": f\"Current question: {query}\"},\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        modelId=MODEL_ID, contentType=\"application/json\", accept=\"application/json\", body=json.dumps(request_body)\n",
    "    )\n",
    "    return json.loads(response[\"body\"].read())\n",
    "\n",
    "\n",
    "print(\"DEMO: Mixed TTL Strategy (InvokeModel API)\\n\")\n",
    "\n",
    "# Request 1: WRITE to both caches\n",
    "r1 = test_mixed_ttl(\"When will my order arrive?\")\n",
    "print(\"Request 1: \")\n",
    "print(r1[\"usage\"])\n",
    "\n",
    "print(\"---\")\n",
    "\n",
    "# Request 2: READ from both caches\n",
    "r2 = test_mixed_ttl(\"Can I change the shipping address?\")\n",
    "print(\"Request 2: \")\n",
    "print(r2[\"usage\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2-3",
   "metadata": {},
   "source": [
    "## 2.3 Monitor Cache Performance\n",
    "\n",
    "### Key Metrics from API Response\n",
    "\n",
    "Cache metrics are available in every API response under the `usage` object:\n",
    "\n",
    "| Metric | Description | Cost Impact |\n",
    "|--------|-------------|-------------|\n",
    "| `inputTokens` | Fresh input tokens (not cached) | 1x base rate |\n",
    "| `cacheWriteInputTokens` | Tokens written to cache | 1.25x (5m) or 2x (1h) |\n",
    "| `cacheReadInputTokens` | Tokens read from cache | 0.1x base rate |\n",
    "\n",
    "### Cache Hit Rate Formula\n",
    "\n",
    "```python\n",
    "cache_hit_rate = cacheReadInputTokens / (cacheReadInputTokens + cacheWriteInputTokens)\n",
    "```\n",
    "\n",
    "**Target**: > 70% cache hit rate for optimal ROI\n",
    "\n",
    "### When Caching Makes Sense\n",
    "\n",
    "| Scenario | Cache? | Reason |\n",
    "|----------|--------|--------|\n",
    "| Same document, multiple queries | ✅ Yes | High reuse potential |\n",
    "| Stable system prompt | ✅ Yes | Rarely changes |\n",
    "| Multi-turn conversations | ✅ Yes | History reused each turn |\n",
    "| One-off document analysis | ❌ No | No reuse opportunity |\n",
    "| Highly dynamic prompts | ❌ No | Cache thrashing |\n",
    "| Content < 1,024 tokens | ❌ No | Won't cache anyway |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "demo-2-3-monitoring",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEMO: Cache Performance Monitoring\n",
      "\n",
      "Query 1: WRITE - write=2,627, read=0\n",
      "Query 2: READ - write=0, read=2,627\n",
      "Query 3: READ - write=0, read=2,627\n",
      "Query 4: READ - write=0, read=2,627\n",
      "Query 5: READ - write=0, read=2,627\n",
      "\n",
      "==================================================\n",
      "CACHE PERFORMANCE REPORT\n",
      "==================================================\n",
      "Requests:        5\n",
      "Cache hit rate:  80.0% ✅\n",
      "Cost savings:    66.8%\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Demo: Cache Performance Monitoring\n",
    "# Uses technical documentation for multiple queries to show cache hit rate\n",
    "\n",
    "session_metrics = []\n",
    "\n",
    "print(\"DEMO: Cache Performance Monitoring\\n\")\n",
    "queries = [\"What is the main topic?\", \"List the key points.\", \"Summarize briefly.\", \"Any action items?\", \"Conclusion?\"]\n",
    "\n",
    "for i, query in enumerate(queries, 1):\n",
    "    response = bedrock_runtime.converse(\n",
    "        modelId=MODEL_ID,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"text\": f\"Documents:\\n{TECHNICAL_DOCS}\"},\n",
    "                    {\"cachePoint\": {\"type\": \"default\"}},\n",
    "                    {\"text\": f\"Question: {query}\"},\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        inferenceConfig={\"maxTokens\": 50},\n",
    "    )\n",
    "    metrics = extract_cache_metrics(response)\n",
    "    session_metrics.append(metrics)\n",
    "    status = \"WRITE\" if metrics[\"cache_write\"] > 0 else \"READ\"\n",
    "    print(f\"Query {i}: {status} - write={metrics['cache_write']:,}, read={metrics['cache_read']:,}\")\n",
    "\n",
    "# Analyze ROI\n",
    "roi = analyze_caching_roi(session_metrics)\n",
    "\n",
    "print(f\"\\n{'=' * 50}\")\n",
    "print(\"CACHE PERFORMANCE REPORT\")\n",
    "print(f\"{'=' * 50}\")\n",
    "print(f\"Requests:        {roi['total_requests']}\")\n",
    "print(f\"Cache hit rate:  {roi['hit_rate']:.1f}% {'✅' if roi['hit_rate'] >= 70 else '⚠️'}\")\n",
    "print(f\"Cost savings:    {roi['roi_pct']:.1f}%\")\n",
    "print(f\"{'=' * 50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, you learned advanced prompt caching strategies for Amazon Bedrock:\n",
    "\n",
    "### Part 1: Key Concepts\n",
    "\n",
    "| Concept | Key Takeaway |\n",
    "|---------|-------------|\n",
    "| **Cacheable Content** | Tools, System, Messages (in assembly order) |\n",
    "| **Checkpoints** | Up to 4 per request, can be distributed within sections |\n",
    "| **Assembly Order** | Tools → System → Messages |\n",
    "| **Token Counting** | **Cumulative** from start, not per-section |\n",
    "| **Invalidation** | Changes to early content invalidate downstream caches |\n",
    "\n",
    "### Part 2: Best Practices\n",
    "\n",
    "| Practice | Key Takeaway |\n",
    "|----------|-------------|\n",
    "| **Static/Dynamic** | Static content first, checkpoint, then dynamic content |\n",
    "| **TTL Strategy** | 5m for regular cadence (>1x per 5 min), 1h for infrequent access |\n",
    "| **Mixing TTLs** | 1-hour cache MUST appear before 5-minute cache |\n",
    "| **Selective Caching** | Only cache content with high reuse potential |\n",
    "| **Monitoring** | Track hit rate (>70% target), calculate break-even |\n",
    "\n",
    "### TTL Quick Reference (InvokeModel API)\n",
    "\n",
    "| When to Use | TTL | Syntax | Write Cost |\n",
    "|-------------|-----|--------|------------|\n",
    "| Regular cadence (>1x per 5 min) | 5 min | `{\"type\": \"ephemeral\", \"ttl\": \"5m\"}` | 1.25x |\n",
    "| Infrequent (5 min - 1 hour) | 1 hour | `{\"type\": \"ephemeral\", \"ttl\": \"1h\"}` | 2x |\n",
    "\n",
    "*Read cost is 0.1x for both TTL types*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resources",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [AWS Bedrock Prompt Caching Documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-caching.html)\n",
    "- [AWS Bedrock Pricing](https://aws.amazon.com/bedrock/pricing/)\n",
    "- [AWS Bedrock Monitoring Documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/monitoring.html)\n",
    "- [Anthropic Prompt Caching Documentation](https://platform.claude.com/docs/en/build-with-claude/prompt-caching)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws-bedrock-prompt-optimization-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}