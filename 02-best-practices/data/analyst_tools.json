[
    {
        "toolSpec": {
            "name": "query_database",
            "description": "Query the analytics database to retrieve sales, customer, or product data. Supports SQL-like queries with filters, aggregations, and joins. Use this tool when you need to fetch raw data or compute metrics like total sales, customer counts, average order values, or product performance statistics.\n\nDETAILED USAGE GUIDELINES:\n- Always validate data integrity before processing queries\n- Apply appropriate data transformations for accurate results\n- Handle edge cases like null values, outliers, and missing data\n- Use proper aggregation functions (SUM, AVG, COUNT, MIN, MAX)\n- Consider performance implications for large datasets\n- Apply filters before aggregations to optimize query performance\n- Ensure proper data type handling for accurate computations\n- Document assumptions made during data analysis\n- Validate results against business logic and domain knowledge\n- Consider temporal aspects of data (time zones, fiscal years, etc.)\n- Handle currency and numeric precision appropriately\n- Apply proper rounding rules for financial calculations\n- Consider seasonality and trends in time-series data\n- Account for data quality issues and missing values\n- Use appropriate statistical methods for data analysis\n- Validate aggregation results against expected ranges\n- Consider data privacy and security requirements\n- Apply proper data masking for sensitive information\n- Use appropriate joins for multi-table queries\n- Optimize query performance with proper indexing\n- Monitor query execution time and resource usage\n- Apply proper error handling for data exceptions\n- Document data lineage and transformation steps\n- Ensure reproducibility of analysis results\n- Consider data freshness and staleness\n- Apply proper date range filters for accurate reporting\n- Use appropriate granularity levels for analysis\n- Consider impact of outliers on aggregate statistics\n- Apply proper normalization for comparative analysis\n- Validate data consistency across related tables",
            "inputSchema": {
                "json": {
                    "type": "object",
                    "properties": {
                        "table": {
                            "type": "string",
                            "description": "Database table to query",
                            "enum": ["sales", "customers", "products", "orders"]
                        },
                        "filters": {
                            "type": "object",
                            "description": "Optional filters to apply (e.g., date range, product category, customer segment)"
                        }
                    },
                    "required": ["table"]
                }
            }
        }
    },
    {
        "toolSpec": {
            "name": "generate_chart",
            "description": "Generate visualization charts from data including line charts, bar charts, pie charts, and scatter plots. Automatically formats data and applies best practices for data visualization. Use this tool when you need to create visual representations of trends, comparisons, or distributions.\n\nDETAILED USAGE GUIDELINES:\n- Always validate data integrity before processing queries\n- Apply appropriate data transformations for accurate results\n- Handle edge cases like null values, outliers, and missing data\n- Use proper aggregation functions (SUM, AVG, COUNT, MIN, MAX)\n- Consider performance implications for large datasets\n- Apply filters before aggregations to optimize query performance\n- Ensure proper data type handling for accurate computations\n- Document assumptions made during data analysis\n- Validate results against business logic and domain knowledge\n- Consider temporal aspects of data (time zones, fiscal years, etc.)\n- Handle currency and numeric precision appropriately\n- Apply proper rounding rules for financial calculations\n- Consider seasonality and trends in time-series data\n- Account for data quality issues and missing values\n- Use appropriate statistical methods for data analysis\n- Validate aggregation results against expected ranges\n- Consider data privacy and security requirements\n- Apply proper data masking for sensitive information\n- Use appropriate joins for multi-table queries\n- Optimize query performance with proper indexing\n- Monitor query execution time and resource usage\n- Apply proper error handling for data exceptions\n- Document data lineage and transformation steps\n- Ensure reproducibility of analysis results\n- Consider data freshness and staleness\n- Apply proper date range filters for accurate reporting\n- Use appropriate granularity levels for analysis\n- Consider impact of outliers on aggregate statistics\n- Apply proper normalization for comparative analysis\n- Validate data consistency across related tables",
            "inputSchema": {
                "json": {
                    "type": "object",
                    "properties": {
                        "chart_type": {
                            "type": "string",
                            "description": "Type of chart to generate",
                            "enum": ["line", "bar", "pie", "scatter"]
                        },
                        "data": {
                            "type": "object",
                            "description": "Data to visualize"
                        }
                    },
                    "required": ["chart_type", "data"]
                }
            }
        }
    }
]
