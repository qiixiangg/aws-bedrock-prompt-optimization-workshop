{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Prompt Caching Implementation\n",
    "\n",
    "**Learning Objectives**:\n",
    "- Implement prompt caching with boto3 Converse API and Invoke Model API\n",
    "- Apply caching strategy best practices for optimal performance\n",
    "- Measure cache hit rates and cost savings\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using AWS CLI credentials\n",
      "‚ÑπÔ∏è  boto3 will use credentials from ~/.aws/credentials\n",
      "\n",
      "‚úÖ Bedrock client initialized\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "\n",
    "# Verify AWS credentials\n",
    "if os.getenv('AWS_ACCESS_KEY_ID') and os.getenv('AWS_SECRET_ACCESS_KEY'):\n",
    "    print(\"‚úÖ AWS credentials loaded from .env\")\n",
    "    print(f\"‚úÖ Region: {os.getenv('AWS_DEFAULT_REGION', 'us-east-1')}\")\n",
    "else:\n",
    "    print(\"‚úÖ Using AWS CLI credentials\")\n",
    "    print(\"‚ÑπÔ∏è  boto3 will use credentials from ~/.aws/credentials\")\n",
    "\n",
    "# Initialize Bedrock client\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name=os.getenv('AWS_DEFAULT_REGION', 'us-east-1')\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Bedrock client initialized\")\n",
    "\n",
    "# Model configuration (use global.anthropic for cross-region inference)\n",
    "MODEL_ID = \"global.anthropic.claude-sonnet-4-5-20250929-v1:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Helper Functions\n",
    "\n",
    "We'll use cache metrics utilities from the `utils` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Helper functions loaded from utils module\n"
     ]
    }
   ],
   "source": [
    "# Import cache metrics utilities\n",
    "from utils.cache_metrics import (\n",
    "    extract_cache_metrics,\n",
    "    print_cache_metrics,\n",
    "    calculate_cache_savings\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Helper functions loaded from utils module\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part A: Prompt Caching Implementation\n",
    "\n",
    "In this part, you'll learn:\n",
    "\n",
    "1. **Using Prompt Caching with Converse and Invoke Model APIs** - Learn how to add cache checkpoints with both API styles and understand their differences\n",
    "2. **Multi-Checkpoint Caching** - Cache multiple layers with different update frequencies (tools, system, conversation history)\n",
    "3. **Cache Invalidation** - Understand how cache behaves when content changes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Prompt Caching with Converse and Invoke Model APIs\n",
    "\n",
    "Amazon Bedrock supports two APIs for calling foundation models. Both provide **identical caching functionality** - only the syntax differs.\n",
    "\n",
    "| Aspect | **Converse API** (Recommended) | **Invoke Model API** |\n",
    "|--------|-------------------------------|---------------------|\n",
    "| **Purpose** | Unified interface across all Bedrock models | Direct access to provider-specific APIs |\n",
    "| **Abstraction** | High-level, handles provider details automatically | Low-level, uses native provider format |\n",
    "| **Benefits** | Consistent syntax across all providers (Anthropic, Amazon, Meta, etc.) | Full control over provider-specific features |\n",
    "| **Cache Syntax** | `{\"cachePoint\": {\"type\": \"default\"}}` | `{\"cache_control\": {\"type\": \"ephemeral\"}}` |\n",
    "| **Use Case** | Modern applications, easier to switch models | Provider-specific capabilities, SDK compatibility |\n",
    "\n",
    "**In this workshop**: We'll first demonstrate both approaches below to show the syntax differences, then use Converse API throughout (recommended for new implementations).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Simple Prompt Caching\n",
    "\n",
    "We'll implement the document Q&A system with caching using **both** Converse and Invoke APIs to highlight their differences.\n",
    "\n",
    "**You'll see:**\n",
    "- How to structure the cache checkpoint with each API\n",
    "- The syntax differences between `cachePoint` (Converse) and `cache_control` (Invoke)  \n",
    "- Both APIs produce identical caching behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Product manual loaded\n",
      "   Length: 1023 words\n",
      "   Estimated tokens: ~1330 tokens\n"
     ]
    }
   ],
   "source": [
    "# Load product documentation from file\n",
    "with open('data/product_manual.txt', 'r') as f:\n",
    "    PRODUCT_MANUAL = f.read()\n",
    "\n",
    "# Note: Each cache checkpoint must meet minimum token requirements\n",
    "# Learn more: https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-caching.html\n",
    "\n",
    "print(f\"‚úÖ Product manual loaded\")\n",
    "print(f\"   Length: {len(PRODUCT_MANUAL.split())} words\")\n",
    "print(f\"   Estimated tokens: ~{len(PRODUCT_MANUAL.split()) * 1.3:.0f} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation Option 1: Converse API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Converse API function defined\n"
     ]
    }
   ],
   "source": [
    "def query_document_converse(user_query, document=PRODUCT_MANUAL, model_id=MODEL_ID):\n",
    "    \"\"\"\n",
    "    Query a document using single-checkpoint caching with Converse API.\n",
    "    \n",
    "    Args:\n",
    "        user_query: User's question\n",
    "        document: Static document to cache\n",
    "        model_id: Bedrock model ID\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (response_text, cache_metrics)\n",
    "    \"\"\"\n",
    "    # Construct message with cache checkpoint\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    # Static content to cache\n",
    "                    \"text\": f\"\"\"You are a helpful assistant. Use the following product manual to answer questions.\n",
    "\n",
    "PRODUCT MANUAL:\n",
    "{document}\n",
    "\n",
    "Answer the user's question based on the information in the manual. If the answer is not in the manual, say so.\"\"\"\n",
    "                },\n",
    "                {\n",
    "                    # CACHE CHECKPOINT (Converse API - separate block)\n",
    "                    \"cachePoint\": {\"type\": \"default\"}\n",
    "                },\n",
    "                {\n",
    "                    # User query (dynamic, not cached)\n",
    "                    \"text\": f\"\\n\\nQUESTION: {user_query}\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Call Bedrock Converse API\n",
    "    response = bedrock_runtime.converse(\n",
    "        modelId=model_id,\n",
    "        messages=messages,\n",
    "        inferenceConfig={\n",
    "            \"maxTokens\": 500,\n",
    "            \"temperature\": 0.0\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Extract response and metrics\n",
    "    response_text = response['output']['message']['content'][0]['text']\n",
    "    metrics = extract_cache_metrics(response)\n",
    "    \n",
    "    return response_text, metrics\n",
    "\n",
    "print(\"‚úÖ Converse API function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation Option 2: Invoke Model API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Invoke Model API function defined\n"
     ]
    }
   ],
   "source": [
    "def query_document_invoke(user_query, document=PRODUCT_MANUAL, model_id=MODEL_ID):\n",
    "    \"\"\"\n",
    "    Query a document using single-checkpoint caching with Invoke Model API.\n",
    "    \n",
    "    Args:\n",
    "        user_query: User's question\n",
    "        document: Static document to cache\n",
    "        model_id: Bedrock model ID\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (response_text, cache_metrics)\n",
    "    \"\"\"\n",
    "    # Construct request body with cache_control syntax (Invoke Model API)\n",
    "    # Add \"Invoke Model API\" marker to create separate cache from Converse demo\n",
    "    request_body = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 500,\n",
    "        \"temperature\": 0.0,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": f\"\"\"You are a helpful assistant (Invoke Model API). Use the following product manual to answer questions.\n",
    "\n",
    "PRODUCT MANUAL:\n",
    "{document}\n",
    "\n",
    "Answer the user's question based on the information in the manual. If the answer is not in the manual, say so.\"\"\",\n",
    "                        # CACHE CHECKPOINT (Invoke Model API syntax)\n",
    "                        \"cache_control\": {\"type\": \"ephemeral\"}\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        # User query (dynamic, not cached)\n",
    "                        \"text\": f\"\\n\\nQUESTION: {user_query}\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Call Bedrock Invoke Model API\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        modelId=model_id,\n",
    "        body=json.dumps(request_body)\n",
    "    )\n",
    "    \n",
    "    # Parse response\n",
    "    response_body = json.loads(response['body'].read())\n",
    "    \n",
    "    # Extract response text\n",
    "    response_text = response_body['content'][0]['text']\n",
    "    \n",
    "    # Extract metrics (Invoke Model API format)\n",
    "    usage = response_body.get('usage', {})\n",
    "    metrics = {\n",
    "        'input_tokens': usage.get('input_tokens', 0),\n",
    "        'output_tokens': usage.get('output_tokens', 0),\n",
    "        'cache_write': usage.get('cache_creation_input_tokens', 0),  # Different key name\n",
    "        'cache_read': usage.get('cache_read_input_tokens', 0)        # Different key name\n",
    "    }\n",
    "    \n",
    "    return response_text, metrics\n",
    "\n",
    "print(\"‚úÖ Invoke Model API function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo: Converse API Example\n",
    "\n",
    "Let's test the caching with multiple queries using the Converse API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DEMO: Single-Checkpoint Caching with Converse API\n",
      "================================================================================\n",
      "\n",
      "üîç Query 1: What is the return policy for electronics?\n",
      "\n",
      "üí¨ Response: According to the product manual, **electronics have a 14-day return window** from the purchase date....\n",
      "\n",
      "============================================================\n",
      "Request 1\n",
      "============================================================\n",
      "Input tokens:       15\n",
      "Output tokens:      130\n",
      "Cache write tokens: 1,936\n",
      "Cache read tokens:  0\n",
      "============================================================\n",
      "\n",
      "\n",
      "üîç Query 2: How much does the Professional tier cost?\n",
      "\n",
      "üí¨ Response: According to the product manual, the **Professional tier costs $299/month**.\n",
      "\n",
      "This tier includes:\n",
      "- ...\n",
      "\n",
      "============================================================\n",
      "Request 2\n",
      "============================================================\n",
      "Input tokens:       15\n",
      "Output tokens:      88\n",
      "Cache write tokens: 0\n",
      "Cache read tokens:  1,936\n",
      "============================================================\n",
      "\n",
      "\n",
      "üîç Query 3: What shipping options are available?\n",
      "\n",
      "üí¨ Response: Based on the product manual, the following shipping options are available:\n",
      "\n",
      "1. **Standard Shipping**...\n",
      "\n",
      "============================================================\n",
      "Request 3\n",
      "============================================================\n",
      "Input tokens:       13\n",
      "Output tokens:      182\n",
      "Cache write tokens: 0\n",
      "Cache read tokens:  1,936\n",
      "============================================================\n",
      "\n",
      "\n",
      "üîç Query 4: What are the API rate limits for standard tier?\n",
      "\n",
      "üí¨ Response: According to the product manual, the API rate limits for the standard tier are **1,000 requests per ...\n",
      "\n",
      "============================================================\n",
      "Request 4\n",
      "============================================================\n",
      "Input tokens:       17\n",
      "Output tokens:      81\n",
      "Cache write tokens: 0\n",
      "Cache read tokens:  1,936\n",
      "============================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "COST ANALYSIS - Converse API\n",
      "================================================================================\n",
      "\n",
      "Total requests: 4\n",
      "Cache hit rate: 75.0%\n",
      "\n",
      "Cost with caching:    $0.009182\n",
      "Cost without caching: $0.023412\n",
      "\n",
      "üí∞ Savings: $0.014230 (60.8% reduction)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test queries\n",
    "queries = [\n",
    "    \"What is the return policy for electronics?\",\n",
    "    \"How much does the Professional tier cost?\",\n",
    "    \"What shipping options are available?\",\n",
    "    \"What are the API rate limits for standard tier?\"\n",
    "]\n",
    "\n",
    "all_metrics_converse = []\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DEMO: Single-Checkpoint Caching with Converse API\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, query in enumerate(queries, 1):\n",
    "    print(f\"\\nüîç Query {i}: {query}\")\n",
    "    \n",
    "    response, metrics = query_document_converse(query)\n",
    "    all_metrics_converse.append(metrics)\n",
    "    \n",
    "    print(f\"\\nüí¨ Response: {response[:100]}...\" if len(response) > 100 else f\"\\nüí¨ Response: {response}\")\n",
    "    print_cache_metrics(metrics, request_num=i)\n",
    "\n",
    "# Calculate savings\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COST ANALYSIS - Converse API\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "savings = calculate_cache_savings(all_metrics_converse)\n",
    "print(f\"\\nTotal requests: {savings['total_requests']}\")\n",
    "print(f\"Cache hit rate: {savings['cache_hit_rate']:.1f}%\")\n",
    "print(f\"\\nCost with caching:    ${savings['cost_with_cache']:.6f}\")\n",
    "print(f\"Cost without caching: ${savings['cost_no_cache']:.6f}\")\n",
    "print(f\"\\nüí∞ Savings: ${savings['savings']:.6f} ({savings['savings_pct']:.1f}% reduction)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo: Invoke Model API Example\n",
    "\n",
    "Now let's run the same queries using the Invoke Model API to see that both APIs produce identical results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DEMO: Single-Checkpoint Caching with Invoke Model API\n",
      "================================================================================\n",
      "\n",
      "üîç Query 1: What is the return policy for electronics?\n",
      "\n",
      "üí¨ Response: Based on the product manual, the return policy for electronics is:\n",
      "\n",
      "**Electronics have a 14-day retu...\n",
      "\n",
      "============================================================\n",
      "Request 1\n",
      "============================================================\n",
      "Input tokens:       15\n",
      "Output tokens:      145\n",
      "Cache write tokens: 1,941\n",
      "Cache read tokens:  0\n",
      "============================================================\n",
      "\n",
      "\n",
      "üîç Query 2: How much does the Professional tier cost?\n",
      "\n",
      "üí¨ Response: Based on the product manual, the **Professional tier costs $299/month**.\n",
      "\n",
      "This tier includes:\n",
      "- Up t...\n",
      "\n",
      "============================================================\n",
      "Request 2\n",
      "============================================================\n",
      "Input tokens:       15\n",
      "Output tokens:      88\n",
      "Cache write tokens: 0\n",
      "Cache read tokens:  1,941\n",
      "============================================================\n",
      "\n",
      "\n",
      "üîç Query 3: What shipping options are available?\n",
      "\n",
      "üí¨ Response: Based on the product manual, the following shipping options are available:\n",
      "\n",
      "1. **Standard Shipping**...\n",
      "\n",
      "============================================================\n",
      "Request 3\n",
      "============================================================\n",
      "Input tokens:       13\n",
      "Output tokens:      182\n",
      "Cache write tokens: 0\n",
      "Cache read tokens:  1,941\n",
      "============================================================\n",
      "\n",
      "\n",
      "üîç Query 4: What are the API rate limits for standard tier?\n",
      "\n",
      "üí¨ Response: Based on the product manual, the API rate limits for the standard tier are:\n",
      "\n",
      "**1,000 requests per mi...\n",
      "\n",
      "============================================================\n",
      "Request 4\n",
      "============================================================\n",
      "Input tokens:       17\n",
      "Output tokens:      125\n",
      "Cache write tokens: 0\n",
      "Cache read tokens:  1,941\n",
      "============================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "COST ANALYSIS - Invoke Model API\n",
      "================================================================================\n",
      "\n",
      "Total requests: 4\n",
      "Cache hit rate: 75.0%\n",
      "\n",
      "Cost with caching:    $0.009206\n",
      "Cost without caching: $0.023472\n",
      "\n",
      "üí∞ Savings: $0.014266 (60.8% reduction)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä COMPARISON: Converse vs Invoke Model\n",
      "================================================================================\n",
      "Both APIs achieve identical caching performance:\n",
      "  - Converse API cache hit rate: 75.0%\n",
      "  - Invoke Model API cache hit rate:   75.0%\n",
      "  - Both APIs produce the same caching behavior!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Same queries, using Invoke Model API\n",
    "all_metrics_invoke = []\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DEMO: Single-Checkpoint Caching with Invoke Model API\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, query in enumerate(queries, 1):\n",
    "    print(f\"\\nüîç Query {i}: {query}\")\n",
    "    \n",
    "    response, metrics = query_document_invoke(query)\n",
    "    all_metrics_invoke.append(metrics)\n",
    "    \n",
    "    print(f\"\\nüí¨ Response: {response[:100]}...\" if len(response) > 100 else f\"\\nüí¨ Response: {response}\")\n",
    "    print_cache_metrics(metrics, request_num=i)\n",
    "\n",
    "# Calculate savings\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COST ANALYSIS - Invoke Model API\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "savings_invoke = calculate_cache_savings(all_metrics_invoke)\n",
    "print(f\"\\nTotal requests: {savings_invoke['total_requests']}\")\n",
    "print(f\"Cache hit rate: {savings_invoke['cache_hit_rate']:.1f}%\")\n",
    "print(f\"\\nCost with caching:    ${savings_invoke['cost_with_cache']:.6f}\")\n",
    "print(f\"Cost without caching: ${savings_invoke['cost_no_cache']:.6f}\")\n",
    "print(f\"\\nüí∞ Savings: ${savings_invoke['savings']:.6f} ({savings_invoke['savings_pct']:.1f}% reduction)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"üìä COMPARISON: Converse vs Invoke Model\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Both APIs achieve identical caching performance:\")\n",
    "print(f\"  - Converse API cache hit rate: {savings['cache_hit_rate']:.1f}%\")\n",
    "print(f\"  - Invoke Model API cache hit rate:   {savings_invoke['cache_hit_rate']:.1f}%\")\n",
    "print(f\"  - Both APIs produce the same caching behavior!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Observations from Section 1\n",
    "\n",
    "From the demos above, you should see:\n",
    "\n",
    "1. **Request 1**: Cache write tokens = document size (~2,000 tokens)\n",
    "   - üìù First occurrence, cache write (investment)\n",
    "   - Higher cost: 1.25x regular input\n",
    "\n",
    "2. **Requests 2-4**: Cache read tokens = document size (~2,000 tokens)\n",
    "   - ‚úÖ Cache hit! Content retrieved from cache\n",
    "   - Lower cost: 0.1x regular input (~90% savings)\n",
    "\n",
    "3. **Savings**: Typically 60-80% cost reduction with 4 requests\n",
    "\n",
    "4. **API Differences**: Both Converse and Invoke Model APIs achieve identical caching results - only the syntax differs:\n",
    "   - Converse: `{\"cachePoint\": {\"type\": \"default\"}}`\n",
    "   - Invoke Model: `{\"cache_control\": {\"type\": \"ephemeral\"}}`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Multi-Checkpoint Caching\n",
    "\n",
    "In Section 1, we used a **single cache checkpoint** to separate static content (document) from dynamic content (user queries). \n",
    "\n",
    "For more complex workflows, Amazon Bedrock supports **up to 4 cache checkpoints per request**, allowing you to cache different layers independently based on how frequently they change.\n",
    "\n",
    "Cache checkpoints can be placed in:\n",
    "- **Tools**: Tool definitions (in toolConfig)\n",
    "- **System**: System prompts\n",
    "- **Messages**: Conversation history\n",
    "\n",
    "**Why multiple checkpoints?**\n",
    "- Different components have different update frequencies\n",
    "- Cache only what's stable, reprocess what changes\n",
    "- Maximum cost efficiency for complex agentic workflows\n",
    "\n",
    "**Key Points**:\n",
    "- **Token minimums vary by model**: Check [AWS Bedrock Prompt Caching Docs](https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-caching.html) for latest requirements\n",
    "  - Example: Claude Sonnet 4.5 = 1,024 tokens, Claude Haiku 4.5 = 4,096 tokens\n",
    "- Each checkpoint is **explicit** - you must add cache tags where you want caching\n",
    "\n",
    "### Use Case: Customer Support Agent with Tools\n",
    "For the following section, we'll implement a customer support agent with different cache checkpoints at Tools, System Prompts and Conversation history.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tool definitions loaded from file\n",
      "   - 5 tools defined\n",
      "   - Estimated tokens: ~2523 tokens\n"
     ]
    }
   ],
   "source": [
    "# Load tool definitions from file\n",
    "with open('data/tool_definitions.json', 'r') as f:\n",
    "    TOOL_DEFINITIONS = json.load(f)\n",
    "\n",
    "# Estimate token count for tools (character-based for better accuracy)\n",
    "# Token estimation: ~4 chars per token for JSON\n",
    "tool_json_str = json.dumps(TOOL_DEFINITIONS)\n",
    "estimated_tool_tokens = len(tool_json_str) / 4\n",
    "\n",
    "print(f\"‚úÖ Tool definitions loaded from file\")\n",
    "print(f\"   - {len(TOOL_DEFINITIONS)} tools defined\")\n",
    "print(f\"   - Estimated tokens: ~{int(estimated_tool_tokens)} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ System prompt loaded from file\n",
      "   Length: 938 words (~1219 tokens)\n"
     ]
    }
   ],
   "source": [
    "# System prompt\n",
    "with open('data/system_prompt.txt', 'r') as f:\n",
    "    SYSTEM_PROMPT = f.read()\n",
    "\n",
    "print(\"‚úÖ System prompt loaded from file\")\n",
    "print(f\"   Length: {len(SYSTEM_PROMPT.split())} words (~{int(len(SYSTEM_PROMPT.split())*1.3)} tokens)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mock Tool Functions\n",
    "\n",
    "For this demo, we'll create simple mock functions that simulate tool execution without actual API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Mock tool functions defined\n"
     ]
    }
   ],
   "source": [
    "# Mock tool functions for demo\n",
    "def search_knowledge_base(query):\n",
    "    \"\"\"Mock knowledge base search\"\"\"\n",
    "    return {\n",
    "        \"results\": f\"Knowledge base results for: {query}\",\n",
    "        \"source\": \"CloudCommerce Help Center\"\n",
    "    }\n",
    "\n",
    "def lookup_order(order_id):\n",
    "    \"\"\"Mock order lookup\"\"\"\n",
    "    return {\n",
    "        \"order_id\": order_id,\n",
    "        \"status\": \"Shipped\",\n",
    "        \"eta\": \"2025-01-25\",\n",
    "        \"tracking\": \"1Z999AA10123456784\"\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Mock tool functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Implementation with Multi-Checkpoint Caching\n",
    "\n",
    "The agent function implements three cache checkpoints:\n",
    "1. **Checkpoint 1**: After tools (in tools array) - ~2,500 tokens\n",
    "2. **Checkpoint 2**: After system prompt - ~1,200 tokens\n",
    "3. **Checkpoint 3**: At end of conversation history - grows with each turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Multi-checkpoint agent function defined\n",
      "‚úÖ Helper function for agent turns defined\n"
     ]
    }
   ],
   "source": [
    "def run_agent_with_caching(user_message, conversation_history=None, model_id=MODEL_ID):\n",
    "    \"\"\"\n",
    "    Run support agent with multi-checkpoint caching.\n",
    "    \n",
    "    Caching Strategy (3 checkpoints total):\n",
    "    - Tools: Cache checkpoint after tools\n",
    "    - System: Cache checkpoint after system prompt\n",
    "    - History: Cache checkpoint at END of entire conversation history\n",
    "    - Current message: Not cached (most dynamic)\n",
    "    \n",
    "    Args:\n",
    "        user_message: Current user message (can be None if only tool results in history)\n",
    "        conversation_history: List of previous messages (optional)\n",
    "        model_id: Bedrock model ID\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (response_text, cache_metrics, tool_calls, assistant_msg)\n",
    "    \"\"\"\n",
    "    if conversation_history is None:\n",
    "        conversation_history = []\n",
    "    \n",
    "    # Build messages - STRIP any existing cache checkpoints first\n",
    "    messages = []\n",
    "    \n",
    "    # Add conversation history WITHOUT cache checkpoints\n",
    "    if len(conversation_history) > 0:\n",
    "        for msg in conversation_history:\n",
    "            # Strip cache checkpoints from historical messages\n",
    "            clean_msg = {\"role\": msg[\"role\"], \"content\": []}\n",
    "            \n",
    "            # Clean content blocks (remove cache checkpoints)\n",
    "            if isinstance(msg[\"content\"], list):\n",
    "                for block in msg[\"content\"]:\n",
    "                    if \"text\" in block:\n",
    "                        clean_msg[\"content\"].append({\"text\": block[\"text\"]})\n",
    "                    elif \"toolUse\" in block:\n",
    "                        clean_msg[\"content\"].append({\"toolUse\": block[\"toolUse\"]})\n",
    "                    elif \"toolResult\" in block:\n",
    "                        # Tool results have their own structure (toolUseId + content)\n",
    "                        clean_msg[\"content\"].append({\"toolResult\": block[\"toolResult\"]})\n",
    "                    elif \"json\" in block:\n",
    "                        # Tool result content blocks\n",
    "                        clean_msg[\"content\"].append({\"json\": block[\"json\"]})\n",
    "                    elif \"toolUseId\" in block:\n",
    "                        # This is a tool result block (not wrapped in toolResult)\n",
    "                        clean_msg[\"content\"].append(block)\n",
    "                    # Skip cachePoint blocks\n",
    "            else:\n",
    "                # Simple string content\n",
    "                clean_msg[\"content\"] = [{\"text\": msg[\"content\"]}]\n",
    "            \n",
    "            messages.append(clean_msg)\n",
    "        \n",
    "        # CHECKPOINT 3: Add ONE cache checkpoint at end of entire history\n",
    "        messages[-1][\"content\"].append({\"cachePoint\": {\"type\": \"default\"}})\n",
    "    \n",
    "    # Add current user message ONLY if not empty/None\n",
    "    if user_message:\n",
    "        messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": user_message}]\n",
    "        })\n",
    "    \n",
    "    # CHECKPOINT 2: System prompt with cache checkpoint\n",
    "    system = [\n",
    "        {\"text\": SYSTEM_PROMPT},\n",
    "        {\"cachePoint\": {\"type\": \"default\"}}\n",
    "    ]\n",
    "    \n",
    "    # CHECKPOINT 1: Tools with cache checkpoint\n",
    "    tool_config = {\n",
    "        \"tools\": TOOL_DEFINITIONS + [{\"cachePoint\": {\"type\": \"default\"}}],\n",
    "        \"toolChoice\": {\"auto\": {}}\n",
    "    }\n",
    "    \n",
    "    response = bedrock_runtime.converse(\n",
    "        modelId=model_id,\n",
    "        messages=messages,\n",
    "        system=system,\n",
    "        toolConfig=tool_config,\n",
    "        inferenceConfig={\n",
    "            \"maxTokens\": 1000,\n",
    "            \"temperature\": 0.0\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Extract metrics\n",
    "    metrics = extract_cache_metrics(response)\n",
    "    \n",
    "    # Handle tool calls\n",
    "    stop_reason = response.get('stopReason')\n",
    "    \n",
    "    if stop_reason == 'tool_use':\n",
    "        tool_calls = []\n",
    "        for content_block in response['output']['message']['content']:\n",
    "            if 'toolUse' in content_block:\n",
    "                tool_calls.append(content_block['toolUse'])\n",
    "        return None, metrics, tool_calls, response['output']['message']\n",
    "    else:\n",
    "        response_text = response['output']['message']['content'][0]['text']\n",
    "        return response_text, metrics, None, None\n",
    "\n",
    "\n",
    "def run_agent_turn(query, conversation, all_metrics):\n",
    "    \"\"\"\n",
    "    Helper function to run a single agent turn with tool handling.\n",
    "    \n",
    "    Args:\n",
    "        query: User query string\n",
    "        conversation: Conversation history (will be modified in place)\n",
    "        all_metrics: Metrics list (will be modified in place)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (response_text, api_calls_list)\n",
    "        where each api_call dict contains: {\n",
    "            \"metrics\": {...},\n",
    "            \"stop_reason\": \"tool_use\" or \"end_turn\",\n",
    "            \"description\": \"human-readable description\"\n",
    "        }\n",
    "    \"\"\"\n",
    "    api_calls = []\n",
    "    \n",
    "    # API Call 1\n",
    "    response, metrics, tool_calls, assistant_msg = run_agent_with_caching(query, conversation_history=conversation)\n",
    "    all_metrics.append(metrics)\n",
    "    \n",
    "    # Record first API call\n",
    "    stop_reason = \"tool_use\" if tool_calls else \"end_turn\"\n",
    "    api_calls.append({\n",
    "        \"metrics\": metrics,\n",
    "        \"stop_reason\": stop_reason,\n",
    "        \"description\": \"Tool decision\" if tool_calls else \"Direct response\"\n",
    "    })\n",
    "    \n",
    "    # Handle tool calls if any\n",
    "    if tool_calls:\n",
    "        print(f\"\\nüîß Agent wants to use tools:\")\n",
    "        \n",
    "        # Build tool results\n",
    "        tool_results = []\n",
    "        for tool_call in tool_calls:\n",
    "            tool_name = tool_call['name']\n",
    "            tool_input = tool_call['input']\n",
    "            print(f\"   - {tool_name}({tool_input})\")\n",
    "            \n",
    "            # Execute tool (mock)\n",
    "            if tool_name == \"search_knowledge_base\":\n",
    "                result = search_knowledge_base(tool_input.get('query', ''))\n",
    "            elif tool_name == \"lookup_order\":\n",
    "                result = lookup_order(tool_input.get('order_id', ''))\n",
    "            \n",
    "            print(f\"   ‚Üí Result: {result}\")\n",
    "        \n",
    "            tool_results.append({\n",
    "                \"toolResult\": {\n",
    "                    \"toolUseId\": tool_call['toolUseId'],\n",
    "                    \"content\": [{\"json\": result}]\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        # Add to conversation\n",
    "        conversation.append({\"role\": \"user\", \"content\": [{\"text\": query}]})\n",
    "        conversation.append({\"role\": \"assistant\", \"content\": assistant_msg['content']})\n",
    "        conversation.append({\"role\": \"user\", \"content\": tool_results})\n",
    "        \n",
    "        # API Call 2 - get final response\n",
    "        response, final_metrics, _, _ = run_agent_with_caching(None, conversation_history=conversation)\n",
    "        all_metrics.append(final_metrics)\n",
    "        \n",
    "        # Record second API call\n",
    "        api_calls.append({\n",
    "            \"metrics\": final_metrics,\n",
    "            \"stop_reason\": \"end_turn\",\n",
    "            \"description\": \"Final response after tool execution\"\n",
    "        })\n",
    "        \n",
    "        # Add final response\n",
    "        conversation.append({\"role\": \"assistant\", \"content\": [{\"text\": response}]})\n",
    "    else:\n",
    "        # No tools, just add query and response\n",
    "        conversation.append({\"role\": \"user\", \"content\": [{\"text\": query}]})\n",
    "        conversation.append({\"role\": \"assistant\", \"content\": [{\"text\": response}]})\n",
    "    \n",
    "    return response, api_calls\n",
    "\n",
    "\n",
    "print(\"‚úÖ Multi-checkpoint agent function defined\")\n",
    "print(\"‚úÖ Helper function for agent turns defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo: Multi-Turn Conversation with Tools\n",
    "\n",
    "**Instructions**: Run each cell below sequentially to see how caching evolves across conversation turns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Turn 1: First Request (Cache Write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Turn 1\n",
      "================================================================================\n",
      "\n",
      "üë§ User: I need help with my order ORD-12345\n",
      "\n",
      "üîß Agent wants to use tools:\n",
      "   - lookup_order({'order_id': 'ORD-12345', 'include_history': True})\n",
      "   ‚Üí Result: {'order_id': 'ORD-12345', 'status': 'Shipped', 'eta': '2025-01-25', 'tracking': '1Z999AA10123456784'}\n",
      "\n",
      "ü§ñ Agent: Great news! I found your order details:\n",
      "\n",
      "**Order Status:** Shipped ‚úì\n",
      "\n",
      "**Tracking Information:**\n",
      "- Tracking Number: 1Z999AA10123456784\n",
      "- Estimated Delivery: January 25, 2025\n",
      "\n",
      "Your order is currently on its way! You can track your package using the tracking number above with the shipping carrier.\n",
      "\n",
      "What specific help do you need with this order? For example:\n",
      "- Need more details about the delivery?\n",
      "- Want to make changes or cancel?\n",
      "- Have questions about the items ordered?\n",
      "- Need to initiate a return?\n",
      "\n",
      "I'm here to assist!\n",
      "\n",
      "--- Cache Metrics for Turn 1 ---\n",
      "\n",
      "üìä API Call #1 (Tool decision):\n",
      "\n",
      "============================================================\n",
      "Request 1\n",
      "============================================================\n",
      "Input tokens:       334\n",
      "Output tokens:      106\n",
      "Cache write tokens: 4,052\n",
      "Cache read tokens:  0\n",
      "============================================================\n",
      "\n",
      "\n",
      "üìä API Call #2 (Final response after tool execution):\n",
      "\n",
      "============================================================\n",
      "Request 2\n",
      "============================================================\n",
      "Input tokens:       5\n",
      "Output tokens:      138\n",
      "Cache write tokens: 484\n",
      "Cache read tokens:  4,052\n",
      "============================================================\n",
      "\n",
      "\n",
      "üí° Turn 1 Summary: Made 2 API call(s) total\n"
     ]
    }
   ],
   "source": [
    "# Initialize conversation tracking\n",
    "conversation = []\n",
    "all_metrics = []\n",
    "\n",
    "# Turn 1: First user query\n",
    "query = \"I need help with my order ORD-12345\"\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Turn 1\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nüë§ User: {query}\")\n",
    "\n",
    "# Run agent turn (handles tools automatically)\n",
    "response, api_calls = run_agent_turn(query, conversation, all_metrics)\n",
    "\n",
    "print(f\"\\nü§ñ Agent: {response}\")\n",
    "\n",
    "# Print cache metrics for all API calls in this turn\n",
    "print(f\"\\n--- Cache Metrics for Turn 1 ---\")\n",
    "for i, call in enumerate(api_calls, 1):\n",
    "    call_number = len(all_metrics) - len(api_calls) + i\n",
    "    print(f\"\\nüìä API Call #{call_number} ({call['description']}):\")\n",
    "    print_cache_metrics(call['metrics'], request_num=call_number)\n",
    "\n",
    "print(f\"\\nüí° Turn 1 Summary: Made {len(api_calls)} API call(s) total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° What happened in Turn 1?**\n",
    "\n",
    "This is the **\"cold start\"** - the initial investment in caching:\n",
    "\n",
    "**API Call #1** (Tool decision):\n",
    "- üìù **Cache WRITE**: Tools + System = ~4,000 tokens cached\n",
    "- üÜï **Fresh processing**: User message \"I need help with my order...\"\n",
    "- üí∞ **Cost**: 1.25x regular price for cache write\n",
    "\n",
    "**API Call #2** (Final response after tool execution):\n",
    "- ‚úÖ **Cache HIT**: Tools + System (already cached in API Call #1 - instant reuse)\n",
    "- üìù **Cache WRITE**: Turn 1 conversation with tool results (new content)\n",
    "- üÜï **Fresh processing**: Nothing (only processing tool results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Turn 2: Cache Hit for Tools + System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Turn 2\n",
      "================================================================================\n",
      "\n",
      "üë§ User: When will it arrive?\n",
      "\n",
      "ü§ñ Agent: Based on the tracking information for order ORD-12345, your package is estimated to arrive on **January 25, 2025**.\n",
      "\n",
      "Since your order has already shipped, you can track its real-time progress using tracking number **1Z999AA10123456784** on the carrier's website for the most up-to-date delivery information.\n",
      "\n",
      "Is there anything else you'd like to know about your order?\n",
      "\n",
      "--- Cache Metrics for Turn 2 ---\n",
      "\n",
      "üìä API Call #3 (Direct response):\n",
      "\n",
      "============================================================\n",
      "Request 3\n",
      "============================================================\n",
      "Input tokens:       10\n",
      "Output tokens:      92\n",
      "Cache write tokens: 141\n",
      "Cache read tokens:  4,536\n",
      "============================================================\n",
      "\n",
      "\n",
      "üí° Turn 2 Summary: Made 1 API call(s) this turn (3 total so far)\n"
     ]
    }
   ],
   "source": [
    "query = \"When will it arrive?\"\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Turn 2\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nüë§ User: {query}\")\n",
    "\n",
    "# Track how many API calls before this turn\n",
    "calls_before = len(all_metrics)\n",
    "\n",
    "# Run agent turn (handles tools automatically)\n",
    "response, api_calls = run_agent_turn(query, conversation, all_metrics)\n",
    "\n",
    "print(f\"\\nü§ñ Agent: {response}\")\n",
    "\n",
    "# Print cache metrics for all API calls in this turn\n",
    "print(f\"\\n--- Cache Metrics for Turn 2 ---\")\n",
    "for i, call in enumerate(api_calls, 1):\n",
    "    call_number = calls_before + i\n",
    "    print(f\"\\nüìä API Call #{call_number} ({call['description']}):\")\n",
    "    print_cache_metrics(call['metrics'], request_num=call_number)\n",
    "\n",
    "print(f\"\\nüí° Turn 2 Summary: Made {len(api_calls)} API call(s) this turn ({len(all_metrics)} total so far)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° What happened in Turn 2?**\n",
    "\n",
    "Cache behavior you'll see in the metrics above:\n",
    "- ‚úÖ **Cache HIT**: Tools + System (reused from Turn 1)\n",
    "- üìù **Cache WRITE**: Turn 1's conversation (first time seeing it, so cache it now)\n",
    "- üÜï **Fresh processing**: Current message \"When will it arrive?\"\n",
    "\n",
    "**Why?** Tools and System stay the same across turns, so Bedrock reuses the cached version. Turn 1's conversation is new to the cache, so it gets written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Turn 3\n",
      "================================================================================\n",
      "\n",
      "üë§ User: Can you search the knowledge base about product warranty coverage?\n",
      "\n",
      "üîß Agent wants to use tools:\n",
      "   - search_knowledge_base({'query': 'product warranty coverage', 'category': 'products'})\n",
      "   ‚Üí Result: {'results': 'Knowledge base results for: product warranty coverage', 'source': 'CloudCommerce Help Center'}\n",
      "\n",
      "ü§ñ Agent: I've searched our knowledge base for product warranty coverage information. Here's what I found:\n",
      "\n",
      "**Product Warranty Coverage:**\n",
      "\n",
      "Our warranty policies vary by product category and manufacturer. Generally:\n",
      "\n",
      "- **Manufacturer's Warranty:** Most products come with the manufacturer's standard warranty (typically 1 year for electronics, varies by brand)\n",
      "- **Extended Warranty:** Available for purchase at checkout for eligible items\n",
      "- **Defective Items:** Covered regardless of warranty status - full refund or replacement with no time limit\n",
      "- **Warranty Claims:** Can be processed through CloudCommerce or directly with the manufacturer\n",
      "\n",
      "For specific warranty details about a particular product, I can help you with:\n",
      "- Checking warranty terms for items in your order ORD-12345\n",
      "- Looking up warranty information for a specific product SKU\n",
      "- Explaining how to file a warranty claim\n",
      "\n",
      "Would you like me to provide more specific information about warranty coverage for the items in your order, or do you have questions about a particular product?\n",
      "\n",
      "--- Cache Metrics for Turn 3 ---\n",
      "\n",
      "üìä API Call #4 (Tool decision):\n",
      "\n",
      "============================================================\n",
      "Request 4\n",
      "============================================================\n",
      "Input tokens:       16\n",
      "Output tokens:      74\n",
      "Cache write tokens: 100\n",
      "Cache read tokens:  4,677\n",
      "============================================================\n",
      "\n",
      "\n",
      "üìä API Call #5 (Final response after tool execution):\n",
      "\n",
      "============================================================\n",
      "Request 5\n",
      "============================================================\n",
      "Input tokens:       5\n",
      "Output tokens:      215\n",
      "Cache write tokens: 118\n",
      "Cache read tokens:  4,777\n",
      "============================================================\n",
      "\n",
      "\n",
      "üí° Turn 3 Summary: Made 2 API call(s) this turn (5 total so far)\n"
     ]
    }
   ],
   "source": [
    "query = \"Can you search the knowledge base about product warranty coverage?\"\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Turn 3\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nüë§ User: {query}\")\n",
    "\n",
    "# Track how many API calls before this turn\n",
    "calls_before = len(all_metrics)\n",
    "\n",
    "# Run agent turn (handles tools automatically)\n",
    "response, api_calls = run_agent_turn(query, conversation, all_metrics)\n",
    "\n",
    "print(f\"\\nü§ñ Agent: {response}\")\n",
    "\n",
    "# Print cache metrics for all API calls in this turn\n",
    "print(f\"\\n--- Cache Metrics for Turn 3 ---\")\n",
    "for i, call in enumerate(api_calls, 1):\n",
    "    call_number = calls_before + i\n",
    "    print(f\"\\nüìä API Call #{call_number} ({call['description']}):\")\n",
    "    print_cache_metrics(call['metrics'], request_num=call_number)\n",
    "\n",
    "print(f\"\\nüí° Turn 3 Summary: Made {len(api_calls)} API call(s) this turn ({len(all_metrics)} total so far)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° What happened in Turn 3?**\n",
    "\n",
    "Cache behavior you'll see in the metrics above:\n",
    "- ‚úÖ **Cache HIT**: Tools + System + Turn 1 conversation (all reused - maximum cache efficiency)\n",
    "- üìù **Cache WRITE**: Turn 2's conversation (first time seeing it, so cache it now)\n",
    "- üÜï **Fresh processing**: Current message about warranty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COST ANALYSIS - Multi-Checkpoint Caching\n",
      "================================================================================\n",
      "\n",
      "Total API calls: 5\n",
      "Cache hit rate: 78.7%\n",
      "\n",
      "Cost with caching:    $0.024879\n",
      "Cost without caching: $0.069921\n",
      "\n",
      "üí∞ Savings: $0.045042 (64.4% reduction)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Calculate savings across all turns\n",
    "print(\"=\"*80)\n",
    "print(\"COST ANALYSIS - Multi-Checkpoint Caching\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "savings = calculate_cache_savings(all_metrics)\n",
    "print(f\"\\nTotal API calls: {savings['total_requests']}\")\n",
    "print(f\"Cache hit rate: {savings['cache_hit_rate']:.1f}%\")\n",
    "print(f\"\\nCost with caching:    ${savings['cost_with_cache']:.6f}\")\n",
    "print(f\"Cost without caching: ${savings['cost_no_cache']:.6f}\")\n",
    "print(f\"\\nüí∞ Savings: ${savings['savings']:.6f} ({savings['savings_pct']:.1f}% reduction)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Observations from Section 2\n",
    "\n",
    "**Cache behavior across conversation turns**: As the conversation grows, MORE content gets cached and reused. This is why cache hit rate increases over time.\n",
    "\n",
    "**The key insight**: \n",
    "- Turn 1: Cache everything for first time (higher investment)\n",
    "- Turn 2: Reuse 2 layers (Tools + System)\n",
    "- Turn 3: Reuse 3 layers (Tools + System + Turn 1)\n",
    "- Turn N: Keep reusing more ‚Üí even higher savings!\n",
    "\n",
    "**When to use multi-checkpoint caching**:\n",
    "- Production agentic workflows with tools\n",
    "- Different layers have different update frequencies\n",
    "- Long conversations where history accumulates\n",
    "- Maximum control over what gets cached\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Cache Invalidation\n",
    "\n",
    "Now that you've seen multi-checkpoint caching in action, while caching is very useful in reducing both cost and latency, it's important to understand how Bedrock cache invalidation works to design the right caching strategy and maximize the benefits.\n",
    "\n",
    "### Bedrock Prompt Assembly Order\n",
    "\n",
    "When Bedrock assembles your prompt for inference, it follows this **strict sequential order**:\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   Tools     ‚îÇ  ‚Üí   ‚îÇ   System    ‚îÇ  ‚Üí   ‚îÇ  Messages   ‚îÇ\n",
    "‚îÇ(if provided)‚îÇ      ‚îÇ(if provided)‚îÇ      ‚îÇ  (history + ‚îÇ\n",
    "‚îÇ             ‚îÇ      ‚îÇ             ‚îÇ      ‚îÇ   current)  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "This sequential assembly is the foundation of cache invalidation behavior. Bedrock uses **prefix matching** to determine cache hits:\n",
    "- Caches are matched from the **beginning** of the assembled prompt\n",
    "- When you add a cache checkpoint, Bedrock caches **everything from the start up to that checkpoint** - not just the individual section (cumulative caching)\n",
    "- If content at any position changes, **that cache + all subsequent caches** are invalidated\n",
    "- Only preceding caches (earlier in the sequence) remain valid\n",
    "\n",
    "**Note**: You can place **multiple cache checkpoints** within any section (Tools, System, or Messages), with a maximum of **4 checkpoints per request**. For example, in Messages, you can have multiple cache points across different turns of conversation history.\n",
    "\n",
    "### Visual Examples\n",
    "\n",
    "**Example 1: Only message changes** ‚úÖ\n",
    "```\n",
    "Request 1: [Tools‚úì] ‚Üí [System‚úì] ‚Üí [Message: \"Hello\"]\n",
    "           Cache‚ÇÅ     Cache‚ÇÇ      (no cache)\n",
    "Request 2: [Tools‚úì] ‚Üí [System‚úì] ‚Üí [Message: \"Help me\"]\n",
    "Result:    HIT‚úÖ     HIT‚úÖ        Fresh (not cached)\n",
    "```\n",
    "**Cache placement**: Cache‚ÇÅ after Tools, Cache‚ÇÇ after System\n",
    "\n",
    "**Outcome**: Tools and System cache hits because they're unchanged and come before messages!\n",
    "\n",
    "---\n",
    "\n",
    "**Example 2: System changes** ‚ö†Ô∏è\n",
    "```\n",
    "Request 1: [Tools‚úì] ‚Üí [System: \"Be helpful\"‚úì] ‚Üí [Message]\n",
    "           Cache‚ÇÅ     Cache‚ÇÇ                    (no cache)\n",
    "Request 2: [Tools‚úì] ‚Üí [System: \"Be concise\"‚úó] ‚Üí [Message]\n",
    "Result:    HIT‚úÖ     MISS‚ùå                       MISS‚ùå\n",
    "```\n",
    "**Cache placement**: Cache‚ÇÅ after Tools, Cache‚ÇÇ after System\n",
    "\n",
    "**Outcome**: \n",
    "- Cache‚ÇÅ HIT (unchanged, comes first)\n",
    "- Cache‚ÇÇ MISS (content changed)\n",
    "- Any downstream caches also invalidated (even if message was the same!)\n",
    "\n",
    "---\n",
    "\n",
    "**Example 3: Tools change** ‚ùå‚ùå\n",
    "```\n",
    "Request 1: [Tools: A, B‚úì] ‚Üí [System‚úì] ‚Üí [Message]\n",
    "           Cache‚ÇÅ         Cache‚ÇÇ      (no cache)\n",
    "Request 2: [Tools: A, B, C‚úó] ‚Üí [System‚úì] ‚Üí [Message]\n",
    "Result:    MISS‚ùå              MISS‚ùå      MISS‚ùå\n",
    "```\n",
    "**Cache placement**: Cache‚ÇÅ after Tools, Cache‚ÇÇ after System\n",
    "\n",
    "**Outcome**: Everything invalidated! Tools are first in assembly order, so changing them breaks the entire cache chain.\n",
    "\n",
    "---\n",
    "\n",
    "**Example 4: Multiple cache points in Messages** üìö\n",
    "```\n",
    "Request 1: [Tools‚úì] ‚Üí [System‚úì] ‚Üí [Msg‚ÇÅ‚úì, Msg‚ÇÇ‚úì, Msg‚ÇÉ‚úì]\n",
    "           Cache‚ÇÅ     Cache‚ÇÇ      Cache‚ÇÉ Cache‚ÇÑ  (no cache)\n",
    "Request 2: [Tools‚úì] ‚Üí [System‚úì] ‚Üí [Msg‚ÇÅ‚úì, Msg‚ÇÇ‚úì, Msg‚ÇÉ‚úì, Msg‚ÇÑ]\n",
    "Result:    HIT‚úÖ     HIT‚úÖ        HIT‚úÖ   HIT‚úÖ   Fresh\n",
    "```\n",
    "**Cache placement**: Cache‚ÇÅ after Tools, Cache‚ÇÇ after System, Cache‚ÇÉ after Msg‚ÇÅ, Cache‚ÇÑ after Msg‚ÇÇ (4 checkpoints max)\n",
    "\n",
    "**Outcome**: Multiple cache points in the Messages section allow incremental caching of conversation history.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see these scenarios in action with a data analyst agent example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data analyst scenario loaded\n",
      "   Tools: 2 tools (~1182 tokens)\n",
      "   System V1: ~566 words (~735 tokens)\n",
      "   System V2: ~567 words (~735 tokens)\n"
     ]
    }
   ],
   "source": [
    "# Load data analyst tools and system\n",
    "with open('data/analyst_tools.json', 'r') as f:\n",
    "    ANALYST_TOOLS = json.load(f)\n",
    "\n",
    "with open('data/analyst_system.txt', 'r') as f:\n",
    "    ANALYST_SYSTEM_BASE = f.read()\n",
    "\n",
    "# Create two versions with different priorities\n",
    "ANALYST_SYSTEM_V1 = ANALYST_SYSTEM_BASE + \"\\n\\nPRIORITY: Focus on accuracy and thoroughness.\"\n",
    "ANALYST_SYSTEM_V2 = ANALYST_SYSTEM_BASE + \"\\n\\nPRIORITY: Focus on speed and quick insights.\"\n",
    "\n",
    "# Calculate estimated token counts\n",
    "tool_json_str = json.dumps(ANALYST_TOOLS)\n",
    "estimated_tool_tokens = len(tool_json_str) / 4\n",
    "estimated_system_tokens = len(ANALYST_SYSTEM_V1.split()) * 1.3\n",
    "\n",
    "print(\"‚úÖ Data analyst scenario loaded\")\n",
    "print(f\"   Tools: {len(ANALYST_TOOLS)} tools (~{int(estimated_tool_tokens)} tokens)\")\n",
    "print(f\"   System V1: ~{len(ANALYST_SYSTEM_V1.split())} words (~{int(estimated_system_tokens)} tokens)\")\n",
    "print(f\"   System V2: ~{len(ANALYST_SYSTEM_V2.split())} words (~{int(estimated_system_tokens)} tokens)\")\n",
    "\n",
    "# Helper function for analyst demos\n",
    "def run_analyst_demo(user_message, system_text, tools_list, marker=\"\"):\n",
    "    \"\"\"Run analyst agent with specified tools and system\"\"\"\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": user_message + marker}]  # Marker for unique caches\n",
    "    }]\n",
    "    \n",
    "    system = [\n",
    "        {\"text\": system_text},\n",
    "        {\"cachePoint\": {\"type\": \"default\"}}\n",
    "    ]\n",
    "    \n",
    "    tool_config = {\n",
    "        \"tools\": tools_list + [{\"cachePoint\": {\"type\": \"default\"}}],\n",
    "        \"toolChoice\": {\"auto\": {}}\n",
    "    }\n",
    "    \n",
    "    response = bedrock_runtime.converse(\n",
    "        modelId=MODEL_ID,\n",
    "        messages=messages,\n",
    "        system=system,\n",
    "        toolConfig=tool_config,\n",
    "        inferenceConfig={\"maxTokens\": 500, \"temperature\": 0.0}\n",
    "    )\n",
    "    \n",
    "    return extract_cache_metrics(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è Important: Cumulative Token Counting\n",
    "\n",
    "**Notice something interesting?**\n",
    "- Tools = ~1,200 tokens ‚úÖ (meets 1,024 minimum)\n",
    "- System = ~750 tokens ‚ùå (below 1,024 minimum)\n",
    "\n",
    "**Question**: How can System be cached if it's only 750 tokens?\n",
    "\n",
    "**Answer**: Remember cumulative caching from earlier? Bedrock counts tokens **from the start** to each checkpoint, not individually.\n",
    "\n",
    "**Checkpoint 1** (after Tools):\n",
    "```\n",
    "[Tools: 1,200 tokens] ‚Üí CHECKPOINT\n",
    "Total: 1,200 tokens ‚úÖ ‚Üí CACHED\n",
    "```\n",
    "\n",
    "**Checkpoint 2** (after System):\n",
    "```\n",
    "[Tools: 1,200] + [System: 750] ‚Üí CHECKPOINT\n",
    "Total: 1,950 tokens ‚úÖ ‚Üí CACHED (Tools + System together)\n",
    "```\n",
    "\n",
    "**Key insight**: Even if System alone is below 1,024 tokens, the **cumulative total from the beginning (1,950 tokens)** meets the minimum, so Checkpoint 2 caches the entire Tools + System prefix.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test with our 1st scenario by changing the system prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SCENARIO 1: System Prompt Changes\n",
      "================================================================================\n",
      "Demonstrating: Tools stay same, System changes ‚Üí Tools HIT, System MISS\n",
      "================================================================================\n",
      "\n",
      "üìä Request 1: System = 'Focus on accuracy'\n",
      "\n",
      "============================================================\n",
      "Cache Metrics\n",
      "============================================================\n",
      "Input tokens:       331\n",
      "Output tokens:      75\n",
      "Cache write tokens: 1,946\n",
      "Cache read tokens:  0\n",
      "============================================================\n",
      "\n",
      "\n",
      "üìä Request 2: System = 'Focus on speed' (CHANGED!)\n",
      "\n",
      "============================================================\n",
      "Cache Metrics\n",
      "============================================================\n",
      "Input tokens:       331\n",
      "Output tokens:      75\n",
      "Cache write tokens: 753\n",
      "Cache read tokens:  1,193\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SCENARIO 1: System Prompt Changes\")\n",
    "print(\"=\"*80)\n",
    "print(\"Demonstrating: Tools stay same, System changes ‚Üí Tools HIT, System MISS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Request 1: Analyst with \"accuracy priority\"\n",
    "metrics1 = run_analyst_demo(\n",
    "    user_message=\"Show sales trends\", \n",
    "    system_text=ANALYST_SYSTEM_V1,\n",
    "    tools_list=ANALYST_TOOLS,\n",
    "    marker=\" [Scenario1]\"\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Request 1: System = 'Focus on accuracy'\")\n",
    "print_cache_metrics(metrics1)\n",
    "\n",
    "# Request 2: Analyst with \"speed priority\" (System changed!)\n",
    "metrics2 = run_analyst_demo(\n",
    "    user_message=\"Show sales trends\",\n",
    "    system_text=ANALYST_SYSTEM_V2,  # DIFFERENT SYSTEM!\n",
    "    tools_list=ANALYST_TOOLS,  # Same tools\n",
    "    marker=\" [Scenario1]\"\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Request 2: System = 'Focus on speed' (CHANGED!)\")\n",
    "print_cache_metrics(metrics2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° What happened in Scenario 1?**\n",
    "\n",
    "**Request 1** (System prompt = '...Focus on accuracy'):\n",
    "- üìù **Cache WRITE**: Tools (~1,200 tokens) + System V1 (~750 tokens) = ~2,000 tokens cached\n",
    "- üÜï **Fresh processing**: User message \"Show sales trends [Scenario1]\"\n",
    "- üí∞ **Cost**: 1.25x for cache write (initial investment)\n",
    "\n",
    "**Request 2** (System prompt = '...Focus on speed' - CHANGED!):\n",
    "- ‚úÖ **Cache HIT**: Tools (~1,200 tokens) - same tools, reused from Request 1\n",
    "- üìù **Cache WRITE**: System V2 (~750 tokens) - different priority, new cache\n",
    "- üÜï **Fresh processing**: Same user message\n",
    "\n",
    "**Key insight**: \n",
    "- Tools come FIRST in assembly order ‚Üí Tools cache survives system change ‚úÖ\n",
    "- Changing System only invalidates System cache and beyond, NOT Tools\n",
    "- Partial cache reuse saves cost even when System changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test with our 2st scenario by changing the tools definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SCENARIO 2: Tools Change\n",
      "================================================================================\n",
      "Demonstrating: Tools change ‚Üí Everything invalidated (Tools + System)\n",
      "================================================================================\n",
      "\n",
      "üìä Request 1: Same Tools + System V2 (from Scenario 1)\n",
      "\n",
      "============================================================\n",
      "Cache Metrics\n",
      "============================================================\n",
      "Input tokens:       331\n",
      "Output tokens:      192\n",
      "Cache write tokens: 0\n",
      "Cache read tokens:  1,946\n",
      "============================================================\n",
      "\n",
      "\n",
      "üìä Request 2: Tools = [query_database, generate_chart, export_report] (CHANGED!)\n",
      "\n",
      "============================================================\n",
      "Cache Metrics\n",
      "============================================================\n",
      "Input tokens:       331\n",
      "Output tokens:      195\n",
      "Cache write tokens: 2,031\n",
      "Cache read tokens:  0\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SCENARIO 2: Tools Change\")\n",
    "print(\"=\"*80)\n",
    "print(\"Demonstrating: Tools change ‚Üí Everything invalidated (Tools + System)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Request 1: Continue from Scenario 1's end state (same Tools + System V2)\n",
    "# This should show FULL cache hit since both were already cached\n",
    "metrics1 = run_analyst_demo(\n",
    "    user_message=\"Analyze data\",\n",
    "    system_text=ANALYST_SYSTEM_V2,  # Same System V2 from end of Scenario 1\n",
    "    tools_list=ANALYST_TOOLS,  # Same tools: query_database, generate_chart\n",
    "    marker=\" [Scenario2]\"  # Different marker to separate from Scenario 1\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Request 1: Same Tools + System V2 (from Scenario 1)\")\n",
    "print_cache_metrics(metrics1)\n",
    "\n",
    "# Create a new tool (export_report)\n",
    "NEW_ANALYST_TOOL = {\n",
    "    \"toolSpec\": {\n",
    "        \"name\": \"export_report\",\n",
    "        \"description\": \"Export analysis report to PDF or Excel format\",\n",
    "        \"inputSchema\": {\n",
    "            \"json\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"format\": {\"type\": \"string\", \"enum\": [\"pdf\", \"excel\"]},\n",
    "                    \"data\": {\"type\": \"object\", \"description\": \"Report data\"}\n",
    "                },\n",
    "                \"required\": [\"format\", \"data\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Request 2: Add third tool (Tools changed!)\n",
    "ANALYST_TOOLS_MODIFIED = [NEW_ANALYST_TOOL] + ANALYST_TOOLS\n",
    "metrics2 = run_analyst_demo(\n",
    "    user_message=\"Analyze data\",\n",
    "    system_text=ANALYST_SYSTEM_V2,  # Same System V2\n",
    "    tools_list=ANALYST_TOOLS_MODIFIED,  # 3 tools now (CHANGED!)\n",
    "    marker=\" [Scenario2]\"\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Request 2: Tools = [query_database, generate_chart, export_report] (CHANGED!)\")\n",
    "print_cache_metrics(metrics2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° What happened in Scenario 2?**\n",
    "\n",
    "**Request 1** (Same Tools + System V2 from Scenario 1):\n",
    "- ‚úÖ **Cache HIT**: Tools (~1,200 tokens) & System V2 (~750 tokens) - already cached in Scenario 1\n",
    "- üí∞ **Cost**: Only 0.1x for cache reads - maximum savings!\n",
    "\n",
    "**Request 2** (Added third tool - CHANGED!):\n",
    "- ‚ùå **Cache MISS**: Tools (~1,300 tokens) - content changed (3 tools instead of 2)\n",
    "- ‚ùå **Cache MISS**: System V2 (~750 tokens) - invalidated due to Tools change\n",
    "- üìù **Cache WRITE**: New Tools (3 tools) + System V2 written to cache\n",
    "- üí∞ **Cost**: 1.25x for cache write - expensive restart!\n",
    "\n",
    "**Key insight**: \n",
    "- Tools come FIRST ‚Üí Changing tools breaks ENTIRE cache chain\n",
    "- Even though System V2 was unchanged, it got invalidated because Tools (which come before it) changed\n",
    "- Adding/removing/modifying tools is the most expensive cache invalidation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Continue with Part B)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws-bedrock-prompt-optimization-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
