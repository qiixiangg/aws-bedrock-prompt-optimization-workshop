{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Getting Started\n",
    "\n",
    "**Duration**: ~20 minutes\n",
    "\n",
    "The basics you need before optimizing prompts and costs.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. How LLMs Work\n",
    "\n",
    "[Large Language Models (LLMs)](https://en.wikipedia.org/wiki/Large_language_model) are AI systems that understand and generate content. Modern models are multimodal - they can process text, images, and more.\n",
    "\n",
    "```\n",
    "┌─────────────┐      ┌─────────────┐      ┌─────────────┐\n",
    "│   PROMPT    │  →   │    MODEL    │  →   │  RESPONSE   │\n",
    "│  (Input)    │      │  (Claude,   │      │  (Output)   │\n",
    "│             │      │   Nova)     │      │             │\n",
    "└─────────────┘      └─────────────┘      └─────────────┘\n",
    "  \"What is the         Processing         \"The capital\n",
    "   capital of           happens            of France\n",
    "   France?\"             here               is Paris.\"\n",
    "```\n",
    "\n",
    "**Key concept**: Models don't read words - they read **tokens** (subword units that models process)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Amazon Bedrock for Inference\n",
    "\n",
    "**Amazon Bedrock** is a fully managed service for building generative AI applications. This workshop focuses on **model inference** - calling LLMs via API.\n",
    "\n",
    "### Why Bedrock?\n",
    "\n",
    "| Benefit | Description |\n",
    "|---------|-------------|\n",
    "| **Multiple models** | Claude, Nova, Llama, Mistral - one API |\n",
    "| **No infrastructure** | Fully managed, auto-scales |\n",
    "| **Pay-per-use** | Only pay for tokens processed |\n",
    "| **Enterprise ready** | Security, compliance, VPC support |\n",
    "\n",
    "### Inference Pricing Modes\n",
    "\n",
    "| Mode | Best For |\n",
    "|------|----------|\n",
    "| **On-demand** | Variable workloads, no commitment |\n",
    "| **Provisioned Throughput** | Predictable high-volume, consistent latency |\n",
    "| **Batch** | Non-urgent processing, up to 50% cheaper |\n",
    "\n",
    "This workshop uses **on-demand** inference with the **Converse API**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokens: The Unit of Everything\n",
    "\n",
    "**Tokens** are how LLMs process text - and how you're billed.\n",
    "\n",
    "| Text | Approximate Tokens |\n",
    "|------|-------------------|\n",
    "| 1 sentence | ~10-20 tokens |\n",
    "| 1 paragraph | ~50-100 tokens |\n",
    "| 1 page | ~300-500 tokens |\n",
    "\n",
    "**Why tokens matter**:\n",
    "- **Cost**: You pay per token (input + output)\n",
    "- **Latency**: More tokens = longer processing\n",
    "- **Limits**: Models have maximum context windows\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setup\n",
    "\n",
    "Let's connect to Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "\n",
    "# Initialize Bedrock client\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name=os.getenv('AWS_DEFAULT_REGION', 'us-east-1')\n",
    ")\n",
    "\n",
    "print(\"Connected to Amazon Bedrock\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using US cross-region inference profile for Claude Haiku 4.5\n",
    "MODEL_ID = \"us.anthropic.claude-haiku-4-5-20251001-v1:0\"\n",
    "print(f\"Model: {MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Available Models\n",
    "\n",
    "Bedrock offers models from multiple providers. Models vary by:\n",
    "- **Capability**: Simple tasks vs complex reasoning\n",
    "- **Speed**: Latency per request\n",
    "- **Cost**: Price per token\n",
    "- **Context window**: Maximum input + output tokens\n",
    "\n",
    "### General Guidance\n",
    "\n",
    "| Task Complexity | Model Tier | Examples |\n",
    "|-----------------|------------|----------|\n",
    "| Simple (classification, extraction) | Fast & cheap | Haiku, Nova Lite/Micro |\n",
    "| Medium (Q&A, summarization) | Balanced | Sonnet, Nova Pro |\n",
    "| Complex (reasoning, code, research) | Most capable | Opus, Nova Premier |\n",
    "\n",
    "**Rule of thumb**: Start with cheaper models, upgrade only if quality is insufficient.\n",
    "\n",
    "> See [Amazon Bedrock Pricing](https://aws.amazon.com/bedrock/pricing/) for current models and pricing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available models in your region\n",
    "bedrock = boto3.client('bedrock', region_name=os.getenv('AWS_DEFAULT_REGION', 'us-east-1'))\n",
    "\n",
    "try:\n",
    "    response = bedrock.list_foundation_models(byOutputModality='TEXT')\n",
    "    \n",
    "    # Group by provider\n",
    "    providers = {}\n",
    "    for model in response['modelSummaries']:\n",
    "        provider = model['providerName']\n",
    "        if provider not in providers:\n",
    "            providers[provider] = []\n",
    "        providers[provider].append(model['modelId'])\n",
    "    \n",
    "    print(\"AVAILABLE MODELS IN YOUR REGION\")\n",
    "    print(\"=\" * 60)\n",
    "    for provider in ['Anthropic', 'Amazon', 'Meta']:\n",
    "        if provider in providers:\n",
    "            print(f\"\\n{provider}:\")\n",
    "            for model_id in providers[provider][:5]:\n",
    "                print(f\"  • {model_id}\")\n",
    "            if len(providers[provider]) > 5:\n",
    "                print(f\"  ... and {len(providers[provider])-5} more\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not list models: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Your First Inference Call\n",
    "\n",
    "Let's call the model using the **Converse API** (recommended for all new projects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is the capital of France? Answer in one word.\"\n",
    "\n",
    "response = bedrock_runtime.converse(\n",
    "    modelId=MODEL_ID,\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": prompt}]\n",
    "    }],\n",
    "    inferenceConfig={\"maxTokens\": 50}\n",
    ")\n",
    "\n",
    "answer = response['output']['message']['content'][0]['text']\n",
    "usage = response['usage']\n",
    "\n",
    "print(f\"Prompt: \\\"{prompt}\\\"\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Usage\n",
    "\n",
    "Every inference response includes token counts - this is how you're billed.\n",
    "\n",
    "| Token Type | Description |\n",
    "|------------|-------------|\n",
    "| **Input** | Tokens in your prompt |\n",
    "| **Output** | Tokens the model generated |\n",
    "\n",
    "**Why this matters**:\n",
    "- You pay for both input AND output tokens\n",
    "- Output tokens typically cost 3-5x more than input\n",
    "- Track usage to estimate and control costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Token Usage\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"  Input:  {usage['inputTokens']} tokens\")\n",
    "print(f\"  Output: {usage['outputTokens']} tokens\")\n",
    "print(f\"  Total:  {usage['inputTokens'] + usage['outputTokens']} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Inference Parameters\n",
    "\n",
    "In the previous call, we used `maxTokens: 50`. Let's explore what this and other parameters do.\n",
    "\n",
    "| Parameter | What it does | Range |\n",
    "|-----------|-------------|-------|\n",
    "| **max_tokens** | Limit output length | 1 - model max |\n",
    "| **temperature** | Randomness (0=deterministic, 1=creative) | 0.0 - 1.0 |\n",
    "| **top_p** | Nucleus sampling (cumulative probability cutoff) | 0.0 - 1.0 |\n",
    "| **top_k** | Limit to top K most likely tokens | 1 - 500 |\n",
    "\n",
    "> See [Inference parameters documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/inference-parameters.html) for all options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### max_tokens Demo\n",
    "\n",
    "Control output length - useful for cost control and getting concise answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Explain what machine learning is.\"\n",
    "print(f\"Prompt: \\\"{prompt}\\\"\\n\")\n",
    "\n",
    "for max_tok in [20, 50, 150]:\n",
    "    response = bedrock_runtime.converse(\n",
    "        modelId=MODEL_ID,\n",
    "        messages=[{\"role\": \"user\", \"content\": [{\"text\": prompt}]}],\n",
    "        inferenceConfig={\"maxTokens\": max_tok, \"temperature\": 0}\n",
    "    )\n",
    "    output = response['output']['message']['content'][0]['text']\n",
    "    tokens_used = response['usage']['outputTokens']\n",
    "    \n",
    "    print(f\"{'─' * 60}\")\n",
    "    print(f\"maxTokens={max_tok} → {tokens_used} tokens used\")\n",
    "    print(f\"{'─' * 60}\")\n",
    "    print(output)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature Demo\n",
    "\n",
    "See how temperature affects output consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Give me a creative name for a coffee shop. Just the name, nothing else.\"\n",
    "print(f\"Prompt: \\\"{prompt}\\\"\\n\")\n",
    "\n",
    "for temp in [0.0, 1.0]:\n",
    "    label = \"DETERMINISTIC\" if temp == 0.0 else \"CREATIVE\"\n",
    "    print(f\"{'─' * 40}\")\n",
    "    print(f\"temperature={temp} ({label})\")\n",
    "    print(f\"{'─' * 40}\")\n",
    "    for i in range(3):\n",
    "        response = bedrock_runtime.converse(\n",
    "            modelId=MODEL_ID,\n",
    "            messages=[{\"role\": \"user\", \"content\": [{\"text\": prompt}]}],\n",
    "            inferenceConfig={\"maxTokens\": 20, \"temperature\": temp}\n",
    "        )\n",
    "        answer = response['output']['message']['content'][0]['text'].strip()\n",
    "        print(f\"  Run {i+1}: {answer}\")\n",
    "    print()\n",
    "\n",
    "print(\"Note: temp=0 gives identical results, temp=1 varies each time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Pricing & Deployment\n",
    "\n",
    "### Pricing Structure\n",
    "\n",
    "You pay separately for input and output tokens:\n",
    "\n",
    "| Token Type | Typical Ratio | Example (Claude Sonnet) |\n",
    "|------------|---------------|------------------------|\n",
    "| Input | 1x | $3.00 / 1M tokens |\n",
    "| Output | 3-5x | $15.00 / 1M tokens |\n",
    "\n",
    "**Optimization focus**: Input tokens often dominate costs in production.\n",
    "\n",
    "> See [Amazon Bedrock Pricing](https://aws.amazon.com/bedrock/pricing/)\n",
    "\n",
    "### Cross-Region Inference (CRIS)\n",
    "\n",
    "Route requests across regions for better availability and cost.\n",
    "\n",
    "| Type | Model ID Prefix | Characteristics |\n",
    "|------|-----------------|----------|\n",
    "| **In-Region** | `anthropic.claude-*` | Lowest latency, single region |\n",
    "| **Geographic** | `us.`, `eu.`, `apac.` | Data stays in geographic area |\n",
    "| **Global** | `global.` | Up to ~10% savings, best throughput |\n",
    "\n",
    "> See [Cross-Region Inference documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/cross-region-inference.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MODEL ID FORMATS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Type':<12} {'Example Model ID'}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'In-Region':<12} anthropic.claude-sonnet-4-20250514-v1:0\")\n",
    "print(f\"{'Geographic':<12} us.anthropic.claude-sonnet-4-20250514-v1:0\")\n",
    "print(f\"{'Global':<12} global.anthropic.claude-sonnet-4-20250514-v1:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Concept | Key Point |\n",
    "|---------|----------|\n",
    "| **LLMs** | Prompt → Model → Response |\n",
    "| **Tokens** | Unit of processing and billing |\n",
    "| **Models** | Match complexity to cost |\n",
    "| **max_tokens** | Limit output length for cost control |\n",
    "| **temperature** | 0 = consistent, 1 = creative |\n",
    "| **Pricing** | Input + Output tokens, output costs more |\n",
    "| **CRIS** | Choose based on compliance and throughput needs |\n",
    "\n",
    "---\n",
    "\n",
    "**Next**: [02-optimization-strategy.ipynb](./02-optimization-strategy.ipynb) - Learn techniques to reduce costs and improve performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
