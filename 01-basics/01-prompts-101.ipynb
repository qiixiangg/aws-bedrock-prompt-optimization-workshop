{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "introduction",
   "metadata": {},
   "source": [
    "# Prompts 101: Understanding the Basics\n",
    "\n",
    "This notebook introduces the fundamental concepts you need to understand when working with Large Language Models (LLMs) on Amazon Bedrock. Before diving into advanced optimization techniques like prompt caching, it's essential to understand the building blocks: tokens, pricing models, API usage, and service quotas.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "- Count tokens using the Bedrock CountTokens API\n",
    "- Calculate inference costs based on token usage\n",
    "- Use the Converse API for model inference\n",
    "- Understand and interpret usage metrics from API responses\n",
    "- Navigate TPM (Tokens Per Minute) and RPM (Requests Per Minute) quotas\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "At production scale, small inefficiencies compound dramatically:\n",
    "- **1,000 extra tokens per request x 1 million requests = 1 billion unnecessary tokens processed**\n",
    "- Understanding token economics is the foundation for all cost optimization strategies\n",
    "- Proper quota management prevents throttling and ensures reliable service\n",
    "\n",
    "**Duration**: ~30 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisites",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, ensure you have:\n",
    "1. An AWS account with Amazon Bedrock access enabled\n",
    "2. AWS credentials configured (via `.env` file, AWS CLI, or IAM role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup-dependencies",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install/upgrade required packages\n",
    "from __future__ import annotations\n",
    "\n",
    "!pip3 install --upgrade boto3 python-dotenv --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "setup-imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region: us-east-1\n",
      "Model: global.anthropic.claude-sonnet-4-5-20250929-v1:0\n",
      "boto3 version: 1.42.25\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import time\n",
    "\n",
    "import boto3\n",
    "\n",
    "# Third-party imports\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Local imports - utility functions for pricing calculations\n",
    "from utils import (\n",
    "    OUTPUT_BURNDOWN_RATE,\n",
    "    calculate_actual_cost,\n",
    "    calculate_tpm_actual,\n",
    "    calculate_tpm_reservation,\n",
    "    compare_optimization,\n",
    "    print_pricing_table,\n",
    ")\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize AWS clients\n",
    "REGION = os.getenv('AWS_DEFAULT_REGION', 'us-east-1')\n",
    "bedrock_runtime = boto3.client('bedrock-runtime', region_name=REGION)\n",
    "bedrock = boto3.client('bedrock', region_name=REGION)\n",
    "service_quotas = boto3.client('service-quotas', region_name=REGION)\n",
    "\n",
    "# Model configuration - using Claude Sonnet 4.5 with global CRIS profile\n",
    "MODEL_ID = \"global.anthropic.claude-sonnet-4-5-20250929-v1:0\"\n",
    "\n",
    "print(f\"Region: {REGION}\")\n",
    "print(f\"Model: {MODEL_ID}\")\n",
    "print(f\"boto3 version: {boto3.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "info-boto3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> This notebook uses Claude Sonnet 4.5 with the <b>global</b> CRIS (Cross-Region Inference Service) profile. The global profile offers ~10% cost savings and higher availability by automatically routing requests to regions with capacity.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-tokens",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Understanding Tokens\n",
    "\n",
    "Tokens are the fundamental units that LLMs use to process text. Understanding tokenization is crucial because:\n",
    "- **Pricing** is based on token count, not character count\n",
    "- **Context limits** are measured in tokens\n",
    "- **Quota limits** (TPM) are based on tokens processed\n",
    "\n",
    "### What is a Token?\n",
    "\n",
    "A token can be:\n",
    "- A complete word (\"hello\" = 1 token)\n",
    "- Part of a word (\"tokenization\" might be 2-3 tokens)\n",
    "- Punctuation or whitespace\n",
    "- Special characters\n",
    "\n",
    "**Rule of thumb**: For English text, 1 token is approximately 4 characters or 0.75 words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "count-tokens-api",
   "metadata": {},
   "source": [
    "### CountTokens API\n",
    "\n",
    "Amazon Bedrock provides a `CountTokens` API that allows you to count tokens before making inference calls. This is useful for:\n",
    "- Estimating costs before processing\n",
    "- Ensuring prompts fit within context limits\n",
    "- Validating caching strategies (minimum token requirements)\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> The CountTokens API is free to use and does not count against your quotas.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "count-tokens-demo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TOKEN COUNT ANALYSIS\n",
      "======================================================================\n",
      "Prompt Preview                                   Chars   Tokens    Ratio\n",
      "----------------------------------------------------------------------\n",
      "Hello, world!                                       13       11     1.2\n",
      "Explain quantum computing in simple terms.          42       15     2.8\n",
      "Write a detailed analysis of the economic ...      212       41     5.2\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "def count_tokens(text, model_id=\"anthropic.claude-sonnet-4-20250514-v1:0\"):\n",
    "    \"\"\"\n",
    "    Count tokens for a given text using Bedrock's CountTokens API.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to count tokens for\n",
    "        model_id: The model ID to use for tokenization\n",
    "    \n",
    "    Returns:\n",
    "        dict with token count and character count\n",
    "    \"\"\"\n",
    "    response = bedrock_runtime.count_tokens(\n",
    "        modelId=model_id,\n",
    "        input={\n",
    "            \"converse\": {\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [{\"text\": text}]\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'tokens': response['inputTokens'],\n",
    "        'characters': len(text),\n",
    "        'chars_per_token': len(text) / response['inputTokens'] if response['inputTokens'] > 0 else 0\n",
    "    }\n",
    "\n",
    "# Test with different prompt lengths\n",
    "sample_prompts = [\n",
    "    \"Hello, world!\",\n",
    "    \"Explain quantum computing in simple terms.\",\n",
    "    \"Write a detailed analysis of the economic impact of artificial intelligence on the global workforce over the next decade, including specific sectors that will be most affected and potential mitigation strategies.\"\n",
    "]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TOKEN COUNT ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Prompt Preview':<45} {'Chars':>8} {'Tokens':>8} {'Ratio':>8}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for prompt in sample_prompts:\n",
    "    result = count_tokens(prompt)\n",
    "    preview = prompt[:42] + \"...\" if len(prompt) > 45 else prompt\n",
    "    print(f\"{preview:<45} {result['characters']:>8} {result['tokens']:>8} {result['chars_per_token']:>7.1f}\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "token-insight",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Key Insight:</b> English text averages approximately 4 characters per token. However, this ratio varies based on vocabulary complexity, special characters, and formatting. Always use the CountTokens API for accurate counts when estimating costs for production workloads.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Token Overhead:</b> The actual token count billed may be slightly higher than just your text content. The underlying API adds a small overhead for:\n",
    "<ul>\n",
    "<li>Message formatting (role markers, turn separators)</li>\n",
    "<li>System prompt wrapper tokens</li>\n",
    "<li>Tool definitions (if using tools)</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-pricing",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Understanding Pricing\n",
    "\n",
    "Amazon Bedrock uses a pay-per-token pricing model for on-demand mode. Understanding this model is essential for cost optimization.\n",
    "\n",
    "### Token Pricing Components\n",
    "\n",
    "| Token Type | Description | Typical Cost Ratio |\n",
    "|------------|-------------|--------------------|\n",
    "| Input Tokens | Tokens in your prompt | 1x (base rate) |\n",
    "| Output Tokens | Tokens generated by model | 3-5x input rate |\n",
    "| Cache Write | Tokens written to cache | 1.25x input rate |\n",
    "| Cache Read | Tokens read from cache | 0.1x input rate |\n",
    "\n",
    "**Key observation**: Output tokens cost significantly more than input tokens. This means:\n",
    "1. Optimizing prompt length saves money\n",
    "2. Controlling output length (via `max_tokens`) manages costs\n",
    "3. Caching can provide substantial savings (up to 90% on cached reads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "pricing-config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Pricing (per 1M tokens) - as of January 2026:\n",
      "==========================================================================================\n",
      "Model                               Input     Output    Cache Write     Cache Read\n",
      "                                                        (5m, 1.25x)     (5m, 0.1x)\n",
      "------------------------------------------------------------------------------------------\n",
      "Claude Sonnet 4.5 (Global)     $    3.00 $   15.00 $        3.75 $        0.30\n",
      "Claude Haiku 4.5 (Global)      $    1.00 $    5.00 $        1.25 $        0.10\n",
      "==========================================================================================\n",
      "\n",
      "Note: Cache pricing shown is for 5-minute TTL cache.\n"
     ]
    }
   ],
   "source": [
    "# Display pricing table from utils.py\n",
    "# Pricing is defined in utils.py for reuse across notebooks\n",
    "# Always verify current pricing at: https://aws.amazon.com/bedrock/pricing/\n",
    "\n",
    "print_pricing_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cost-calculator-header",
   "metadata": {},
   "source": [
    "### Interactive Cost Calculator\n",
    "\n",
    "Let's build a cost calculator to understand the economics of token optimization at scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cost-comparison-demo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COST COMPARISON: Token Optimization at Scale\n",
      "======================================================================\n",
      "Scenario: 1 million customer support requests\n",
      "Model: Claude Sonnet 4.5 (Global)\n",
      "\n",
      "Unoptimized (5,000 input tokens per request):\n",
      "  Input cost:  $   15,000.00\n",
      "  Output cost: $    2,250.00\n",
      "  Total cost:  $   17,250.00\n",
      "\n",
      "Optimized (3,000 input tokens per request):\n",
      "  Input cost:  $    9,000.00\n",
      "  Output cost: $    2,250.00\n",
      "  Total cost:  $   11,250.00\n",
      "\n",
      "SAVINGS: $6,000.00 (34.8% reduction)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Scenario: Customer support chatbot\n",
    "# - Unoptimized: 5,000 token prompt (verbose system instructions + policies)\n",
    "# - Optimized: 3,000 token prompt (streamlined instructions)\n",
    "# - Average output: 150 tokens per response\n",
    "\n",
    "comparison = compare_optimization(\n",
    "    original_tokens=5000,\n",
    "    optimized_tokens=3000,\n",
    "    output_tokens=150,\n",
    "    num_requests=1_000_000,  # 1 million requests\n",
    "    model_id=MODEL_ID\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"COST COMPARISON: Token Optimization at Scale\")\n",
    "print(\"=\"*70)\n",
    "print(\"Scenario: 1 million customer support requests\")\n",
    "print(f\"Model: {comparison['original']['model']}\")\n",
    "print()\n",
    "\n",
    "print(\"Unoptimized (5,000 input tokens per request):\")\n",
    "print(f\"  Input cost:  ${comparison['original']['input_cost']:>12,.2f}\")\n",
    "print(f\"  Output cost: ${comparison['original']['output_cost']:>12,.2f}\")\n",
    "print(f\"  Total cost:  ${comparison['original']['total_cost']:>12,.2f}\")\n",
    "\n",
    "print()\n",
    "print(\"Optimized (3,000 input tokens per request):\")\n",
    "print(f\"  Input cost:  ${comparison['optimized']['input_cost']:>12,.2f}\")\n",
    "print(f\"  Output cost: ${comparison['optimized']['output_cost']:>12,.2f}\")\n",
    "print(f\"  Total cost:  ${comparison['optimized']['total_cost']:>12,.2f}\")\n",
    "\n",
    "print()\n",
    "print(f\"SAVINGS: ${comparison['savings']:,.2f} ({comparison['savings_pct']:.1f}% reduction)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roi-at-scale",
   "metadata": {},
   "source": [
    "### ROI at Different Scales\n",
    "\n",
    "Let's see how optimization savings compound at different request volumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "roi-calculator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ROI ANALYSIS: 5000 -> 3000 tokens\n",
      "================================================================================\n",
      "Scale                       Original       Optimized         Savings          %\n",
      "--------------------------------------------------------------------------------\n",
      "1K (pilot)           $        17.25 $        11.25 $         6.00      34.8%\n",
      "10K (small)          $       172.50 $       112.50 $        60.00      34.8%\n",
      "100K (medium)        $     1,725.00 $     1,125.00 $       600.00      34.8%\n",
      "1M (large)           $    17,250.00 $    11,250.00 $     6,000.00      34.8%\n",
      "10M (enterprise)     $   172,500.00 $   112,500.00 $    60,000.00      34.8%\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def roi_at_scale(original_tokens, optimized_tokens, output_tokens=150, model_id=MODEL_ID):\n",
    "    \"\"\"\n",
    "    Calculate ROI of prompt optimization at different scales.\n",
    "    \"\"\"\n",
    "    scales = [\n",
    "        (1_000, \"1K (pilot)\"),\n",
    "        (10_000, \"10K (small)\"),\n",
    "        (100_000, \"100K (medium)\"),\n",
    "        (1_000_000, \"1M (large)\"),\n",
    "        (10_000_000, \"10M (enterprise)\"),\n",
    "    ]\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ROI ANALYSIS: {original_tokens} -> {optimized_tokens} tokens\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Scale':<20} {'Original':>15} {'Optimized':>15} {'Savings':>15} {'%':>10}\")\n",
    "    print(\"-\"*80)\n",
    "\n",
    "    for num_requests, label in scales:\n",
    "        result = compare_optimization(original_tokens, optimized_tokens, output_tokens, num_requests, model_id)\n",
    "        print(f\"{label:<20} ${result['original']['total_cost']:>13,.2f} ${result['optimized']['total_cost']:>13,.2f} ${result['savings']:>13,.2f} {result['savings_pct']:>9.1f}%\")\n",
    "\n",
    "    print(\"=\"*80)\n",
    "\n",
    "# Run ROI analysis\n",
    "roi_at_scale(original_tokens=5000, optimized_tokens=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cost-insight",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Key Insight:</b> A 40% reduction in input tokens (5,000 to 3,000) yields approximately 34% total cost savings. At enterprise scale (10M requests), this translates to $60,000+ in savings - and this is before applying prompt caching, which can save up to 90% on cached read tokens!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-latency",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Latency Impact\n",
    "\n",
    "Token count affects not just cost but also latency. Understanding this relationship helps you build responsive applications.\n",
    "\n",
    "### Key Latency Metrics\n",
    "\n",
    "| Metric | What It Measures | Why It Matters |\n",
    "|--------|------------------|----------------|\n",
    "| **TTFT** (Time to First Token) | How quickly the model starts responding | Critical for perceived speed - users see something happening |\n",
    "| **TPS** (Tokens per Second) | How fast text appears after it starts | Affects how quickly the full response is generated |\n",
    "| **TTLT** (Time to Last Token) | Total time until the last token is generated | The actual generation time (excludes app overhead) |\n",
    "| **E2E Latency** | Total time from request to complete response | The full picture including your application overhead |\n",
    "\n",
    "### Measuring Real API Latency\n",
    "\n",
    "Let's measure actual latency from the Bedrock API using the streaming response to capture TTFT and TPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "latency-calculator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "REAL API LATENCY MEASUREMENT\n",
      "==========================================================================================\n",
      "\n",
      "Prompt             TTFT         TTLT          E2E        TPS       In      Out\n",
      "------------------------------------------------------------------------------------------\n",
      "Short             208ms       2029ms       2296ms      6.0/s       17       11\n",
      "Medium            210ms       2491ms       2759ms     39.0/s       21       89\n",
      "Long              208ms       3882ms       4210ms     38.1/s       16      140\n",
      "==========================================================================================\n",
      "\n",
      "Key metrics:\n",
      "- TTFT (Time to First Token): Estimated time until first token starts generating\n",
      "- TTLT (Time to Last Token): Total model processing time (from API metrics)\n",
      "- E2E (End-to-End): Total round-trip time including network overhead\n",
      "- TPS: Output tokens generated per second\n"
     ]
    }
   ],
   "source": [
    "def measure_latency(prompt, model_id=MODEL_ID, max_tokens=200):\n",
    "    \"\"\"\n",
    "    Measure latency metrics using the Converse API.\n",
    "    \n",
    "    Returns E2E latency, TTFT (estimated), TTLT, and TPS.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    response = bedrock_runtime.converse(\n",
    "        modelId=model_id,\n",
    "        messages=[{\"role\": \"user\", \"content\": [{\"text\": prompt}]}],\n",
    "        inferenceConfig={\"maxTokens\": max_tokens, \"temperature\": 0}\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Extract metrics\n",
    "    usage = response.get('usage', {})\n",
    "    metrics = response.get('metrics', {})\n",
    "    \n",
    "    e2e_ms = (end_time - start_time) * 1000\n",
    "    output_tokens = usage.get('outputTokens', 0)\n",
    "    input_tokens = usage.get('inputTokens', 0)\n",
    "    \n",
    "    # API-reported latency (TTLT - time to last token)\n",
    "    ttlt_ms = metrics.get('latencyMs', e2e_ms)\n",
    "    \n",
    "    # Estimate TTFT: input processing time (typically ~0.5ms per token + overhead)\n",
    "    # This is approximate - for precise TTFT, use streaming API\n",
    "    estimated_ttft_ms = (input_tokens * 0.5) + 200  # ~200ms network/startup overhead\n",
    "    \n",
    "    # TPS: tokens per second for output generation\n",
    "    generation_time_ms = ttlt_ms - estimated_ttft_ms\n",
    "    tps = (output_tokens / (generation_time_ms / 1000)) if generation_time_ms > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'ttft_ms': estimated_ttft_ms,\n",
    "        'ttlt_ms': ttlt_ms,\n",
    "        'e2e_ms': e2e_ms,\n",
    "        'tps': tps,\n",
    "        'input_tokens': input_tokens,\n",
    "        'output_tokens': output_tokens,\n",
    "    }\n",
    "\n",
    "\n",
    "# Measure real latency with different prompt sizes\n",
    "print(\"=\"*90)\n",
    "print(\"REAL API LATENCY MEASUREMENT\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "test_prompts = [\n",
    "    (\"Short\", \"What is 2+2? Answer briefly.\"),\n",
    "    (\"Medium\", \"Explain photosynthesis in 2-3 sentences.\"),\n",
    "    (\"Long\", \"Explain machine learning training in one paragraph.\")\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Prompt':<10} {'TTFT':>12} {'TTLT':>12} {'E2E':>12} {'TPS':>10} {'In':>8} {'Out':>8}\")\n",
    "print(\"-\"*90)\n",
    "\n",
    "for label, prompt in test_prompts:\n",
    "    metrics = measure_latency(prompt, max_tokens=150)\n",
    "    print(f\"{label:<10} {metrics['ttft_ms']:>10.0f}ms {metrics['ttlt_ms']:>10.0f}ms {metrics['e2e_ms']:>10.0f}ms {metrics['tps']:>8.1f}/s {metrics['input_tokens']:>8} {metrics['output_tokens']:>8}\")\n",
    "\n",
    "print(\"=\"*90)\n",
    "print(\"\\nKey metrics:\")\n",
    "print(\"- TTFT (Time to First Token): Estimated time until first token starts generating\")\n",
    "print(\"- TTLT (Time to Last Token): Total model processing time (from API metrics)\")\n",
    "print(\"- E2E (End-to-End): Total round-trip time including network overhead\")\n",
    "print(\"- TPS: Output tokens generated per second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latency-insight",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> TTFT (Time to First Token) is critical for user-perceived responsiveness. In interactive applications, use <b>streaming</b> to show tokens as they're generated - users perceive faster responses even if total generation time (TTLT) is the same. TPS (Tokens per Second) indicates how fast the model generates output after starting.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-converse",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Converse API\n",
    "\n",
    "Amazon Bedrock provides two main APIs for model inference:\n",
    "\n",
    "### Converse API vs InvokeModel API\n",
    "\n",
    "| Aspect | Converse API | InvokeModel API |\n",
    "|--------|--------------|-----------------|\n",
    "| **Interface** | Unified across all models | Model-specific request format |\n",
    "| **Code portability** | Same code works for Claude, Nova, Mistral, etc. | Different code per model provider |\n",
    "| **Multi-turn support** | Built-in conversation history | Manual history management |\n",
    "| **Tool use** | Native support | Provider-specific implementation |\n",
    "| **Prompt caching** | Supported with cache checkpoints | Provider-specific |\n",
    "| **Best for** | Most applications | Model-specific features |\n",
    "\n",
    "### Which API Should You Use?\n",
    "\n",
    "| Scenario | Recommended |\n",
    "|----------|-------------|\n",
    "| Building a new application | **Converse API** - easier to switch models |\n",
    "| Multi-turn conversations | **Converse API** - built-in support |\n",
    "| Tool use / function calling | **Converse API** - unified interface |\n",
    "| Need Anthropic-specific features | InvokeModel API |\n",
    "| Existing InvokeModel code | Continue using it (both work fine) |\n",
    "\n",
    "> **For this workshop**: We use the **Converse API** as it's the recommended approach for most applications. All subsequent notebooks will use the Converse API.\n",
    "\n",
    "### Key Benefits of Converse API\n",
    "\n",
    "- **Unified interface**: Same API structure for Claude, Nova, Mistral, and other models\n",
    "- **Multi-turn support**: Built-in conversation history management\n",
    "- **Tool use**: Native support for function calling\n",
    "- **Prompt caching**: Cache checkpoints for cost optimization (covered in next notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "converse-demo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CONVERSE API DEMO\n",
      "======================================================================\n",
      "Response: The capital of France is Paris.\n",
      "Stop Reason: end_turn\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "def converse(prompt, model_id=MODEL_ID, max_tokens=100, temperature=0):\n",
    "    \"\"\"\n",
    "    Make a simple inference call using the Converse API.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The user message\n",
    "        model_id: Model to use for inference\n",
    "        max_tokens: Maximum tokens in the response\n",
    "        temperature: Sampling temperature (0 = deterministic)\n",
    "    \n",
    "    Returns:\n",
    "        dict with response text and usage metrics\n",
    "    \"\"\"\n",
    "    response = bedrock_runtime.converse(\n",
    "        modelId=model_id,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"text\": prompt}]\n",
    "            }\n",
    "        ],\n",
    "        inferenceConfig={\n",
    "            \"maxTokens\": max_tokens,\n",
    "            \"temperature\": temperature\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"text\": response['output']['message']['content'][0]['text'],\n",
    "        \"stop_reason\": response['stopReason'],\n",
    "        \"usage\": response['usage'],\n",
    "        \"latency_ms\": response.get('metrics', {}).get('latencyMs', None)\n",
    "    }\n",
    "\n",
    "# Make a simple inference call\n",
    "result = converse(\"What is the capital of France? Answer in one sentence.\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CONVERSE API DEMO\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Response: {result['text']}\")\n",
    "print(f\"Stop Reason: {result['stop_reason']}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-usage",
   "metadata": {},
   "source": [
    "### Understanding Usage Metrics\n",
    "\n",
    "Every Converse API response includes usage metrics that are essential for cost tracking and optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "usage-metrics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "USAGE METRICS BREAKDOWN\n",
      "======================================================================\n",
      "\n",
      "Token Counts:\n",
      "  inputTokens:                 19  (uncached input)\n",
      "  outputTokens:                10  (generated response)\n",
      "  cacheReadInputTokens:         0  (read from 5m cache - 0.1x cost)\n",
      "  cacheWriteInputTokens:        0  (written to 5m cache - 1.25x cost)\n",
      "  totalTokens:                 29  (total processed)\n",
      "\n",
      "Estimated Cost: $0.00020700\n",
      "Latency: 1552 ms\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Display detailed usage metrics from the previous call\n",
    "usage = result['usage']\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"USAGE METRICS BREAKDOWN\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(\"Token Counts:\")\n",
    "print(f\"  inputTokens:           {usage.get('inputTokens', 0):>8}  (uncached input)\")\n",
    "print(f\"  outputTokens:          {usage.get('outputTokens', 0):>8}  (generated response)\")\n",
    "print(f\"  cacheReadInputTokens:  {usage.get('cacheReadInputTokens', 0):>8}  (read from 5m cache - 0.1x cost)\")\n",
    "print(f\"  cacheWriteInputTokens: {usage.get('cacheWriteInputTokens', 0):>8}  (written to 5m cache - 1.25x cost)\")\n",
    "print(f\"  totalTokens:           {usage.get('totalTokens', usage.get('inputTokens', 0) + usage.get('outputTokens', 0)):>8}  (total processed)\")\n",
    "print()\n",
    "\n",
    "# Calculate actual cost using the utility function\n",
    "actual_cost = calculate_actual_cost(\n",
    "    input_tokens=usage.get('inputTokens', 0),\n",
    "    output_tokens=usage.get('outputTokens', 0),\n",
    "    model_id=MODEL_ID\n",
    ")\n",
    "\n",
    "print(f\"Estimated Cost: ${actual_cost:.8f}\")\n",
    "if result.get('latency_ms'):\n",
    "    print(f\"Latency: {result['latency_ms']} ms\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usage-table",
   "metadata": {},
   "source": [
    "### Usage Metrics Reference\n",
    "\n",
    "| Metric | Description | Cost Impact |\n",
    "|--------|-------------|-------------|\n",
    "| `inputTokens` | Tokens in prompt that were NOT cached | 1x base rate |\n",
    "| `outputTokens` | Tokens generated by model | 3-5x base rate |\n",
    "| `cacheWriteInputTokens` | Tokens written to 5m cache (first request) | 1.25x base rate |\n",
    "| `cacheReadInputTokens` | Tokens read from 5m cache (subsequent requests) | 0.1x base rate (90% savings!) |\n",
    "| `totalTokens` | Sum of all tokens processed | Varies by type |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-quotas",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. TPM and RPM Quotas\n",
    "\n",
    "Amazon Bedrock enforces throughput limits to ensure fair resource allocation. Understanding these quotas is essential for building reliable production systems.\n",
    "\n",
    "### Quota Types\n",
    "\n",
    "| Quota | Description |\n",
    "|-------|-------------|\n",
    "| **TPM** (Tokens Per Minute) | Maximum tokens processed per minute |\n",
    "| **RPM** (Requests Per Minute) | Maximum API calls per minute |\n",
    "\n",
    "Both limits apply simultaneously - you must stay within both to avoid throttling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tpm-formula-info",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Important:</b> TPM quota consumption is calculated as: <code>InputTokenCount + CacheWriteInputTokens + (OutputTokenCount x output_burndown_rate)</code>. For newer Claude models, the output burndown rate is typically 5x, meaning each output token consumes 5 tokens of your TPM quota. This is because output generation is more compute-intensive than input processing.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tpm-calculation",
   "metadata": {},
   "source": [
    "### TPM Quota: Before vs After Request\n",
    "\n",
    "Let's see how TPM quota consumption works in practice:\n",
    "1. **Before request**: Bedrock reserves quota based on `max_tokens` setting\n",
    "2. **After request**: Actual consumption is calculated from real token usage\n",
    "\n",
    "This is important because over-setting `max_tokens` reserves more quota than needed, reducing concurrency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "tpm-calculator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TPM QUOTA: BEFORE vs AFTER REQUEST (Claude Sonnet 4.5, 5x burndown)\n",
      "================================================================================\n",
      "\n",
      ">>> BEFORE REQUEST (Quota Reservation)\n",
      "   Input tokens:                 18\n",
      "   max_tokens setting:          500\n",
      "   Output quota reserved:      2500  (500 x 5)\n",
      "   ------------------------------------\n",
      "   TOTAL TPM RESERVED:         2518\n",
      "\n",
      "<<< AFTER REQUEST (Actual Usage)\n",
      "   Input tokens:                 18\n",
      "   Output tokens:                80\n",
      "   Output TPM consumed:         400  (80 x 5)\n",
      "   ------------------------------------\n",
      "   ACTUAL TPM CONSUMED:         418\n",
      "\n",
      "=== ANALYSIS\n",
      "   TPM reserved:               2518\n",
      "   TPM actually used:           418\n",
      "   TPM over-reserved:          2100  (wasted quota capacity)\n",
      "   Quota efficiency:          16.6%\n",
      "\n",
      "$$$ ACTUAL COST: $0.001254\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# TPM Calculation: Before vs After Request\n",
    "# Using Claude Sonnet 4.5 with 5x output burndown rate\n",
    "# Functions imported from utils.py: calculate_tpm_reservation, calculate_tpm_actual, calculate_actual_cost\n",
    "\n",
    "# Make a real API call to demonstrate\n",
    "print(\"=\"*80)\n",
    "print(f\"TPM QUOTA: BEFORE vs AFTER REQUEST (Claude Sonnet 4.5, {OUTPUT_BURNDOWN_RATE}x burndown)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Set up request parameters\n",
    "test_prompt = \"What are three benefits of cloud computing? Be brief.\"\n",
    "max_tokens_setting = 500  # What we set in the API call\n",
    "\n",
    "# Count input tokens first\n",
    "token_count = count_tokens(test_prompt)\n",
    "input_tokens = token_count['tokens']\n",
    "\n",
    "# BEFORE: Calculate reservation\n",
    "tpm_reserved = calculate_tpm_reservation(input_tokens, max_tokens_setting)\n",
    "\n",
    "print(\"\\n>>> BEFORE REQUEST (Quota Reservation)\")\n",
    "print(f\"   Input tokens:           {input_tokens:>8}\")\n",
    "print(f\"   max_tokens setting:     {max_tokens_setting:>8}\")\n",
    "print(f\"   Output quota reserved:  {max_tokens_setting * OUTPUT_BURNDOWN_RATE:>8}  ({max_tokens_setting} x {OUTPUT_BURNDOWN_RATE})\")\n",
    "print(\"   ------------------------------------\")\n",
    "print(f\"   TOTAL TPM RESERVED:     {tpm_reserved:>8}\")\n",
    "\n",
    "# Make the actual API call\n",
    "result = converse(test_prompt, max_tokens=max_tokens_setting)\n",
    "usage = result['usage']\n",
    "actual_output = usage['outputTokens']\n",
    "actual_input = usage['inputTokens']\n",
    "\n",
    "# AFTER: Calculate actual consumption\n",
    "tpm_actual = calculate_tpm_actual(actual_input, actual_output)\n",
    "actual_cost = calculate_actual_cost(actual_input, actual_output, MODEL_ID)\n",
    "\n",
    "print(\"\\n<<< AFTER REQUEST (Actual Usage)\")\n",
    "print(f\"   Input tokens:           {actual_input:>8}\")\n",
    "print(f\"   Output tokens:          {actual_output:>8}\")\n",
    "print(f\"   Output TPM consumed:    {actual_output * OUTPUT_BURNDOWN_RATE:>8}  ({actual_output} x {OUTPUT_BURNDOWN_RATE})\")\n",
    "print(\"   ------------------------------------\")\n",
    "print(f\"   ACTUAL TPM CONSUMED:    {tpm_actual:>8}\")\n",
    "\n",
    "# Show the difference\n",
    "tpm_wasted = tpm_reserved - tpm_actual\n",
    "efficiency = (tpm_actual / tpm_reserved) * 100 if tpm_reserved > 0 else 100\n",
    "\n",
    "print(\"\\n=== ANALYSIS\")\n",
    "print(f\"   TPM reserved:           {tpm_reserved:>8}\")\n",
    "print(f\"   TPM actually used:      {tpm_actual:>8}\")\n",
    "print(f\"   TPM over-reserved:      {tpm_wasted:>8}  (wasted quota capacity)\")\n",
    "print(f\"   Quota efficiency:       {efficiency:>7.1f}%\")\n",
    "print(f\"\\n$$$ ACTUAL COST: ${actual_cost:.6f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quota-insight",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Key Insight:</b> The output burndown rate (5x for Claude Sonnet 4.5) means each output token consumes 5 tokens of your TPM quota. <b>But remember:</b>\n",
    "<ul>\n",
    "<li><b>TPM affects concurrency, not cost</b> - you're billed for actual tokens, not reserved quota</li>\n",
    "<li><b>Over-setting max_tokens wastes quota capacity</b> - the example above shows how setting max_tokens=500 but only using ~100 tokens reserves 5x more quota than needed</li>\n",
    "<li><b>Right-size max_tokens</b> - set it to expected output + 10-15% buffer, not the model maximum</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quota-considerations",
   "metadata": {},
   "source": [
    "### Additional Quota Considerations\n",
    "\n",
    "- **In-region vs CRIS**: Cross-Region Inference Service (CRIS) has separate quotas from in-region calls\n",
    "- **max_tokens reservation**: Setting `max_tokens` reserves that capacity during request processing\n",
    "- **Quota increases**: Request increases via the AWS Service Quotas console for production workloads\n",
    "- **Concurrent requests**: Both TPM and RPM limits apply simultaneously"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": "---\n\n## Summary\n\nIn this notebook, you learned the fundamental concepts for working with LLMs on Amazon Bedrock:\n\n| Concept | Key Takeaway |\n|---------|-------------|\n| **Tokens** | ~4 characters per token for English; use `CountTokens` API for accurate counts; watch for API overhead |\n| **Pricing** | Output tokens cost 3-5x more than input; 5m cache reads save 90%; global CRIS saves ~10% |\n| **Cost at Scale** | 40% token reduction = 34% cost savings; compounds to $60K+ at 10M requests |\n| **Latency** | TTFT critical for UX; TTLT = total generation time; use streaming for responsiveness |\n| **APIs** | Use **Converse API** for most applications; unified interface across all models |\n| **Usage Metrics** | Track `inputTokens`, `outputTokens`, and cache metrics for cost visibility |\n| **Quotas** | TPM formula includes output burndown (5x); TPM affects concurrency, not cost |\n\n### What's Next\n\nIn the next notebook, **02-optimization-strategy.ipynb**, you will learn:\n- Model selection strategies (right-sizing for your use case)\n- Prompt design best practices (clear instructions, few-shot examples)\n- Parameter tuning (temperature, max_tokens)\n- Structured output with tool use\n- Prompt caching fundamentals\n- CloudWatch monitoring for Bedrock"
  },
  {
   "cell_type": "markdown",
   "id": "resources",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [Amazon Bedrock Pricing](https://aws.amazon.com/bedrock/pricing/)\n",
    "- [Amazon Bedrock Quotas](https://docs.aws.amazon.com/bedrock/latest/userguide/quotas.html)\n",
    "- [Converse API Documentation](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html)\n",
    "- [Prompt Caching Documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-caching.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws-bedrock-prompt-optimization-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}