{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Strategy\n",
    "\n",
    "This notebook covers practical techniques to reduce costs and improve quality when working with Amazon Bedrock foundation models. We cover model selection, prompt design best practices, parameter tuning, and introduce prompt caching fundamentals.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "- Choose the right model for your task (don't always go for the largest)\n",
    "- Write effective prompts with clear instructions and few-shot examples\n",
    "- Use tool use for reliable structured output\n",
    "- Optimize `max_tokens` and temperature parameters\n",
    "- Implement basic prompt caching for cost savings\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "At production scale, optimization choices compound significantly:\n",
    "- **Wrong model selection** can cost 3-10x more than necessary\n",
    "- **Verbose prompts** waste input tokens on every request\n",
    "- **High max_tokens** reserves unnecessary quota, reducing concurrency\n",
    "- **Missing cache opportunities** means paying full price for repeated content\n",
    "\n",
    "**Duration**: ~60 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, ensure you have:\n",
    "1. An AWS account with Amazon Bedrock access enabled\n",
    "2. AWS credentials configured (via `.env` file, AWS CLI, or IAM role)\n",
    "3. Completed the previous notebook (01-prompts-101.ipynb) or have equivalent knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region: us-east-1\n",
      "Small/Fast model:     global.anthropic.claude-haiku-4-5-20251001-v1:0\n",
      "Medium/Balanced model: global.anthropic.claude-sonnet-4-5-20250929-v1:0\n",
      "\n",
      "Setup complete!\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "import boto3\n",
    "\n",
    "# Initialize Bedrock client\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name=\"bedrock-runtime\", region_name=os.getenv(\"AWS_DEFAULT_REGION\", \"us-east-1\")\n",
    ")\n",
    "\n",
    "# Model IDs - using model categories rather than specific variants\n",
    "# Small/Fast model: Good for simple extraction, classification\n",
    "# Medium/Balanced model: Good for most production workloads\n",
    "SMALL_MODEL = \"global.anthropic.claude-haiku-4-5-20251001-v1:0\"  # Small/Fast\n",
    "MEDIUM_MODEL = \"global.anthropic.claude-sonnet-4-5-20250929-v1:0\"  # Medium/Balanced\n",
    "\n",
    "print(f\"Region: {os.getenv('AWS_DEFAULT_REGION', 'us-east-1')}\")\n",
    "print(f\"Small/Fast model:     {SMALL_MODEL}\")\n",
    "print(f\"Medium/Balanced model: {MEDIUM_MODEL}\")\n",
    "print(\"\\nSetup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> This notebook uses Claude models with the <b>global</b> CRIS (Cross-Region Inference Service) profile for higher availability.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Model Selection\n",
    "\n",
    "One of the most impactful optimization decisions is choosing the right model for your task. A common mistake is always using the largest, most capable model - this wastes money and increases latency.\n",
    "\n",
    "### Key Principle: Start Small, Scale Up\n",
    "\n",
    "| Category | Typical Models | Best For | Cost |\n",
    "|----------|----------------|----------|------|\n",
    "| **Small/Fast** | Claude Haiku 4.5, Nova Lite, Nova Micro | Extraction, classification, simple Q&A | $ (lowest) |\n",
    "| **Medium/Balanced** | Claude Sonnet 4.5, Nova Pro | General reasoning, summarization, code | $$ |\n",
    "| **Large/Flagship** | Claude Opus 4.5, Nova Premier | Complex reasoning, research, analysis | $$$ (highest) |\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<strong>Key Insight:</strong> The best model is the <strong>smallest model that meets your quality requirements</strong> - not the most capable one available.\n",
    "</div>\n",
    "\n",
    "Let's compare a smaller and larger model on a simple extraction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Extract the person's name and email from this text: 'Contact John Smith at john.smith@example.com for details.'\n",
      "\n",
      "Model              Latency    In Tok     Out Tok    Cost        \n",
      "-----------------------------------------------------------------\n",
      "Small (Haiku)      1.52s     34         24         $0.000154\n",
      "Medium (Sonnet)    1.85s     34         19         $0.000387\n",
      "\n",
      "Cost ratio: Medium is 2.5x more expensive\n",
      "\n",
      "--- Small Model Output ---\n",
      "# Extracted Information\n",
      "\n",
      "**Name:** John Smith\n",
      "\n",
      "**Email:** john.smith@example.com\n",
      "\n",
      "--- Medium Model Output ---\n",
      "**Name:** John Smith\n",
      "\n",
      "**Email:** john.smith@example.com\n"
     ]
    }
   ],
   "source": [
    "# Compare cost: Small vs Medium model on extraction task\n",
    "\n",
    "task = (\n",
    "    \"Extract the person's name and email from this text: 'Contact John Smith at john.smith@example.com for details.'\"\n",
    ")\n",
    "\n",
    "# Current pricing per 1,000 tokens (check https://aws.amazon.com/bedrock/pricing/ for updates)\n",
    "# Using Claude model pricing as of Jan 2026\n",
    "PRICING = {\n",
    "    \"small\": {\"input\": 0.001, \"output\": 0.005},  # Haiku 4.5\n",
    "    \"medium\": {\"input\": 0.003, \"output\": 0.015},  # Sonnet 4.5\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_id, label, pricing_key in [\n",
    "    (SMALL_MODEL, \"Small (Haiku)\", \"small\"),\n",
    "    (MEDIUM_MODEL, \"Medium (Sonnet)\", \"medium\"),\n",
    "]:\n",
    "    start = time.time()\n",
    "    response = bedrock_runtime.converse(\n",
    "        modelId=model_id,\n",
    "        messages=[{\"role\": \"user\", \"content\": [{\"text\": task}]}],\n",
    "        inferenceConfig={\"maxTokens\": 100, \"temperature\": 0},\n",
    "    )\n",
    "    latency = time.time() - start\n",
    "\n",
    "    usage = response[\"usage\"]\n",
    "    output = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "\n",
    "    # Calculate cost per 1,000 tokens\n",
    "    cost = (usage[\"inputTokens\"] / 1_000) * PRICING[pricing_key][\"input\"] + (usage[\"outputTokens\"] / 1_000) * PRICING[\n",
    "        pricing_key\n",
    "    ][\"output\"]\n",
    "\n",
    "    results[label] = {\n",
    "        \"output\": output,\n",
    "        \"latency\": latency,\n",
    "        \"input_tokens\": usage[\"inputTokens\"],\n",
    "        \"output_tokens\": usage[\"outputTokens\"],\n",
    "        \"cost\": cost,\n",
    "    }\n",
    "\n",
    "# Display comparison\n",
    "print(f\"Task: {task}\\n\")\n",
    "print(f\"{'Model':<18} {'Latency':<10} {'In Tok':<10} {'Out Tok':<10} {'Cost':<12}\")\n",
    "print(\"-\" * 65)\n",
    "for name, data in results.items():\n",
    "    print(\n",
    "        f\"{name:<18} {data['latency']:.2f}s{'':<4} {data['input_tokens']:<10} {data['output_tokens']:<10} ${data['cost']:.6f}\"\n",
    "    )\n",
    "\n",
    "print(\n",
    "    f\"\\nCost ratio: Medium is {results['Medium (Sonnet)']['cost'] / results['Small (Haiku)']['cost']:.1f}x more expensive\"\n",
    ")\n",
    "\n",
    "print(\"\\n--- Small Model Output ---\")\n",
    "print(results[\"Small (Haiku)\"][\"output\"][:200])\n",
    "print(\"\\n--- Medium Model Output ---\")\n",
    "print(results[\"Medium (Sonnet)\"][\"output\"][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insight: Model Selection\n",
    "\n",
    "For simple extraction tasks, both models produce correct results. The key difference is **cost and latency**.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<strong>Recommendation:</strong> For extraction, classification, and simple pattern matching, use <strong>smaller models</strong>. Reserve larger models for tasks requiring deeper reasoning.\n",
    "</div>\n",
    "\n",
    "| Task Type | Recommended | Rationale |\n",
    "|-----------|-------------|----------|\n",
    "| Entity extraction | Small model | Simple pattern matching |\n",
    "| Classification | Small model | Follows clear rules |\n",
    "| Summarization | Medium model | Needs context understanding |\n",
    "| Complex Q&A | Medium/Large model | Requires inference |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Prompt Design Best Practices\n",
    "\n",
    "Good prompt design reduces token usage while improving output quality. This section covers key techniques:\n",
    "\n",
    "1. **Clear, specific instructions**\n",
    "2. **Few-shot examples**\n",
    "3. **Structured output**\n",
    "\n",
    "---\n",
    "\n",
    "### 2.1 Clear, Specific Instructions\n",
    "\n",
    "Vague prompts lead to verbose, unfocused responses that waste output tokens. Specific prompts produce concise, targeted responses.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<strong>Tip:</strong> When you need a specific format, length, or focus, specify it in your prompt. For open-ended creative tasks, you may want to leave more freedom for the model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VAGUE PROMPT\n",
      "============================================================\n",
      "Prompt: Tell me about Paris.\n",
      "\n",
      "Output tokens: 200\n",
      "\n",
      "Response:\n",
      "# Paris\n",
      "\n",
      "Paris is the capital and largest city of France, located in north-central France along the Seine River. Here are some key highlights:\n",
      "\n",
      "## History & Culture\n",
      "- One of the world's most historically significant cities, with roots dating back over 2,000 years\n",
      "- Center of art, philosophy, and intellectual movements throughout European history\n",
      "- Known for its classical architecture, museums, and cultural institutions\n",
      "\n",
      "## Famous Landmarks\n",
      "- **Eiffel Tower** – iconic iron lattice monument built in 1889\n",
      "- **Notre-Dame Cathedral** – medieval Gothic masterpiece (currently under restoration)\n",
      "- **Louvre Museum** – world's largest art museum, home to the Mona Lisa\n",
      "- **Arc de Triomphe** – monumental arch honoring military victories\n",
      "- **Sacré-Cœur** – white basilica overlooking the city\n",
      "\n",
      "## Characteristics\n",
      "- Population of about 2.\n",
      "\n",
      "============================================================\n",
      "SPECIFIC PROMPT\n",
      "============================================================\n",
      "Prompt: List 3 must-see attractions in Paris for first-time visitors.\n",
      "Format: numbered list with attraction name only.\n",
      "\n",
      "Output tokens: 25\n",
      "\n",
      "Response:\n",
      "1. Eiffel Tower\n",
      "2. Louvre Museum\n",
      "3. Notre-Dame Cathedral\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare vague vs specific prompts\n",
    "\n",
    "vague_prompt = \"Tell me about Paris.\"\n",
    "\n",
    "specific_prompt = \"\"\"List 3 must-see attractions in Paris for first-time visitors.\n",
    "Format: numbered list with attraction name only.\"\"\"\n",
    "\n",
    "for label, prompt in [(\"VAGUE\", vague_prompt), (\"SPECIFIC\", specific_prompt)]:\n",
    "    response = bedrock_runtime.converse(\n",
    "        modelId=SMALL_MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": [{\"text\": prompt}]}],\n",
    "        inferenceConfig={\"maxTokens\": 200, \"temperature\": 0},\n",
    "    )\n",
    "\n",
    "    output = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    usage = response[\"usage\"]\n",
    "\n",
    "    print(f\"{'=' * 60}\")\n",
    "    print(f\"{label} PROMPT\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    print(f\"Prompt: {prompt}\\n\")\n",
    "    print(f\"Output tokens: {usage['outputTokens']}\")\n",
    "    print(f\"\\nResponse:\\n{output}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Few-Shot Examples\n",
    "\n",
    "Few-shot learning uses examples to teach the model the exact format and style you want. While this adds input tokens, it dramatically improves output consistency.\n",
    "\n",
    "**When to use few-shot:**\n",
    "- When you need consistent output format across many requests\n",
    "- For domain-specific terminology or style\n",
    "- When zero-shot produces inconsistent results\n",
    "\n",
    "**When NOT to use few-shot:**\n",
    "- Simple tasks where the model already performs well (unnecessary token cost)\n",
    "- Tasks with highly variable output (examples may constrain creativity)\n",
    "- When input context is already long (token limits)\n",
    "\n",
    "Let's compare zero-shot vs few-shot for **extracting product info into a specific format**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ZERO-SHOT vs FEW-SHOT: Product Data Extraction\n",
      "======================================================================\n",
      "\n",
      "Product: Apple MacBook Pro 16-inch with M3 chip, 32GB RAM, Space Black - $2,499\n",
      "------------------------------------------------------------\n",
      "  ZERO-SHOT: Apple MacBook Pro 16-inch with M3 chip, 32GB RAM, Space Black|Apple|$2,499|Laptops\n",
      "  FEW-SHOT: MacBook Pro 16-inch|Apple|2499|laptop\n",
      "\n",
      "Product: Sony WH-1000XM5 wireless noise cancelling headphones in silver for $348\n",
      "------------------------------------------------------------\n",
      "  ZERO-SHOT: Sony WH-1000XM5|Sony|$348|Wireless Noise Cancelling Headphones\n",
      "  FEW-SHOT: WH-1000XM5|Sony|348|headphones\n",
      "\n",
      "Product: Samsung 65 inch OLED 4K Smart TV (2024 model) priced at $1,799.99\n",
      "------------------------------------------------------------\n",
      "  ZERO-SHOT: Samsung|Samsung|$1,799.99|Television\n",
      "  FEW-SHOT: 65\" OLED 4K Smart TV|Samsung|1799.99|tv\n"
     ]
    }
   ],
   "source": [
    "# Zero-shot vs few-shot for data extraction with specific format\n",
    "\n",
    "# Test with multiple product descriptions to see consistency\n",
    "products = [\n",
    "    \"Apple MacBook Pro 16-inch with M3 chip, 32GB RAM, Space Black - $2,499\",\n",
    "    \"Sony WH-1000XM5 wireless noise cancelling headphones in silver for $348\",\n",
    "    \"Samsung 65 inch OLED 4K Smart TV (2024 model) priced at $1,799.99\",\n",
    "]\n",
    "\n",
    "zero_shot = \"\"\"Extract product info and output in this exact format:\n",
    "PRODUCT|BRAND|PRICE|CATEGORY\n",
    "\n",
    "Product: {product}\"\"\"\n",
    "\n",
    "few_shot = \"\"\"Extract product info in this exact format:\n",
    "PRODUCT|BRAND|PRICE|CATEGORY\n",
    "\n",
    "Examples:\n",
    "Product: \"Dell XPS 15 laptop with Intel i7, 16GB memory - $1,299\"\n",
    "Output: XPS 15|Dell|1299|laptop\n",
    "\n",
    "Product: \"Bose QuietComfort earbuds, noise cancelling, $279 retail\"\n",
    "Output: QuietComfort Earbuds|Bose|279|audio\n",
    "\n",
    "Product: \"LG 55\" C3 OLED TV (2023) on sale for $1,196.99\"\n",
    "Output: C3 OLED 55\"|LG|1196.99|tv\n",
    "\n",
    "Now extract:\n",
    "Product: \"{product}\"\n",
    "Output:\"\"\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ZERO-SHOT vs FEW-SHOT: Product Data Extraction\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for product in products:\n",
    "    print(f\"\\nProduct: {product}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for label, template in [(\"ZERO-SHOT\", zero_shot), (\"FEW-SHOT\", few_shot)]:\n",
    "        prompt = template.format(product=product)\n",
    "        response = bedrock_runtime.converse(\n",
    "            modelId=SMALL_MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": [{\"text\": prompt}]}],\n",
    "            inferenceConfig={\"maxTokens\": 100, \"temperature\": 0},\n",
    "        )\n",
    "\n",
    "        output = response[\"output\"][\"message\"][\"content\"][0][\"text\"].strip()\n",
    "        # Show first line only for cleaner comparison\n",
    "        first_line = output.split(\"\\n\")[0]\n",
    "        print(f\"  {label}: {first_line}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.3 Structured Output\n\nWhen you need structured output, there are three approaches: **prompt-based**, **tool use**, and **native structured output**.\n\n#### Prompt-Based Structured Output\n\nYou can ask the model to return structured data directly in the prompt. This works for JSON, XML, or any format you specify:"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PROMPT-BASED STRUCTURED OUTPUT\n",
      "============================================================\n",
      "\n",
      "JSON Format:\n",
      "----------------------------------------\n",
      "```json\n",
      "{\n",
      "  \"name\": \"John Smith\",\n",
      "  \"age\": 35,\n",
      "  \"job_title\": \"Software Engineer\",\n",
      "  \"company\": \"TechCorp\",\n",
      "  \"location\": \"San Francisco\"\n",
      "}\n",
      "```\n",
      "\n",
      "Tokens - Input: 64, Output: 59\n",
      "\n",
      "XML Format:\n",
      "----------------------------------------\n",
      "```xml\n",
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<person>\n",
      "  <name>John Smith</name>\n",
      "  <age>35</age>\n",
      "  <job_title>Software Engineer</job_title>\n",
      "  <company>TechCorp</company>\n",
      "  <location>San Francisco</location>\n",
      "</person>\n",
      "```\n",
      "\n",
      "Tokens - Input: 75, Output: 86\n"
     ]
    }
   ],
   "source": [
    "# Prompt-based structured output: JSON and XML\n",
    "\n",
    "text_to_extract = \"John Smith, 35 years old, works as a Software Engineer at TechCorp in San Francisco.\"\n",
    "\n",
    "# JSON format\n",
    "json_prompt = f\"\"\"Extract information from this text and return ONLY valid JSON (no markdown):\n",
    "\n",
    "Text: {text_to_extract}\n",
    "\n",
    "Return JSON with: name, age, job_title, company, location\"\"\"\n",
    "\n",
    "# XML format\n",
    "xml_prompt = f\"\"\"Extract information from this text and return ONLY valid XML (no markdown):\n",
    "\n",
    "Text: {text_to_extract}\n",
    "\n",
    "Return XML with tags: <person><name>, <age>, <job_title>, <company>, <location></person>\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PROMPT-BASED STRUCTURED OUTPUT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for label, prompt in [(\"JSON\", json_prompt), (\"XML\", xml_prompt)]:\n",
    "    response = bedrock_runtime.converse(\n",
    "        modelId=SMALL_MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": [{\"text\": prompt}]}],\n",
    "        inferenceConfig={\"maxTokens\": 200, \"temperature\": 0},\n",
    "    )\n",
    "\n",
    "    output = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    usage = response[\"usage\"]\n",
    "\n",
    "    print(f\"\\n{label} Format:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(output)\n",
    "    print(f\"\\nTokens - Input: {usage['inputTokens']}, Output: {usage['outputTokens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tool Use (Schema-Enforced)\n",
    "\n",
    "**Tool use** (function calling) provides native schema enforcement. The model is constrained to output valid JSON matching your defined schema.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<strong>Trade-off:</strong> Tool use adds significant input tokens (tool definitions) but guarantees valid, parseable output. Choose based on your reliability requirements vs cost sensitivity.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Tool use: Schema-enforced JSON extraction\n\n# Define tool with JSON schema\ntools = [\n    {\n        \"toolSpec\": {\n            \"name\": \"extract_person_info\",\n            \"strict\": True,  # Enforce strict schema validation\n            \"description\": \"Extract structured person information from text\",\n            \"inputSchema\": {\n                \"json\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"name\": {\"type\": \"string\", \"description\": \"Person's full name\"},\n                        \"age\": {\"type\": \"integer\", \"description\": \"Person's age\"},\n                        \"job_title\": {\"type\": \"string\", \"description\": \"Job title or occupation\"},\n                        \"company\": {\"type\": \"string\", \"description\": \"Company name\"},\n                        \"location\": {\"type\": \"string\", \"description\": \"City or location\"},\n                    },\n                    \"required\": [\"name\", \"age\", \"job_title\"],\n                }\n            },\n        }\n    }\n]\n\nresponse = bedrock_runtime.converse(\n    modelId=SMALL_MODEL,\n    messages=[{\"role\": \"user\", \"content\": [{\"text\": f\"Extract person information from: {text_to_extract}\"}]}],\n    toolConfig={\"tools\": tools},\n    inferenceConfig={\"maxTokens\": 200, \"temperature\": 0},\n)\n\n# Extract tool use result\noutput_content = response[\"output\"][\"message\"][\"content\"]\nfor block in output_content:\n    if \"toolUse\" in block:\n        extracted = block[\"toolUse\"][\"input\"]\n        print(\"=\" * 60)\n        print(\"TOOL USE (Schema-Enforced, strict)\")\n        print(\"=\" * 60)\n        print(\"\\nExtracted:\")\n        print(json.dumps(extracted, indent=2))\n        break\n\nusage = response[\"usage\"]\nprint(f\"\\nTokens - Input: {usage['inputTokens']}, Output: {usage['outputTokens']}\")\nprint(\"\\n⚠️  Notice: Tool use input tokens are significantly higher due to schema definition.\")"
  },
  {
   "cell_type": "markdown",
   "source": "#### Structured Output (Native JSON Schema)\n\nAmazon Bedrock supports **native structured output** via the `outputConfig` parameter. Instead of using tool use as a workaround for JSON extraction, you can pass a JSON schema directly and the model will return valid JSON conforming to that schema.\n\n<div class=\"alert alert-success\">\n<strong>Best of both worlds:</strong> Native structured output provides guaranteed schema compliance like tool use, but without the extra token overhead of tool definitions. This is the recommended approach for production JSON extraction.\n</div>",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Structured Output: Native JSON Schema via outputConfig\n\nresponse = bedrock_runtime.converse(\n    modelId=SMALL_MODEL,\n    messages=[{\"role\": \"user\", \"content\": [{\"text\": f\"Extract person information from: {text_to_extract}\"}]}],\n    inferenceConfig={\"maxTokens\": 200, \"temperature\": 0},\n    outputConfig={\n        \"textFormat\": {\n            \"type\": \"json_schema\",\n            \"structure\": {\n                \"jsonSchema\": {\n                    \"schema\": json.dumps({\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"name\": {\"type\": \"string\", \"description\": \"Person's full name\"},\n                            \"age\": {\"type\": \"integer\", \"description\": \"Person's age\"},\n                            \"job_title\": {\"type\": \"string\", \"description\": \"Job title or occupation\"},\n                            \"company\": {\"type\": \"string\", \"description\": \"Company name\"},\n                            \"location\": {\"type\": \"string\", \"description\": \"City or location\"},\n                        },\n                        \"required\": [\"name\", \"age\", \"job_title\", \"company\", \"location\"],\n                        \"additionalProperties\": False,\n                    }),\n                    \"name\": \"person_info_extraction\",\n                    \"description\": \"Extract structured person information from text\",\n                }\n            },\n        }\n    },\n)\n\n# The response text is valid JSON conforming to the schema\nextracted = json.loads(response[\"output\"][\"message\"][\"content\"][0][\"text\"])\n\nprint(\"=\" * 60)\nprint(\"STRUCTURED OUTPUT (Native JSON Schema)\")\nprint(\"=\" * 60)\nprint(\"\\nExtracted:\")\nprint(json.dumps(extracted, indent=2))\n\nusage = response[\"usage\"]\nprint(f\"\\nTokens - Input: {usage['inputTokens']}, Output: {usage['outputTokens']}\")\nprint(\"\\n✅ Lower input tokens than tool use, with the same schema guarantee!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Comparison: Three Approaches to Structured Output\n\n| Approach | Format | Reliability | Input Tokens | Best For |\n|----------|--------|-------------|--------------|----------|\n| **Prompt-based** | JSON, XML, custom | Good (may need cleanup) | Lower | Prototyping, cost-sensitive apps |\n| **Tool Use (strict)** | JSON only | Guaranteed valid | Higher (schema overhead) | Production APIs needing function calling |\n| **Structured Output** | JSON only | Guaranteed valid | Lower than tool use | Production APIs needing direct JSON response |\n\n<div class=\"alert alert-info\">\n<strong>When to use each:</strong>\n<ul>\n<li><strong>Prompt-based</strong>: Lower token cost, flexible formats (JSON/XML/custom), good for exploration</li>\n<li><strong>Tool use (strict)</strong>: Guaranteed schema compliance with function calling semantics, use when you need tool orchestration</li>\n<li><strong>Structured output</strong>: Guaranteed schema compliance with lowest token overhead, use when you need direct JSON responses without function calling</li>\n</ul>\n</div>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Parameter Tuning\n",
    "\n",
    "### 3.1 max_tokens\n",
    "\n",
    "The `max_tokens` parameter serves two purposes:\n",
    "\n",
    "1. **Limit output length** - Caps how many tokens the model generates\n",
    "2. **Affects concurrency** - Bedrock reserves `input + max_tokens` from your TPM quota at request start\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<strong>Key Trade-off:</strong>\n",
    "<ul>\n",
    "<li><strong>Too small</strong> → Output gets truncated (incomplete response)</li>\n",
    "<li><strong>Too large</strong> → Wastes quota reservation, reduces concurrent requests</li>\n",
    "</ul>\n",
    "Set <code>max_tokens</code> to your expected output + 10-15% buffer.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \"List 5 benefits of exercise with a brief explanation for each.\"\n",
      "\n",
      "================================================================================\n",
      "max_tokens   Output Tok   Truncated?   Quota Reserved \n",
      "-------------------------------------------------------\n",
      "50           50           ⚠️ YES       ~71            \n",
      "100          100          ⚠️ YES       ~121           \n",
      "200          190          No           ~221           \n",
      "500          190          No           ~521           \n",
      "\n",
      "================================================================================\n",
      "ACTUAL OUTPUTS (notice truncation with small max_tokens):\n",
      "================================================================================\n",
      "\n",
      "max_tokens=50 [⚠️ TRUNCATED]:\n",
      "----------------------------------------\n",
      "# 5 Benefits of Exercise\n",
      "\n",
      "1. **Improved Cardiovascular Health**\n",
      "Regular exercise strengthens your heart and improves circulation, reducing blood pressure and lowering the risk of heart disease and stroke.\n",
      "\n",
      "2. **Weight Management\n",
      "\n",
      "max_tokens=100 [⚠️ TRUNCATED]:\n",
      "----------------------------------------\n",
      "# 5 Benefits of Exercise\n",
      "\n",
      "1. **Improved Cardiovascular Health**\n",
      "Regular exercise strengthens your heart and improves circulation, reducing blood pressure and lowering the risk of heart disease and stroke.\n",
      "\n",
      "2. **Weight Management**\n",
      "Physical activity burns calories and builds muscle, helping you maint...\n",
      "\n",
      "max_tokens=200 [✓ Complete]:\n",
      "----------------------------------------\n",
      "# 5 Benefits of Exercise\n",
      "\n",
      "1. **Improved Cardiovascular Health**\n",
      "Regular exercise strengthens your heart and improves circulation, reducing blood pressure and lowering the risk of heart disease and stroke.\n",
      "\n",
      "2. **Weight Management**\n",
      "Physical activity burns calories and builds muscle, helping you maint...\n",
      "\n",
      "max_tokens=500 [✓ Complete]:\n",
      "----------------------------------------\n",
      "# 5 Benefits of Exercise\n",
      "\n",
      "1. **Improved Cardiovascular Health**\n",
      "Regular exercise strengthens your heart and improves circulation, reducing blood pressure and lowering the risk of heart disease and stroke.\n",
      "\n",
      "2. **Weight Management**\n",
      "Physical activity burns calories and builds muscle, helping you maint...\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate max_tokens impact on output quality and quota\n",
    "\n",
    "prompt = \"List 5 benefits of exercise with a brief explanation for each.\"\n",
    "\n",
    "print(f'Prompt: \"{prompt}\"\\n')\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results = []\n",
    "for max_tok in [50, 100, 200, 500]:\n",
    "    response = bedrock_runtime.converse(\n",
    "        modelId=SMALL_MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": [{\"text\": prompt}]}],\n",
    "        inferenceConfig={\"maxTokens\": max_tok, \"temperature\": 0},\n",
    "    )\n",
    "\n",
    "    output = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    usage = response[\"usage\"]\n",
    "    stop_reason = response[\"stopReason\"]\n",
    "    truncated = stop_reason == \"max_tokens\"\n",
    "\n",
    "    results.append(\n",
    "        {\n",
    "            \"max_tokens\": max_tok,\n",
    "            \"output_tokens\": usage[\"outputTokens\"],\n",
    "            \"input_tokens\": usage[\"inputTokens\"],\n",
    "            \"truncated\": truncated,\n",
    "            \"output\": output,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Show summary table\n",
    "print(f\"{'max_tokens':<12} {'Output Tok':<12} {'Truncated?':<12} {'Quota Reserved':<15}\")\n",
    "print(\"-\" * 55)\n",
    "for r in results:\n",
    "    quota = r[\"input_tokens\"] + r[\"max_tokens\"]\n",
    "    trunc = \"⚠️ YES\" if r[\"truncated\"] else \"No\"\n",
    "    print(f\"{r['max_tokens']:<12} {r['output_tokens']:<12} {trunc:<12} ~{quota:<14}\")\n",
    "\n",
    "# Show actual outputs to demonstrate truncation\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ACTUAL OUTPUTS (notice truncation with small max_tokens):\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for r in results:\n",
    "    status = \"⚠️ TRUNCATED\" if r[\"truncated\"] else \"✓ Complete\"\n",
    "    print(f\"\\nmax_tokens={r['max_tokens']} [{status}]:\")\n",
    "    print(\"-\" * 40)\n",
    "    # Show first 200 chars to keep output manageable\n",
    "    preview = r[\"output\"][:300] + \"...\" if len(r[\"output\"]) > 300 else r[\"output\"]\n",
    "    print(preview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Temperature\n",
    "\n",
    "Temperature (range: **0.0 to 1.0**) controls randomness in token selection:\n",
    "- **Lower** (0.0-0.3) → More deterministic, consistent outputs\n",
    "- **Higher** (0.7-1.0) → More creative, varied outputs\n",
    "\n",
    "| Temperature | Behavior | Use Cases |\n",
    "|-------------|----------|----------|\n",
    "| **0.0** | Deterministic (same input = same output) | Extraction, classification, factual Q&A |\n",
    "| **0.3-0.5** | Low randomness | Summarization, general Q&A |\n",
    "| **0.7-1.0** | High randomness | Creative writing, brainstorming |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \"Write a one-sentence tagline for a coffee shop.\"\n",
      "\n",
      "Temperature: 0.0\n",
      "----------------------------------------\n",
      "  Run 1: \"Wake up to something extraordinary.\"\n",
      "  Run 2: \"Wake up to something extraordinary.\"\n",
      "  Run 3: \"Wake up to something extraordinary.\"\n",
      "\n",
      "Temperature: 0.7\n",
      "----------------------------------------\n",
      "  Run 1: \"Wake up to something extraordinary.\"\n",
      "  Run 2: \"Wake up to something extraordinary.\"\n",
      "  Run 3: \"Wake up to something extraordinary.\"\n",
      "\n",
      "Temperature: 1.0\n",
      "----------------------------------------\n",
      "  Run 1: \"Wake up to something wonderful.\"\n",
      "  Run 2: \"Wake up to what matters—great coffee, good people, and the ...\n",
      "  Run 3: \"Start your day the right way—where every cup tells a story....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate temperature effect on output consistency\n",
    "\n",
    "prompt = \"Write a one-sentence tagline for a coffee shop.\"\n",
    "\n",
    "print(f'Prompt: \"{prompt}\"\\n')\n",
    "\n",
    "for temp in [0.0, 0.7, 1.0]:\n",
    "    print(f\"Temperature: {temp}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # Run 3 times to show consistency/variation\n",
    "    for i in range(3):\n",
    "        response = bedrock_runtime.converse(\n",
    "            modelId=SMALL_MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": [{\"text\": prompt}]}],\n",
    "            inferenceConfig={\"maxTokens\": 50, \"temperature\": temp},\n",
    "        )\n",
    "        output = response[\"output\"][\"message\"][\"content\"][0][\"text\"].strip()\n",
    "        print(f\"  Run {i + 1}: {output[:60]}...\" if len(output) > 60 else f\"  Run {i + 1}: {output}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Prompt Caching Basics\n",
    "\n",
    "Prompt caching allows you to cache static portions of your prompt and reuse them across multiple requests.\n",
    "\n",
    "### How Caching Works\n",
    "\n",
    "| Step | What Happens | Cost |\n",
    "|------|--------------|------|\n",
    "| **First Request** | Static content is cached | 1.25x normal cost (5min cache) or 2x (1h cache) |\n",
    "| **Subsequent Requests** | Cached content is reused | 0.1x normal cost (cache read) |\n",
    "| **Cache Expiry** | After TTL with no hits | Cache is cleared |\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<strong>Key Benefit:</strong> Cache reads cost only 10% of normal input cost - <strong>up to 90% savings</strong> on cached content with high hit rates.\n",
    "</div>\n",
    "\n",
    "### Caching Requirements by Model\n",
    "\n",
    "| Model | Minimum Tokens per Checkpoint |\n",
    "|-------|-------------------------------|\n",
    "| Claude Sonnet 4.5 | 1,024 |\n",
    "| Claude Haiku 4.5 | 2,048 |\n",
    "| Claude Opus 4.5 | 1,024 |\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<strong>Note:</strong> Minimum token requirements vary by model. Content below the minimum will not be cached. See <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-caching.html\">Prompt Caching Documentation</a> for the latest requirements. Cache pricing varies by TTL option - verify at <a href=\"https://aws.amazon.com/bedrock/pricing/\">Amazon Bedrock Pricing</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache metrics utilities loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import cache metrics utilities\n",
    "from utils.cache_metrics import calculate_cache_savings, extract_cache_metrics\n",
    "\n",
    "print(\"Cache metrics utilities loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Sample Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product manual loaded\n",
      "  Word count: 1,023\n",
      "  Estimated tokens: ~1,329\n",
      "  Meets caching minimum (1,024 for Sonnet): Yes\n"
     ]
    }
   ],
   "source": [
    "# Load product documentation from file\n",
    "with open(\"data/product_manual.txt\") as f:\n",
    "    PRODUCT_MANUAL = f.read()\n",
    "\n",
    "# Estimate token count\n",
    "word_count = len(PRODUCT_MANUAL.split())\n",
    "estimated_tokens = int(word_count * 1.3)  # Rough estimate\n",
    "\n",
    "print(\"Product manual loaded\")\n",
    "print(f\"  Word count: {word_count:,}\")\n",
    "print(f\"  Estimated tokens: ~{estimated_tokens:,}\")\n",
    "print(f\"  Meets caching minimum (1,024 for Sonnet): {'Yes' if estimated_tokens >= 1024 else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Basic Caching\n",
    "\n",
    "To enable caching with the Converse API, add a `cachePoint` after the static content:\n",
    "\n",
    "```python\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\"text\": \"Your static content here...\"},\n",
    "        {\"cachePoint\": {\"type\": \"default\"}},  # <-- Cache checkpoint\n",
    "        {\"text\": \"Your dynamic query here...\"}\n",
    "    ]\n",
    "}]\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<strong>Important:</strong> Place static content <em>before</em> the cache checkpoint and dynamic content <em>after</em> it.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document query function with caching defined!\n"
     ]
    }
   ],
   "source": [
    "def query_document_with_cache(user_query, document=PRODUCT_MANUAL, model_id=MEDIUM_MODEL):\n",
    "    \"\"\"\n",
    "    Query a document using single-checkpoint caching.\n",
    "\n",
    "    The document is placed before the cache checkpoint (cached),\n",
    "    and the user query is placed after (not cached).\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    # Static content to cache\n",
    "                    \"text\": f\"\"\"You are a helpful assistant. Use the following product manual to answer questions.\n",
    "\n",
    "PRODUCT MANUAL:\n",
    "{document}\n",
    "\n",
    "Answer the user's question based on the information in the manual. If the answer is not in the manual, say so.\"\"\"\n",
    "                },\n",
    "                {\n",
    "                    # Cache checkpoint - everything before this gets cached\n",
    "                    \"cachePoint\": {\"type\": \"default\"}\n",
    "                },\n",
    "                {\n",
    "                    # Dynamic content (not cached)\n",
    "                    \"text\": f\"\\n\\nQUESTION: {user_query}\"\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    response = bedrock_runtime.converse(\n",
    "        modelId=model_id, messages=messages, inferenceConfig={\"maxTokens\": 500, \"temperature\": 0.0}\n",
    "    )\n",
    "\n",
    "    response_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    metrics = extract_cache_metrics(response)\n",
    "\n",
    "    return response_text, metrics\n",
    "\n",
    "\n",
    "print(\"Document query function with caching defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo: Cache in Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DEMO: Single-Checkpoint Caching\n",
      "================================================================================\n",
      "\n",
      "Query 1: What is the return policy for electronics?\n",
      "\n",
      "Response: According to the product manual, **electronics have a 14-day return window** from the purchase date.\n",
      "\n",
      "For the return to be accepted, items must be:\n",
      "- ...\n",
      "\n",
      "Cache metrics:\n",
      "  Input tokens (fresh):     15\n",
      "  Cache write tokens:       1,936\n",
      "  Cache read tokens:        0\n",
      "------------------------------------------------------------\n",
      "\n",
      "Query 2: How much does the Professional tier cost?\n",
      "\n",
      "Response: According to the product manual, the **Professional tier costs $299/month**.\n",
      "\n",
      "This tier includes:\n",
      "- Up to 10,000 orders/month\n",
      "- 10 user accounts with ...\n",
      "\n",
      "Cache metrics:\n",
      "  Input tokens (fresh):     15\n",
      "  Cache write tokens:       0\n",
      "  Cache read tokens:        1,936\n",
      "------------------------------------------------------------\n",
      "\n",
      "Query 3: What shipping options are available?\n",
      "\n",
      "Response: Based on the product manual, the following shipping options are available:\n",
      "\n",
      "1. **Standard Shipping**: 5-7 business days, FREE on orders over $50\n",
      "\n",
      "2. *...\n",
      "\n",
      "Cache metrics:\n",
      "  Input tokens (fresh):     13\n",
      "  Cache write tokens:       0\n",
      "  Cache read tokens:        1,936\n",
      "------------------------------------------------------------\n",
      "\n",
      "Query 4: What are the API rate limits for standard tier?\n",
      "\n",
      "Response: According to the product manual, the API rate limits for the standard tier are **1,000 requests per minute**.\n",
      "\n",
      "The manual specifies this in the Techni...\n",
      "\n",
      "Cache metrics:\n",
      "  Input tokens (fresh):     17\n",
      "  Cache write tokens:       0\n",
      "  Cache read tokens:        1,936\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test multiple queries to see caching in action\n",
    "queries = [\n",
    "    \"What is the return policy for electronics?\",\n",
    "    \"How much does the Professional tier cost?\",\n",
    "    \"What shipping options are available?\",\n",
    "    \"What are the API rate limits for standard tier?\",\n",
    "]\n",
    "\n",
    "all_metrics = []\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DEMO: Single-Checkpoint Caching\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, query in enumerate(queries, 1):\n",
    "    print(f\"\\nQuery {i}: {query}\")\n",
    "\n",
    "    response, metrics = query_document_with_cache(query)\n",
    "    all_metrics.append(metrics)\n",
    "\n",
    "    # Show abbreviated response\n",
    "    response_preview = response[:150] + \"...\" if len(response) > 150 else response\n",
    "    print(f\"\\nResponse: {response_preview}\")\n",
    "\n",
    "    # Show cache metrics\n",
    "    print(\"\\nCache metrics:\")\n",
    "    print(f\"  Input tokens (fresh):     {metrics['input_tokens']:,}\")\n",
    "    print(f\"  Cache write tokens:       {metrics['cache_write']:,}\")\n",
    "    print(f\"  Cache read tokens:        {metrics['cache_read']:,}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Cache Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CACHE PERFORMANCE SUMMARY\n",
      "============================================================\n",
      "Total requests:      4\n",
      "Cache hit rate:      75.0%\n",
      "\n",
      "Cost WITH caching:    $0.009182\n",
      "Cost WITHOUT caching: $0.023412\n",
      "\n",
      "Savings:             $0.014230 (60.8% reduction)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Calculate overall cache savings\n",
    "# Using Sonnet 4.5 pricing: input $0.003/1K, cache read $0.0003/1K (0.1x)\n",
    "savings = calculate_cache_savings(all_metrics, input_price_per_million=3.0)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CACHE PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total requests:      {savings['total_requests']}\")\n",
    "print(f\"Cache hit rate:      {savings['cache_hit_rate']:.1f}%\")\n",
    "print(\"\")\n",
    "print(f\"Cost WITH caching:    ${savings['cost_with_cache']:.6f}\")\n",
    "print(f\"Cost WITHOUT caching: ${savings['cost_no_cache']:.6f}\")\n",
    "print(\"\")\n",
    "print(f\"Savings:             ${savings['savings']:.6f} ({savings['savings_pct']:.1f}% reduction)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Results\n",
    "\n",
    "**Request 1 (Cache Write):**\n",
    "- First occurrence of the document\n",
    "- Document tokens written to cache (1.25x cost)\n",
    "- Higher initial investment\n",
    "\n",
    "**Requests 2-4 (Cache Hit):**\n",
    "- Document retrieved from cache (0.1x cost - 90% savings!)\n",
    "- Only the new user query processed fresh\n",
    "- Significant cost reduction\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<strong>Key Insight:</strong> The break-even point for caching is typically 2-3 requests. After that, every additional request saves ~90% on cached content!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, you learned practical optimization strategies for Amazon Bedrock:\n",
    "\n",
    "| Technique | Key Takeaway |\n",
    "|-----------|-------------|\n",
    "| **Model Selection** | Start small, scale up; smaller models are 3-10x cheaper and often equally effective for simple tasks |\n",
    "| **Clear Instructions** | Specific prompts reduce output tokens; use imperative language (\"Extract...\" not \"Could you...\") |\n",
    "| **Few-Shot Examples** | Improves format consistency; use when zero-shot produces inconsistent results |\n",
    "| **Structured Output** | Prompt-based for flexibility (JSON/XML); Tool Use for guaranteed schema compliance |\n",
    "| **max_tokens** | Right-size to expected output + 10-15% buffer; over-setting wastes quota capacity |\n",
    "| **Temperature** | Use 0.0 for deterministic tasks (extraction, classification); higher for creative tasks |\n",
    "| **Prompt Caching** | Cache static content for up to 90% input cost savings; break-even at 2-3 requests |\n",
    "\n",
    "### What's Next\n",
    "\n",
    "In the next notebook, **03-langfuse-observability.ipynb**, you will learn:\n",
    "- Setting up LangFuse for prompt tracing\n",
    "- Tracking cost, latency, and token usage\n",
    "- Building dashboards for production monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [Supported models in Amazon Bedrock](https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html)\n",
    "- [Prompt engineering concepts](https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-engineering-guidelines.html)\n",
    "- [Inference request parameters](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html)\n",
    "- [Prompt caching](https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-caching.html)\n",
    "- [Tool use with Converse API](https://docs.aws.amazon.com/bedrock/latest/userguide/tool-use.html)\n",
    "- [Monitoring Amazon Bedrock](https://docs.aws.amazon.com/bedrock/latest/userguide/monitoring.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws-bedrock-prompt-optimization-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}