{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 01: Baseline Agent\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we deploy an **intentionally unoptimized** customer support agent to establish baseline metrics. This baseline will be used to measure improvements in subsequent notebooks.\n",
    "\n",
    "**What you'll learn:**\n",
    "- How to deploy an agent to AgentCore Runtime\n",
    "- How to configure Langfuse for observability\n",
    "- How to invoke agents via the AgentCore API\n",
    "- How to establish baseline metrics for cost and latency\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- AWS account with Bedrock and AgentCore access\n",
    "- Langfuse account (free tier works)\n",
    "- `.env` file configured with your credentials\n",
    "\n",
    "## Workshop Journey\n",
    "\n",
    "```\n",
    "[01 Baseline] → 02 Quick Wins → 03 Caching → 04 Routing → 05 Guardrails → 06 Gateway → 07 Evaluations\n",
    "     ↑\n",
    "  You are here\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Dependencies\n",
    "\n",
    "Before starting, ensure you've run `uv sync` in the terminal and selected the `.venv` kernel in VS Code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "import boto3\n",
    "from bedrock_agentcore_starter_toolkit import Runtime\n",
    "\n",
    "region = os.environ.get(\"AWS_DEFAULT_REGION\", \"us-east-1\")\n",
    "control_client = boto3.client(\"bedrock-agentcore-control\", region_name=region)\n",
    "data_client = boto3.client(\"bedrock-agentcore\", region_name=region)\n",
    "agentcore_runtime = Runtime()\n",
    "\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Langfuse Host: {os.environ.get('LANGFUSE_BASE_URL', 'Not set')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Review the Baseline Agent\n",
    "\n",
    "Our baseline agent (`agents/v1_baseline.py`) is intentionally unoptimized:\n",
    "\n",
    "- **Verbose system prompt** (~1500 tokens instead of ~250)\n",
    "- **No max_tokens limit** (model can generate unlimited output)\n",
    "- **No prompt caching** (system prompt processed every request)\n",
    "- **No stop sequences** (model decides when to stop)\n",
    "\n",
    "Let's examine the key parts of the baseline agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and display the baseline agent code\n",
    "agent_file = Path(\"agents/v1_baseline.py\")\n",
    "print(agent_file.read_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configure the Agent for Deployment\n",
    "\n",
    "Now we'll configure the agent for deployment to AgentCore Runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_name = \"customer_support_v1_baseline\"\n",
    "agent_file = str(Path(\"agents/v1_baseline.py\").absolute())\n",
    "requirements_file = str(Path(\"requirements-for-agentcore.txt\").absolute())\n",
    "\n",
    "print(f\"Agent name: {agent_name}\")\n",
    "print(f\"Agent file: {agent_file}\")\n",
    "print(f\"Requirements: {requirements_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up any existing build files from previous labs\n",
    "for f in [\"Dockerfile\", \".dockerignore\", \".bedrock_agentcore.yaml\"]:\n",
    "    p = Path(f)\n",
    "    if p.exists():\n",
    "        p.unlink()\n",
    "        print(f\"Removed existing: {f}\")\n",
    "\n",
    "# Configure the runtime\n",
    "print(f\"Configuring agent: {agent_name}\")\n",
    "agentcore_runtime.configure(\n",
    "    entrypoint=agent_file,\n",
    "    auto_create_execution_role=True,\n",
    "    auto_create_ecr=True,\n",
    "    requirements_file=requirements_file,\n",
    "    region=region,\n",
    "    agent_name=agent_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Modify Dockerfile for Langfuse\n",
    "\n",
    "We need to disable the default OpenTelemetry instrumentation and use Langfuse instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dockerfile_path = Path(\"Dockerfile\")\n",
    "if dockerfile_path.exists():\n",
    "    content = dockerfile_path.read_text()\n",
    "    # Replace opentelemetry-instrument wrapper with direct python call\n",
    "    # Keep the correct module path: agents.v1_baseline\n",
    "    if \"opentelemetry-instrument\" in content:\n",
    "        import re\n",
    "\n",
    "        content = re.sub(\n",
    "            r'CMD \\[\"opentelemetry-instrument\", \"python\", \"-m\", \"([^\"]+)\"\\]', r'CMD [\"python\", \"-m\", \"\\1\"]', content\n",
    "        )\n",
    "        dockerfile_path.write_text(content)\n",
    "        print(\"Dockerfile modified for Langfuse\")\n",
    "    else:\n",
    "        print(\"Dockerfile already configured or using different format\")\n",
    "else:\n",
    "    print(\"Dockerfile not found - will be created during deployment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Deploy to AgentCore Runtime\n",
    "\n",
    "Deploy the agent with Langfuse environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_vars = {\n",
    "    \"LANGFUSE_BASE_URL\": os.environ.get(\"LANGFUSE_BASE_URL\"),\n",
    "    \"LANGFUSE_PUBLIC_KEY\": os.environ.get(\"LANGFUSE_PUBLIC_KEY\"),\n",
    "    \"LANGFUSE_SECRET_KEY\": os.environ.get(\"LANGFUSE_SECRET_KEY\"),\n",
    "    \"PYTHONUNBUFFERED\": \"1\",\n",
    "}\n",
    "\n",
    "print(\"Deploying to AgentCore Runtime...\")\n",
    "print(\"This may take 5-10 minutes for the first deployment.\")\n",
    "launch_result = agentcore_runtime.launch(env_vars=env_vars, auto_update_on_conflict=True)\n",
    "print(f\"Agent deployed: {launch_result.agent_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the agent ARN for later use\n",
    "agent_arn = launch_result.agent_arn\n",
    "print(f\"Agent ARN: {agent_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test the Baseline Agent\n",
    "\n",
    "Let's invoke the agent with some test queries to establish our baseline metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_agent(prompt):\n",
    "    \"\"\"Invoke the agent via AgentCore API.\"\"\"\n",
    "    response = data_client.invoke_agent_runtime(\n",
    "        agentRuntimeArn=agent_arn,\n",
    "        runtimeSessionId=str(uuid.uuid4()),\n",
    "        payload=json.dumps({\"prompt\": prompt}).encode(),\n",
    "    )\n",
    "    return json.loads(response[\"response\"].read().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Langfuse metrics helper\n",
    "from utils.langfuse_metrics import (\n",
    "    clear_metrics,\n",
    "    collect_metric,\n",
    "    get_latest_trace_metrics,\n",
    "    print_metrics,\n",
    "    print_metrics_table,\n",
    ")\n",
    "\n",
    "# Clear any previously collected metrics\n",
    "clear_metrics()\n",
    "\n",
    "\n",
    "def invoke_agent_with_metrics(prompt, test_name=\"\"):\n",
    "    \"\"\"Invoke the agent and fetch + print Langfuse metrics.\"\"\"\n",
    "    response = invoke_agent(prompt)\n",
    "    print(response)\n",
    "\n",
    "    # Fetch and print metrics from Langfuse\n",
    "    metrics = get_latest_trace_metrics(\n",
    "        agent_name=\"customer-support-v1-baseline\",\n",
    "        wait_seconds=5,\n",
    "        max_retries=5,\n",
    "        timeout_seconds=120,\n",
    "    )\n",
    "    print_metrics(metrics, test_name)\n",
    "\n",
    "    # Collect metrics for summary table\n",
    "    collect_metric(metrics, test_name)\n",
    "\n",
    "    return response, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard test prompts - same across all notebooks for consistent comparison\n",
    "TEST_PROMPTS = [\n",
    "    # Single tool: get_return_policy\n",
    "    (\"Return Policy\", \"What is your return policy for laptops?\"),\n",
    "    # Single tool: get_product_info\n",
    "    (\"Product Info\", \"Tell me about your smartphone options\"),\n",
    "    # Single tool: get_technical_support (Bedrock KB)\n",
    "    (\"Technical Support\", \"My laptop won't turn on, can you help me troubleshoot?\"),\n",
    "    # Multi-tool: get_product_info + get_return_policy\n",
    "    (\"Multi-part Question\", \"I want to buy a laptop. What are the specs and what's the return policy?\"),\n",
    "    # No tool: General greeting\n",
    "    (\"General Question\", \"Hello! What can you help me with today?\"),\n",
    "]\n",
    "\n",
    "# Run all tests and collect metrics\n",
    "for test_name, prompt in TEST_PROMPTS:\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Test: {test_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    response, metrics = invoke_agent_with_metrics(prompt, test_name=test_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary table of all test metrics\n",
    "print_metrics_table()\n",
    "\n",
    "# Save metrics for comparison in later notebooks\n",
    "from utils.langfuse_metrics import save_metrics\n",
    "save_metrics(\"v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: View Langfuse Dashboard\n",
    "\n",
    "Now let's check the Langfuse dashboard to see our baseline metrics.\n",
    "\n",
    "### What to look for:\n",
    "\n",
    "1. **Token Usage**: Note the input and output token counts\n",
    "2. **Latency**: Time taken for each request\n",
    "3. **Cost**: Estimated cost per request\n",
    "4. **No Cache Hits**: `cacheReadInputTokens` should be 0 (no caching)\n",
    "\n",
    "### Dashboard URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langfuse_base_url = os.environ.get(\"LANGFUSE_BASE_URL\", \"https://cloud.langfuse.com\")\n",
    "print(f\"View your traces at: {langfuse_base_url}\")\n",
    "print(\"\\nFilter by tags: 'baseline', 'no-optimization'\")\n",
    "print(\"\\nMetrics to record:\")\n",
    "print(\"- Average input tokens: _____\")\n",
    "print(\"- Average output tokens: _____\")\n",
    "print(\"- Average latency: _____ ms\")\n",
    "print(\"- Cache read tokens: 0 (expected)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "1. Deployed an unoptimized baseline agent to AgentCore Runtime\n",
    "2. Configured Langfuse for observability\n",
    "3. Ran test scenarios to establish baseline metrics\n",
    "4. Identified areas for optimization:\n",
    "   - Verbose system prompt (~1500 tokens)\n",
    "   - No output token limits\n",
    "   - No prompt caching\n",
    "   - No model routing\n",
    "\n",
    "**Next Steps:** In the next notebook, we'll apply \"quick wins\" optimizations:\n",
    "- Concise system prompt\n",
    "- max_tokens limit\n",
    "- stop_sequences\n",
    "\n",
    "**Next notebook:** [02-quick-wins.ipynb](./02-quick-wins.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup (Optional)\n",
    "\n",
    "Uncomment and run if you want to delete the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to delete resources created in this lab\n",
    "# agentcore_runtime.destroy(delete_ecr_repo=True)\n",
    "# print(f\"Deleted agent and ECR repository: {agent_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws-bedrock-prompt-optimization-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
