{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 02: Quick Wins\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we apply **low-effort, high-impact** optimizations to reduce costs and improve response quality. The key insight is that **prompt structure matters more than length**.\n",
    "\n",
    "**What you'll learn:**\n",
    "- How to structure system prompts for clarity and consistency\n",
    "- How to use `max_tokens` to limit output length\n",
    "- How to use `stop_sequences` to end responses early\n",
    "- How to set appropriate `temperature` for accuracy\n",
    "\n",
    "**Optimizations in this notebook:**\n",
    "1. Well-structured prompt with sections, guidelines, and few-shot examples (~1,030 tokens vs ~1,500 unstructured)\n",
    "2. `max_tokens=1024` limit (allows detailed responses)\n",
    "3. Stop sequences `[\"###\", \"END_RESPONSE\"]`\n",
    "4. `temperature=0.1` for accurate, consistent customer support responses\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed Lab 01 (baseline agent deployed)\n",
    "- Baseline metrics recorded\n",
    "\n",
    "## Workshop Journey\n",
    "\n",
    "```\n",
    "01 Baseline → [02 Quick Wins] → 03 Caching → 04 Routing → 05 Guardrails → 06 Gateway → 07 Evaluations\n",
    "                   ↑\n",
    "              You are here\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "import boto3\n",
    "from bedrock_agentcore_starter_toolkit import Runtime\n",
    "\n",
    "region = os.environ.get(\"AWS_DEFAULT_REGION\", \"us-east-1\")\n",
    "control_client = boto3.client(\"bedrock-agentcore-control\", region_name=region)\n",
    "data_client = boto3.client(\"bedrock-agentcore\", region_name=region)\n",
    "agentcore_runtime = Runtime()\n",
    "\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Langfuse Host: {os.environ.get('LANGFUSE_BASE_URL', 'Not set')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Review the Quick Wins Optimizations\n",
    "\n",
    "Let's compare the baseline vs optimized configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization 1: Well-Structured System Prompt\n",
    "\n",
    "### Baseline Prompt (v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.v1_baseline import SYSTEM_PROMPT as BASELINE_PROMPT\n",
    "\n",
    "print(BASELINE_PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opportunities for Improvement\n",
    "\n",
    "| Area | Observation |\n",
    "|------|-------------|\n",
    "| **Structure** | Dense paragraphs without visual hierarchy make it difficult for the model to quickly locate relevant instructions |\n",
    "| **Hedging language** | Phrases like \"try to\", \"as best you can\", \"hopefully\", \"if possible\" introduce ambiguity about expected behavior |\n",
    "| **Filler phrases** | \"Please\", \"Can you please\", \"It would be great if\" add tokens without providing actionable guidance |\n",
    "| **Task definition** | The expected output format and response structure are not specified |\n",
    "| **Redundancy** | Adjective chains like \"helpful and friendly and professional and also knowledgeable and empathetic\" could be condensed |\n",
    "\n",
    "**Estimated token count: ~1,500 tokens**\n",
    "\n",
    "---\n",
    "\n",
    "### Optimized Prompt (v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.agent_config import SYSTEM_PROMPT_TEXT as OPTIMIZED_PROMPT\n",
    "\n",
    "print(OPTIMIZED_PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Makes This Prompt Effective\n",
    "\n",
    "| Technique | Implementation |\n",
    "|-----------|----------------|\n",
    "| **Visual hierarchy** | Headers (`# ROLE`, `# GUIDELINES`, `# EXAMPLES`) organize content into scannable sections |\n",
    "| **Direct language** | \"You are Alex\" instead of \"Please try to be...\"; action-oriented guidelines |\n",
    "| **Numbered guidelines** | Clear, prioritized instructions (1-6) that the model can reference |\n",
    "| **Explicit output format** | Defines required fields: answer, category, confidence |\n",
    "| **Few-shot examples** | 4 examples demonstrate tool usage, response structure, and expected behavior |\n",
    "\n",
    "**Key insight:** Few-shot examples are one of the most effective prompting techniques. They show the model exactly what you want—tool calling patterns, output structure, and tone—without verbose explanations.\n",
    "\n",
    "**Estimated token count: ~1,030 tokens** (32% reduction, meets 1,024-token minimum for prompt caching)\n",
    "\n",
    "---\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "For more on prompt engineering best practices, see:\n",
    "\n",
    "- [Anthropic: Prompt Engineering Overview](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview)\n",
    "- [AWS: Prompt Engineering Techniques with Claude on Amazon Bedrock](https://aws.amazon.com/blogs/machine-learning/prompt-engineering-techniques-and-best-practices-learn-by-doing-with-anthropics-claude-3-on-amazon-bedrock/)\n",
    "- [OpenAI: Prompt Engineering Guide](https://platform.openai.com/docs/guides/prompt-engineering)\n",
    "- [Anthropic: Use XML Tags to Structure Your Prompts](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/use-xml-tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizations 2, 3, 4: Model Configuration\n",
    "\n",
    "### Optimization 2: `max_tokens=1024`\n",
    "\n",
    "Limits output length to prevent runaway responses. 1024 tokens is enough for detailed troubleshooting while keeping costs predictable.\n",
    "\n",
    "### Optimization 3: `stop_sequences=[\"###\", \"END_RESPONSE\"]`\n",
    "\n",
    "Allows early termination when the response is complete. The model can stop generating before hitting `max_tokens` if it reaches a natural endpoint.\n",
    "\n",
    "### Optimization 4: `temperature=0.1`\n",
    "\n",
    "Low temperature for accuracy and consistency. Customer support needs factual, predictable responses—not creative variations.\n",
    "\n",
    "| Temperature | Use Case |\n",
    "|-------------|----------|\n",
    "| 0.0 - 0.3 | Factual tasks, customer support, code generation |\n",
    "| 0.4 - 0.7 | Balanced creativity and accuracy |\n",
    "| 0.8 - 1.0 | Creative writing, brainstorming |\n",
    "\n",
    "### Final Model Configuration\n",
    "\n",
    "```python\n",
    "model = BedrockModel(\n",
    "    model_id=\"us.anthropic.claude-sonnet-4-5-20250929-v1:0\",\n",
    "    temperature=0.1,                          # Optimization 4\n",
    "    max_tokens=1024,                          # Optimization 2\n",
    "    stop_sequences=[\"###\", \"END_RESPONSE\"],   # Optimization 3\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the v2 agent code\n",
    "agent_file = Path(\"agents/v2_quick_wins.py\")\n",
    "print(agent_file.read_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Deploy the Quick Wins Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_name = \"customer_support_v2_quick_wins\"\n",
    "agent_file = str(Path(\"agents/v2_quick_wins.py\").absolute())\n",
    "requirements_file = str(Path(\"requirements-for-agentcore.txt\").absolute())\n",
    "\n",
    "# Clean up any existing build files from previous labs\n",
    "for f in [\"Dockerfile\", \".dockerignore\", \".bedrock_agentcore.yaml\"]:\n",
    "    p = Path(f)\n",
    "    if p.exists():\n",
    "        p.unlink()\n",
    "        print(f\"Removed existing: {f}\")\n",
    "\n",
    "print(f\"Configuring agent: {agent_name}\")\n",
    "agentcore_runtime.configure(\n",
    "    entrypoint=agent_file,\n",
    "    auto_create_execution_role=True,\n",
    "    auto_create_ecr=True,\n",
    "    requirements_file=requirements_file,\n",
    "    region=region,\n",
    "    agent_name=agent_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify Dockerfile for Langfuse\n",
    "dockerfile_path = Path(\"Dockerfile\")\n",
    "if dockerfile_path.exists():\n",
    "    content = dockerfile_path.read_text()\n",
    "    # Replace opentelemetry-instrument wrapper with direct python call\n",
    "    # Keep the correct module path using regex\n",
    "    if \"opentelemetry-instrument\" in content:\n",
    "        import re\n",
    "\n",
    "        content = re.sub(\n",
    "            r'CMD \\[\"opentelemetry-instrument\", \"python\", \"-m\", \"([^\"]+)\"\\]', r'CMD [\"python\", \"-m\", \"\\1\"]', content\n",
    "        )\n",
    "        dockerfile_path.write_text(content)\n",
    "        print(\"Dockerfile modified for Langfuse\")\n",
    "    else:\n",
    "        print(\"Dockerfile already configured or using different format\")\n",
    "else:\n",
    "    print(\"Dockerfile not found - will be created during deployment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_vars = {\n",
    "    \"LANGFUSE_BASE_URL\": os.environ.get(\"LANGFUSE_BASE_URL\"),\n",
    "    \"LANGFUSE_PUBLIC_KEY\": os.environ.get(\"LANGFUSE_PUBLIC_KEY\"),\n",
    "    \"LANGFUSE_SECRET_KEY\": os.environ.get(\"LANGFUSE_SECRET_KEY\"),\n",
    "    \"PYTHONUNBUFFERED\": \"1\",\n",
    "}\n",
    "\n",
    "print(\"Deploying to AgentCore Runtime...\")\n",
    "launch_result = agentcore_runtime.launch(env_vars=env_vars, auto_update_on_conflict=True)\n",
    "agent_arn = launch_result.agent_arn\n",
    "print(f\"Agent deployed: {agent_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the agent ARN for later use\n",
    "agent_arn = launch_result.agent_arn\n",
    "print(f\"Agent ARN: {agent_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test the Optimized Agent\n",
    "\n",
    "Run the same test scenarios as the baseline to compare metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_agent(prompt):\n",
    "    \"\"\"Invoke the agent via AgentCore API.\"\"\"\n",
    "    response = data_client.invoke_agent_runtime(\n",
    "        agentRuntimeArn=agent_arn,\n",
    "        runtimeSessionId=str(uuid.uuid4()),\n",
    "        payload=json.dumps({\"prompt\": prompt}).encode(),\n",
    "    )\n",
    "    return json.loads(response[\"response\"].read().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Langfuse metrics helper\n",
    "from utils.langfuse_metrics import (\n",
    "    clear_metrics,\n",
    "    collect_metric,\n",
    "    get_latest_trace_metrics,\n",
    "    print_metrics,\n",
    "    print_metrics_table,\n",
    ")\n",
    "\n",
    "# Clear any previously collected metrics\n",
    "clear_metrics()\n",
    "\n",
    "# Standard test prompts - each demonstrates a specific tool usage pattern\n",
    "TEST_PROMPTS = [\n",
    "    # Single tool: get_return_policy\n",
    "    (\"Return Policy\", \"What is your return policy for laptops?\"),\n",
    "    # Single tool: get_product_info\n",
    "    (\"Product Info\", \"Tell me about your smartphone options\"),\n",
    "    # Single tool: get_technical_support (Bedrock KB)\n",
    "    (\"Technical Support\", \"My laptop won't turn on, can you help me troubleshoot?\"),\n",
    "    # Multi-tool: get_product_info + get_return_policy\n",
    "    (\"Multi-part Question\", \"I want to buy a laptop. What are the specs and what's the return policy?\"),\n",
    "    # No tool: General greeting\n",
    "    (\"General Question\", \"Hello! What can you help me with today?\"),\n",
    "]\n",
    "\n",
    "# Run all tests and collect metrics\n",
    "for test_name, prompt in TEST_PROMPTS:\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Test: {test_name}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    response = invoke_agent(prompt)\n",
    "    print(response)\n",
    "\n",
    "    # Fetch and collect metrics\n",
    "    metrics = get_latest_trace_metrics(\n",
    "        agent_name=\"customer-support-v2-quick-wins\",\n",
    "        wait_seconds=5,\n",
    "        max_retries=5,\n",
    "        timeout_seconds=120,\n",
    "    )\n",
    "    print_metrics(metrics, test_name)\n",
    "    collect_metric(metrics, test_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary table\n",
    "print_metrics_table()\n",
    "\n",
    "# Save metrics for comparison in later notebooks\n",
    "from utils.langfuse_metrics import save_metrics\n",
    "save_metrics(\"v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Compare with v1 (Baseline)\n",
    "\n",
    "Enter your metrics from Lab 01 (v1 baseline) to compare cost, latency, and token usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.langfuse_metrics import load_metrics, print_comparison\n",
    "\n",
    "# Load metrics from Lab 01 (saved automatically when you ran print_metrics_table())\n",
    "v1 = load_metrics(\"v1\")\n",
    "\n",
    "# Or enter manually if Lab 01 metrics weren't saved:\n",
    "# v1 = {\"total_cost\": 0.1042, \"avg_latency\": 8.90, \"total_input_tokens\": 24523, \"total_output_tokens\": 2040}\n",
    "\n",
    "# Print comparison (current metrics auto-calculated from collected)\n",
    "print_comparison(\n",
    "    prev_name=\"v1 (Baseline)\",\n",
    "    curr_name=\"v2 (Quick Wins)\",\n",
    "    prev_cost=v1[\"total_cost\"],\n",
    "    prev_latency=v1[\"avg_latency\"],\n",
    "    prev_input_tokens=v1[\"total_input_tokens\"],\n",
    "    prev_output_tokens=v1[\"total_output_tokens\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Analysis\n",
    "\n",
    "The comparison above shows the impact of simple, low-effort optimizations. By restructuring the system prompt and tuning model parameters, we achieved meaningful reductions in both token usage and cost—without changing the agent's functionality.\n",
    "\n",
    "**Key takeaways:**\n",
    "\n",
    "- **Structure matters more than brevity.** A well-organized prompt with headers, bullet points, and clear sections helps the model locate instructions efficiently. This reduces both input tokens (shorter prompt) and output tokens (more focused responses).\n",
    "\n",
    "- **Explicit constraints improve consistency.** Defining the expected output format and boundaries (\"Do NOT...\") leads to more predictable behavior across requests.\n",
    "\n",
    "- **Model parameters are free optimizations.** Setting `max_tokens`, `temperature`, and `stop_sequences` costs nothing to implement but can meaningfully reduce token usage and improve response quality.\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** In Lab 03, we'll explore **prompt caching**—a technique that can dramatically reduce costs when the same system prompt is used across multiple requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cleanup\n",
    "\n",
    "To delete the agent deployed in this notebook, uncomment and run the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to delete resources created in this lab\n",
    "# agentcore_runtime.destroy(delete_ecr_repo=True)\n",
    "# print(f\"Deleted agent and ECR repository: {agent_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "developer-journey (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
