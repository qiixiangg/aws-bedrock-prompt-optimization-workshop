{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 03: Prompt Caching\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we enable **prompt caching** to reduce costs on repeated requests. Prompt caching stores the processed system prompt and tool definitions, so subsequent requests can reuse them at a 90% discount.\n",
    "\n",
    "**What you'll learn:**\n",
    "- How to configure system prompt caching\n",
    "- How to enable tool definition caching\n",
    "- How to verify cache hits in Langfuse\n",
    "- How to calculate cost savings from caching\n",
    "\n",
    "**Optimizations in this notebook:**\n",
    "- `SystemContentBlock` with `cachePoint` (system prompt caching - provider-agnostic)\n",
    "- `cache_tools=\"default\"` on BedrockModel (tool definition caching)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed Labs 01-02\n",
    "\n",
    "## Workshop Journey\n",
    "\n",
    "```\n",
    "01 Baseline → 02 Quick Wins → [03 Caching] → 04 Routing → 05 Guardrails → 06 Gateway → 07 Evaluations\n",
    "                                   ↑\n",
    "                              You are here\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "import boto3\n",
    "from bedrock_agentcore_starter_toolkit import Runtime\n",
    "\n",
    "region = os.environ.get(\"AWS_DEFAULT_REGION\", \"us-east-1\")\n",
    "control_client = boto3.client(\"bedrock-agentcore-control\", region_name=region)\n",
    "data_client = boto3.client(\"bedrock-agentcore\", region_name=region)\n",
    "agentcore_runtime = Runtime()\n",
    "\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Langfuse Host: {os.environ.get('LANGFUSE_BASE_URL', 'Not set')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Understanding Prompt Caching\n",
    "\n",
    "### What is Prompt Caching?\n",
    "\n",
    "Prompt caching stores frequently reused content so subsequent requests can skip reprocessing it. This notebook enables **two types of caching**:\n",
    "\n",
    "| Cache Type | What's Cached | Configuration |\n",
    "|------------|---------------|---------------|\n",
    "| **System Prompt** | The entire system prompt (~1,100 tokens) | `SystemContentBlock` with `cachePoint` |\n",
    "| **Tool Definitions** | All tool schemas and descriptions | `cache_tools=\"default\"` on BedrockModel |\n",
    "\n",
    "### Minimum Token Requirement\n",
    "\n",
    "> **Important:** Claude Sonnet 4.5 requires **at least 1,024 tokens** before a cache checkpoint for caching to activate.\n",
    ">\n",
    "> If your system prompt is below this threshold:\n",
    "> - Inference still succeeds normally (no error)\n",
    "> - No cache checkpoint is created (silently skipped)\n",
    "> - `cacheWriteInputTokens` and `cacheReadInputTokens` remain 0\n",
    ">\n",
    "> **Solution:** Expand short system prompts with few-shot examples to meet the minimum.\n",
    "\n",
    "### System Prompt Caching\n",
    "\n",
    "Use `SystemContentBlock` with a cache point at the end:\n",
    "\n",
    "```python\n",
    "from strands.types.content import SystemContentBlock\n",
    "\n",
    "# Create system prompt with cache point\n",
    "system_prompt = [\n",
    "    SystemContentBlock(text=SYSTEM_PROMPT_TEXT),  # Must be 1,024+ tokens\n",
    "    SystemContentBlock(cachePoint={\"type\": \"default\"})  # Cache checkpoint\n",
    "]\n",
    "\n",
    "agent = Agent(system_prompt=system_prompt)\n",
    "```\n",
    "\n",
    "### Tool Definition Caching\n",
    "\n",
    "Enable tool caching via `BedrockModel`:\n",
    "\n",
    "```python\n",
    "model = BedrockModel(\n",
    "    model_id=\"us.anthropic.claude-sonnet-4-5-20250929-v1:0\",\n",
    "    cache_tools=\"default\"  # Cache all tool definitions\n",
    ")\n",
    "```\n",
    "\n",
    "### Pricing\n",
    "\n",
    "Cache pricing varies by model. See [Amazon Bedrock Pricing](https://aws.amazon.com/bedrock/pricing/) for current rates.\n",
    "\n",
    "### Cache Metrics in Langfuse\n",
    "\n",
    "- `cacheWriteInputTokens` - Tokens written to cache (first request after cache expires)\n",
    "- `cacheReadInputTokens` - Tokens read from cache (subsequent requests within 5-min TTL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the caching configuration in v3 agent\n",
    "agent_file = Path(\"agents/v3_caching.py\")\n",
    "print(agent_file.read_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Deploy the Caching Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_name = \"customer_support_v3_caching\"\n",
    "agent_file = str(Path(\"agents/v3_caching.py\").absolute())\n",
    "requirements_file = str(Path(\"requirements-for-agentcore.txt\").absolute())\n",
    "\n",
    "# Clean up any existing build files from previous labs\n",
    "for f in [\"Dockerfile\", \".dockerignore\", \".bedrock_agentcore.yaml\"]:\n",
    "    p = Path(f)\n",
    "    if p.exists():\n",
    "        p.unlink()\n",
    "        print(f\"Removed existing: {f}\")\n",
    "\n",
    "print(f\"Configuring agent: {agent_name}\")\n",
    "agentcore_runtime.configure(\n",
    "    entrypoint=agent_file,\n",
    "    auto_create_execution_role=True,\n",
    "    auto_create_ecr=True,\n",
    "    requirements_file=requirements_file,\n",
    "    region=region,\n",
    "    agent_name=agent_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify Dockerfile for Langfuse\n",
    "dockerfile_path = Path(\"Dockerfile\")\n",
    "if dockerfile_path.exists():\n",
    "    content = dockerfile_path.read_text()\n",
    "    # Replace opentelemetry-instrument wrapper with direct python call\n",
    "    # Keep the correct module path using regex\n",
    "    if \"opentelemetry-instrument\" in content:\n",
    "        import re\n",
    "\n",
    "        content = re.sub(\n",
    "            r'CMD \\[\"opentelemetry-instrument\", \"python\", \"-m\", \"([^\"]+)\"\\]', r'CMD [\"python\", \"-m\", \"\\1\"]', content\n",
    "        )\n",
    "        dockerfile_path.write_text(content)\n",
    "        print(\"Dockerfile modified for Langfuse\")\n",
    "    else:\n",
    "        print(\"Dockerfile already configured or using different format\")\n",
    "else:\n",
    "    print(\"Dockerfile not found - will be created during deployment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_vars = {\n",
    "    \"LANGFUSE_BASE_URL\": os.environ.get(\"LANGFUSE_BASE_URL\"),\n",
    "    \"LANGFUSE_PUBLIC_KEY\": os.environ.get(\"LANGFUSE_PUBLIC_KEY\"),\n",
    "    \"LANGFUSE_SECRET_KEY\": os.environ.get(\"LANGFUSE_SECRET_KEY\"),\n",
    "    \"PYTHONUNBUFFERED\": \"1\",\n",
    "}\n",
    "\n",
    "print(\"Deploying to AgentCore Runtime...\")\n",
    "launch_result = agentcore_runtime.launch(env_vars=env_vars, auto_update_on_conflict=True)\n",
    "agent_arn = launch_result.agent_arn\n",
    "print(f\"Agent deployed: {agent_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the agent ARN for later use\n",
    "agent_arn = launch_result.agent_arn\n",
    "print(f\"Agent ARN: {agent_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test Caching Behavior\n",
    "\n",
    "Run the standard test prompts and observe cache metrics in Langfuse.\n",
    "\n",
    "**What to look for:**\n",
    "- `Cache Read Tokens` > 0 indicates the system prompt and tool definitions are being read from cache\n",
    "- `Cache Write Tokens` > 0 indicates tokens were written to cache (only happens when cache is cold/expired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_agent(prompt):\n",
    "    \"\"\"Invoke the agent via AgentCore API.\"\"\"\n",
    "    response = data_client.invoke_agent_runtime(\n",
    "        agentRuntimeArn=agent_arn,\n",
    "        runtimeSessionId=str(uuid.uuid4()),\n",
    "        payload=json.dumps({\"prompt\": prompt}).encode(),\n",
    "    )\n",
    "    return json.loads(response[\"response\"].read().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Langfuse metrics helper\n",
    "from utils.langfuse_metrics import (\n",
    "    clear_metrics,\n",
    "    collect_metric,\n",
    "    get_latest_trace_metrics,\n",
    "    print_metrics,\n",
    "    print_metrics_table,\n",
    ")\n",
    "\n",
    "# Clear any previously collected metrics\n",
    "clear_metrics()\n",
    "\n",
    "# Standard test prompts - same across all notebooks for consistent comparison\n",
    "TEST_PROMPTS = [\n",
    "    # Single tool: get_return_policy\n",
    "    (\"Return Policy\", \"What is your return policy for laptops?\"),\n",
    "    # Single tool: get_product_info\n",
    "    (\"Product Info\", \"Tell me about your smartphone options\"),\n",
    "    # Single tool: get_technical_support (Bedrock KB)\n",
    "    (\"Technical Support\", \"My laptop won't turn on, can you help me troubleshoot?\"),\n",
    "    # Multi-tool: get_product_info + get_return_policy\n",
    "    (\"Multi-part Question\", \"I want to buy a laptop. What are the specs and what's the return policy?\"),\n",
    "    # No tool: General greeting\n",
    "    (\"General Question\", \"Hello! What can you help me with today?\"),\n",
    "]\n",
    "\n",
    "# Run all tests and collect metrics\n",
    "# Cache behavior: First request after deployment writes to cache, subsequent requests read from cache\n",
    "# If cache is already warm (within 5-min TTL), all requests will show cache reads\n",
    "for i, (test_name, prompt) in enumerate(TEST_PROMPTS):\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Test {i + 1}: {test_name}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    response = invoke_agent(prompt)\n",
    "    print(response)\n",
    "\n",
    "    # Fetch and collect metrics\n",
    "    metrics = get_latest_trace_metrics(\n",
    "        agent_name=\"customer-support-v3-caching\",\n",
    "        wait_seconds=5,\n",
    "        max_retries=5,\n",
    "        timeout_seconds=120,\n",
    "    )\n",
    "    print_metrics(metrics, test_name)\n",
    "    collect_metric(metrics, test_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary table\n",
    "print_metrics_table()\n",
    "\n",
    "# Save metrics for comparison in later notebooks\n",
    "from utils.langfuse_metrics import save_metrics\n",
    "save_metrics(\"v3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Compare with v2 (Quick Wins)\n",
    "\n",
    "Enter your metrics from Lab 02 (v2 quick wins) to compare cost, latency, and token usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.langfuse_metrics import calculate_totals_from_collected, load_metrics, print_comparison\n",
    "\n",
    "# Load metrics from Lab 02 (saved automatically when you ran print_metrics_table())\n",
    "v2 = load_metrics(\"v2\")\n",
    "\n",
    "# Or enter manually if Lab 02 metrics weren't saved:\n",
    "# v2 = {\"total_cost\": 0.0858, \"avg_latency\": 7.30, \"total_input_tokens\": 19920, \"total_output_tokens\": 1737}\n",
    "\n",
    "# Print comparison (current metrics auto-calculated from collected)\n",
    "print_comparison(\n",
    "    prev_name=\"v2 (Quick Wins)\",\n",
    "    curr_name=\"v3 (Caching)\",\n",
    "    prev_cost=v2[\"total_cost\"],\n",
    "    prev_latency=v2[\"avg_latency\"],\n",
    "    prev_input_tokens=v2[\"total_input_tokens\"],\n",
    "    prev_output_tokens=v2[\"total_output_tokens\"],\n",
    ")\n",
    "\n",
    "# Show cache-specific metrics (unique to v3)\n",
    "totals = calculate_totals_from_collected()\n",
    "print(\"\\nCache Metrics (v3 only):\")\n",
    "print(f\"  Cache Read Tokens:  {totals['total_cache_read_tokens']:,}\")\n",
    "print(f\"  Cache Write Tokens: {totals['total_cache_write_tokens']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we enabled **two types of caching** on top of the v2 optimizations:\n",
    "\n",
    "| Cache Type | Configuration | Tokens Cached |\n",
    "|------------|---------------|---------------|\n",
    "| **System Prompt** | `SystemContentBlock` + `cachePoint` | ~1,100 tokens |\n",
    "| **Tool Definitions** | `cache_tools=\"default\"` | ~1,100 tokens |\n",
    "\n",
    "```python\n",
    "from strands.types.content import SystemContentBlock\n",
    "\n",
    "# 1. System prompt caching\n",
    "system_prompt = [\n",
    "    SystemContentBlock(text=SYSTEM_PROMPT_TEXT),  # Must be 1,024+ tokens\n",
    "    SystemContentBlock(cachePoint={\"type\": \"default\"})\n",
    "]\n",
    "\n",
    "# 2. Tool definition caching\n",
    "model = BedrockModel(\n",
    "    cache_tools=\"default\",\n",
    "    ...\n",
    ")\n",
    "\n",
    "agent = Agent(model=model, system_prompt=system_prompt)\n",
    "```\n",
    "\n",
    "**Key Observations:**\n",
    "- First request after cache expires writes to cache (premium pricing)\n",
    "- Subsequent requests read from cache (discounted pricing)\n",
    "- The 5-minute TTL refreshes on each cache hit\n",
    "- System prompt must be **at least 1,024 tokens** for caching\n",
    "\n",
    "**When to use caching:**\n",
    "- Agents with consistent traffic (requests within 5-minute windows)\n",
    "- Static system prompts and tool definitions\n",
    "- High-volume production workloads\n",
    "\n",
    "**Pricing:** See [Amazon Bedrock Pricing](https://aws.amazon.com/bedrock/pricing/) for current cache read/write rates.\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** In Lab 04, we'll explore **model routing** to use cheaper models for simple queries.\n",
    "\n",
    "**Next notebook:** [04-llm-routing.ipynb](./04-llm-routing.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup\n",
    "\n",
    "To delete the agent deployed in this notebook, uncomment and run the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to delete resources created in this lab\n",
    "# agentcore_runtime.destroy(delete_ecr_repo=True)\n",
    "# print(f\"Deleted agent and ECR repository: {agent_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "developer-journey (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
