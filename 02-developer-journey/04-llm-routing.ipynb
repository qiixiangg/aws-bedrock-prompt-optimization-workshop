{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Lab 04: LLM Routing\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we implement **model routing** to use cheaper models for simple queries while preserving quality for complex ones. The key insight is that **an LLM can classify queries more accurately than keyword matching**.\n",
    "\n",
    "**What you'll learn:**\n",
    "- How to classify query complexity using an LLM\n",
    "- How to route queries to appropriate models (Haiku vs Sonnet)\n",
    "- How to verify routing decisions in Langfuse\n",
    "- Cost savings from intelligent routing\n",
    "\n",
    "**Routing Strategy:**\n",
    "- Simple queries → Claude Haiku (smaller, faster, cheaper)\n",
    "- Complex queries → Claude Sonnet (more capable, higher quality)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed Labs 01-03\n",
    "\n",
    "## Workshop Journey\n",
    "\n",
    "```\n",
    "01 Baseline → 02 Quick Wins → 03 Caching → [04 Routing] → 05 Guardrails → 06 Gateway → 07 Evaluations\n",
    "                                               ↑\n",
    "                                          You are here\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "import boto3\n",
    "from bedrock_agentcore_starter_toolkit import Runtime\n",
    "\n",
    "region = os.environ.get(\"AWS_DEFAULT_REGION\", \"us-east-1\")\n",
    "control_client = boto3.client(\"bedrock-agentcore-control\", region_name=region)\n",
    "data_client = boto3.client(\"bedrock-agentcore\", region_name=region)\n",
    "agentcore_runtime = Runtime()\n",
    "\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Langfuse Host: {os.environ.get('LANGFUSE_BASE_URL', 'Not set')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Step 2: Understanding Model Routing\n",
    "\n",
    "### Why Route Between Models?\n",
    "\n",
    "Not all queries require the same level of reasoning. A simple \"What's your return policy?\" doesn't need the full power of Sonnet—Haiku can handle it at a fraction of the cost.\n",
    "\n",
    "**Claude Haiku** — ~4x cheaper than Sonnet, best for simple Q&A, lookups, and greetings. Requires 4,096 tokens minimum for prompt caching.\n",
    "\n",
    "**Claude Sonnet** — More capable model for complex reasoning and troubleshooting. Requires 1,024 tokens minimum for prompt caching.\n",
    "\n",
    "Our system prompt (~1,030 tokens) only meets Sonnet's caching threshold. Haiku requests won't benefit from prompt caching, but the ~4x cost savings still make it worthwhile for simple queries.\n",
    "\n",
    "### Routing Approaches\n",
    "\n",
    "**Keyword matching** — Fast with no LLM cost, but brittle and misses semantic variations.\n",
    "\n",
    "**LLM-based classification** — Accurate and handles edge cases, with negligible overhead.\n",
    "\n",
    "**Embedding similarity** — No LLM call needed, but requires training data and more complexity.\n",
    "\n",
    "We'll use **LLM-based classification** with Haiku—the classification cost is negligible compared to the savings from routing simple queries away from Sonnet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "### Query Classification Examples\n",
    "\n",
    "These examples match our test prompts:\n",
    "\n",
    "**Simple → Haiku:**\n",
    "- \"What is your return policy for laptops?\" — Single factual lookup\n",
    "- \"Tell me about your smartphone options\" — Direct product question\n",
    "- \"Hello! What can you help me with today?\" — Greeting\n",
    "\n",
    "**Complex → Sonnet:**\n",
    "- \"My laptop won't turn on, can you help me troubleshoot?\" — Multi-step troubleshooting\n",
    "- \"I want to buy a laptop. What are the specs and what's the return policy?\" — Multiple questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Step 3: Review the Routing Logic\n",
    "\n",
    "The v4 agent uses Haiku to classify queries before routing to the appropriate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.v4_routing import CLASSIFIER_PROMPT\n",
    "\n",
    "print(\"=== CLASSIFIER PROMPT ===\")\n",
    "print(CLASSIFIER_PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "### How the Classifier Works\n",
    "\n",
    "1. **Haiku receives the query** with the classifier prompt\n",
    "2. **Haiku responds** with a single word: \"simple\" or \"complex\"\n",
    "3. **Router parses the response** and selects the model:\n",
    "   - \"simple\" → Haiku handles the full request\n",
    "   - \"complex\" → Sonnet handles the full request\n",
    "\n",
    "**Classification overhead:** ~400 input tokens + ~5 output tokens per query\n",
    "\n",
    "This overhead is negligible compared to the savings from routing 60-70% of queries to Haiku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the v4 agent code\n",
    "agent_file = Path(\"agents/v4_routing.py\")\n",
    "print(agent_file.read_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Step 4: Deploy the Routing Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_name = \"customer_support_v4_routing\"\n",
    "agent_file = str(Path(\"agents/v4_routing.py\").absolute())\n",
    "requirements_file = str(Path(\"requirements-for-agentcore.txt\").absolute())\n",
    "\n",
    "print(f\"Agent name: {agent_name}\")\n",
    "print(f\"Agent file: {agent_file}\")\n",
    "print(f\"Requirements: {requirements_file}\")\n",
    "\n",
    "# Clean up any existing build files from previous labs\n",
    "for f in [\"Dockerfile\", \".dockerignore\", \".bedrock_agentcore.yaml\"]:\n",
    "    p = Path(f)\n",
    "    if p.exists():\n",
    "        p.unlink()\n",
    "        print(f\"Removed existing: {f}\")\n",
    "\n",
    "print(f\"Configuring agent: {agent_name}\")\n",
    "agentcore_runtime.configure(\n",
    "    entrypoint=agent_file,\n",
    "    auto_create_execution_role=True,\n",
    "    auto_create_ecr=True,\n",
    "    requirements_file=requirements_file,\n",
    "    region=region,\n",
    "    agent_name=agent_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify Dockerfile for Langfuse\n",
    "dockerfile_path = Path(\"Dockerfile\")\n",
    "if dockerfile_path.exists():\n",
    "    content = dockerfile_path.read_text()\n",
    "    if \"opentelemetry-instrument\" in content:\n",
    "        import re\n",
    "\n",
    "        content = re.sub(\n",
    "            r'CMD \\[\"opentelemetry-instrument\", \"python\", \"-m\", \"([^\"]+)\"\\]', r'CMD [\"python\", \"-m\", \"\\1\"]', content\n",
    "        )\n",
    "        dockerfile_path.write_text(content)\n",
    "        print(\"Dockerfile modified for Langfuse\")\n",
    "    else:\n",
    "        print(\"Dockerfile already configured or using different format\")\n",
    "else:\n",
    "    print(\"Dockerfile not found - will be created during deployment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_vars = {\n",
    "    \"LANGFUSE_BASE_URL\": os.environ.get(\"LANGFUSE_BASE_URL\"),\n",
    "    \"LANGFUSE_PUBLIC_KEY\": os.environ.get(\"LANGFUSE_PUBLIC_KEY\"),\n",
    "    \"LANGFUSE_SECRET_KEY\": os.environ.get(\"LANGFUSE_SECRET_KEY\"),\n",
    "    \"PYTHONUNBUFFERED\": \"1\",\n",
    "}\n",
    "\n",
    "print(\"Deploying to AgentCore Runtime...\")\n",
    "launch_result = agentcore_runtime.launch(env_vars=env_vars, auto_update_on_conflict=True)\n",
    "agent_arn = launch_result.agent_arn\n",
    "print(f\"Agent deployed: {agent_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cpl02jcx3rq",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the agent ARN for later use\n",
    "agent_arn = launch_result.agent_arn\n",
    "print(f\"Agent ARN: {agent_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Step 5: Test Model Routing\n",
    "\n",
    "Let's run the same test prompts and observe which model handles each query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_agent(prompt):\n",
    "    \"\"\"Invoke the agent via AgentCore API.\"\"\"\n",
    "    response = data_client.invoke_agent_runtime(\n",
    "        agentRuntimeArn=agent_arn,\n",
    "        runtimeSessionId=str(uuid.uuid4()),\n",
    "        payload=json.dumps({\"prompt\": prompt}).encode(),\n",
    "    )\n",
    "    return json.loads(response[\"response\"].read().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.langfuse_metrics import (\n",
    "    clear_metrics,\n",
    "    collect_metric,\n",
    "    get_latest_trace_metrics,\n",
    "    print_metrics,\n",
    "    print_metrics_table,\n",
    ")\n",
    "\n",
    "clear_metrics()\n",
    "\n",
    "TEST_PROMPTS = [\n",
    "    (\"Return Policy\", \"What is your return policy for laptops?\"),\n",
    "    (\"Product Info\", \"Tell me about your smartphone options\"),\n",
    "    (\"Technical Support\", \"My laptop won't turn on, can you help me troubleshoot?\"),\n",
    "    (\"Multi-part Question\", \"I want to buy a laptop. What are the specs and what's the return policy?\"),\n",
    "    (\"General Question\", \"Hello! What can you help me with today?\"),\n",
    "]\n",
    "\n",
    "for test_name, prompt in TEST_PROMPTS:\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Test: {test_name}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    result = invoke_agent(prompt)\n",
    "\n",
    "    if isinstance(result, dict):\n",
    "        print(f\"Model used: {result.get('model_used', 'N/A')}\")\n",
    "        print(f\"Complexity: {result.get('complexity', 'N/A')}\")\n",
    "        print(f\"Response: {str(result.get('response', result))[:200]}...\")\n",
    "    else:\n",
    "        print(result)\n",
    "\n",
    "    # Get metrics from parent trace (includes classifier + main agent)\n",
    "    metrics = get_latest_trace_metrics(\n",
    "        agent_name=\"customer-support-v4-routing\",\n",
    "        wait_seconds=5,\n",
    "        max_retries=5,\n",
    "        timeout_seconds=120,\n",
    "    )\n",
    "    print_metrics(metrics, test_name)\n",
    "    collect_metric(metrics, test_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics_table()\n",
    "\n",
    "# Save metrics for comparison in later notebooks\n",
    "from utils.langfuse_metrics import save_metrics\n",
    "save_metrics(\"v4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "### Expected Routing Results\n",
    "\n",
    "**Routed to Haiku (3 queries):**\n",
    "- Return Policy — Single factual lookup\n",
    "- Product Info — Direct product question  \n",
    "- General Question — Simple greeting\n",
    "\n",
    "**Routed to Sonnet (2 queries):**\n",
    "- Technical Support — Multi-step troubleshooting\n",
    "- Multi-part Question — Multiple questions requiring reasoning\n",
    "\n",
    "**Result:** 60% of queries routed to the cheaper model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d822eee9",
   "metadata": {},
   "source": [
    "## Step 6: Compare with v3 (Caching)\n",
    "\n",
    "Enter your metrics from Lab 03 (v3 caching) to compare with v4 routing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k1bezj7akz9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.langfuse_metrics import load_metrics, print_comparison\n",
    "\n",
    "# Load metrics from Lab 03 (saved automatically when you ran print_metrics_table())\n",
    "v3 = load_metrics(\"v3\")\n",
    "\n",
    "# Or enter manually if Lab 03 metrics weren't saved:\n",
    "# v3 = {\"total_cost\": 0.0438, \"avg_latency\": 8.10, \"total_input_tokens\": 4228, \"total_output_tokens\": 1795}\n",
    "\n",
    "# Print comparison (current metrics auto-calculated from collected)\n",
    "print_comparison(\n",
    "    prev_name=\"v3 (Caching)\",\n",
    "    curr_name=\"v4 (Routing)\",\n",
    "    prev_cost=v3[\"total_cost\"],\n",
    "    prev_latency=v3[\"avg_latency\"],\n",
    "    prev_input_tokens=v3[\"total_input_tokens\"],\n",
    "    prev_output_tokens=v3[\"total_output_tokens\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we implemented intelligent model routing:\n",
    "\n",
    "1. **LLM-based classification** — Haiku classifies queries as \"simple\" or \"complex\" using a single-word response\n",
    "2. **Cost-effective routing** — Simple queries go to Haiku (~4x cheaper), complex to Sonnet\n",
    "3. **Prompt caching for Sonnet only** — Haiku requires 4,096 tokens minimum (our prompt is ~1,030), so only Sonnet requests benefit from caching\n",
    "\n",
    "**Key insights:**\n",
    "\n",
    "- **LLM classification beats keyword matching** — Handles semantic variations and edge cases\n",
    "- **Simple text parsing is reliable** — Few-shot examples in the prompt ensure consistent \"simple\" or \"complex\" responses\n",
    "- **Real cost savings** — Compare the v3 vs v4 metrics above to see actual savings from routing\n",
    "\n",
    "**Next:** In Lab 05, we'll add Bedrock Guardrails to filter off-topic queries before they reach the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cleanup\n",
    "\n",
    "To delete the agent deployed in this notebook, uncomment and run the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to delete resources created in this lab\n",
    "# agentcore_runtime.destroy(delete_ecr_repo=True)\n",
    "# print(f\"Deleted agent and ECR repository: {agent_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "developer-journey (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
