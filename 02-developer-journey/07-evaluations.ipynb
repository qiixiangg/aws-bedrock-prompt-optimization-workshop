{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 07: Evaluations\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this final notebook, we run comprehensive evaluations across all deployed agent versions to validate that optimizations improved cost and latency **without degrading quality**.\n",
    "\n",
    "**What you'll learn:**\n",
    "- How to run systematic evaluations across agent versions\n",
    "- How to compare metrics side-by-side\n",
    "- How to validate quality hasn't degraded\n",
    "- How to generate a final optimization report\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed Labs 01-06 (all agent versions deployed)\n",
    "\n",
    "## Workshop Journey\n",
    "\n",
    "```\n",
    "01 Baseline → 02 Quick Wins → 03 Caching → 04 Routing → 05 Guardrails → 06 Gateway → [07 Evaluations]\n",
    "                                                                                          ↑\n",
    "                                                                                     You are here\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "region = os.environ.get(\"AWS_DEFAULT_REGION\", \"us-east-1\")\n",
    "control_client = boto3.client(\"bedrock-agentcore-control\", region_name=region)\n",
    "data_client = boto3.client(\"bedrock-agentcore\", region_name=region)\n",
    "\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Langfuse Host: {os.environ.get('LANGFUSE_BASE_URL', 'https://cloud.langfuse.com')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Find All Deployed Agent Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_agent_by_name(name_pattern):\n",
    "    \"\"\"Find agent ARN by name pattern (handles both hyphen and underscore naming).\"\"\"\n",
    "    response = control_client.list_agent_runtimes()\n",
    "    agents = response.get(\"agentRuntimes\", [])\n",
    "    for agent in agents:\n",
    "        agent_name = agent[\"agentRuntimeName\"]\n",
    "        # Handle both hyphen and underscore naming conventions\n",
    "        if name_pattern.replace(\"-\", \"_\") in agent_name or name_pattern in agent_name:\n",
    "            return agent[\"agentRuntimeArn\"]\n",
    "    return None\n",
    "\n",
    "\n",
    "# Find all agent versions\n",
    "versions = {\n",
    "    \"v1-baseline\": find_agent_by_name(\"v1-baseline\"),\n",
    "    \"v2-quick-wins\": find_agent_by_name(\"v2-quick-wins\"),\n",
    "    \"v3-caching\": find_agent_by_name(\"v3-caching\"),\n",
    "    \"v4-routing\": find_agent_by_name(\"v4-routing\"),\n",
    "    \"v5-guardrails\": find_agent_by_name(\"v5-guardrails\"),\n",
    "    \"v6-gateway\": find_agent_by_name(\"v6-gateway\"),\n",
    "}\n",
    "\n",
    "print(\"Found agent versions:\")\n",
    "for name, arn in versions.items():\n",
    "    status = \"Found\" if arn else \"Not found\"\n",
    "    print(f\"  {name}: {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Test Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard test prompts - each demonstrates a specific tool usage pattern\n",
    "TEST_PROMPTS = [\n",
    "    # Single tool: get_return_policy\n",
    "    {\"id\": \"return-policy\", \"query\": \"What is your return policy for laptops?\"},\n",
    "    # Single tool: get_product_info\n",
    "    {\"id\": \"product-info\", \"query\": \"Tell me about your smartphone options\"},\n",
    "    # Single tool: get_technical_support (Bedrock KB)\n",
    "    {\"id\": \"technical-support\", \"query\": \"My laptop won't turn on, can you help me troubleshoot?\"},\n",
    "    # Multi-tool: get_product_info + get_return_policy\n",
    "    {\"id\": \"multi-part\", \"query\": \"I want to buy a laptop. What are the specs and what's the return policy?\"},\n",
    "    # No tool: General greeting\n",
    "    {\"id\": \"general\", \"query\": \"Hello! What can you help me with today?\"},\n",
    "]\n",
    "\n",
    "test_scenarios = TEST_PROMPTS\n",
    "\n",
    "print(f\"Loaded {len(test_scenarios)} test scenarios:\")\n",
    "for scenario in test_scenarios:\n",
    "    print(f\"  - {scenario['id']}: {scenario['query'][:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Langfuse metrics helper\n",
    "from utils.langfuse_metrics import clear_metrics, get_latest_trace_metrics\n",
    "\n",
    "print(\"Langfuse metrics helper imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map version name to agent trace name (uses hyphens)\n",
    "TRACE_NAME_MAP = {\n",
    "    \"v1-baseline\": \"customer-support-v1-baseline\",\n",
    "    \"v2-quick-wins\": \"customer-support-v2-quick-wins\",\n",
    "    \"v3-caching\": \"customer-support-v3-caching\",\n",
    "    \"v4-routing\": \"customer-support-v4-routing\",\n",
    "    \"v5-guardrails\": \"customer-support-v5-guardrails\",\n",
    "    \"v6-gateway\": \"customer-support-v6-gateway\",\n",
    "}\n",
    "\n",
    "\n",
    "def invoke_agent(agent_arn, prompt):\n",
    "    \"\"\"Invoke agent and measure latency.\"\"\"\n",
    "    start_time = time.time()\n",
    "    response = data_client.invoke_agent_runtime(\n",
    "        agentRuntimeArn=agent_arn,\n",
    "        runtimeSessionId=str(uuid.uuid4()),\n",
    "        payload=json.dumps({\"prompt\": prompt}).encode(),\n",
    "    )\n",
    "    latency_ms = (time.time() - start_time) * 1000\n",
    "    result = json.loads(response[\"response\"].read().decode(\"utf-8\"))\n",
    "    return result, latency_ms\n",
    "\n",
    "\n",
    "def run_evaluation(version_name, agent_arn, scenarios):\n",
    "    \"\"\"Run all scenarios against an agent version and collect Langfuse metrics.\"\"\"\n",
    "    results = []\n",
    "    total_latency = 0\n",
    "    successful = 0\n",
    "    langfuse_metrics_list = []\n",
    "\n",
    "    agent_trace_name = TRACE_NAME_MAP.get(version_name, version_name)\n",
    "\n",
    "    print(f\"\\nEvaluating {version_name}...\")\n",
    "    clear_metrics()  # Clear for this version\n",
    "\n",
    "    for scenario in scenarios:\n",
    "        try:\n",
    "            _result, latency = invoke_agent(agent_arn, scenario[\"query\"])\n",
    "\n",
    "            # Fetch Langfuse metrics for this trace\n",
    "            metrics = get_latest_trace_metrics(\n",
    "                agent_name=agent_trace_name,\n",
    "                wait_seconds=3,\n",
    "                max_retries=3,\n",
    "                timeout_seconds=60,\n",
    "            )\n",
    "            langfuse_metrics_list.append(metrics)\n",
    "\n",
    "            results.append(\n",
    "                {\n",
    "                    \"scenario_id\": scenario[\"id\"],\n",
    "                    \"success\": True,\n",
    "                    \"latency_ms\": latency,\n",
    "                }\n",
    "            )\n",
    "            total_latency += latency\n",
    "            successful += 1\n",
    "            print(f\"  [{scenario['id']}] {latency:.0f}ms\")\n",
    "        except Exception as e:\n",
    "            results.append(\n",
    "                {\n",
    "                    \"scenario_id\": scenario[\"id\"],\n",
    "                    \"success\": False,\n",
    "                    \"error\": str(e),\n",
    "                }\n",
    "            )\n",
    "            langfuse_metrics_list.append({\"error\": str(e)})\n",
    "            print(f\"  [{scenario['id']}] FAILED: {e}\")\n",
    "\n",
    "    return {\n",
    "        \"version\": version_name,\n",
    "        \"results\": results,\n",
    "        \"total_scenarios\": len(scenarios),\n",
    "        \"successful\": successful,\n",
    "        \"avg_latency_ms\": total_latency / successful if successful > 0 else 0,\n",
    "        \"langfuse_metrics\": langfuse_metrics_list,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluations for all available versions\n",
    "all_results = {}\n",
    "\n",
    "for version_name, agent_arn in versions.items():\n",
    "    if agent_arn:\n",
    "        all_results[version_name] = run_evaluation(version_name, agent_arn, test_scenarios)\n",
    "    else:\n",
    "        print(f\"\\nSkipping {version_name} (not deployed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Generate Comparison Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A note on sample size:** This evaluation runs 5 test queries per version. With a small sample, individual runs may show variance — for example, one version might appear slightly slower or costlier than expected due to network jitter, cold starts, or cache timing. Don't over-index on small differences between adjacent versions (e.g., v4 vs v5).\n",
    "\n",
    "What to look for:\n",
    "- **Overall trend** from v1 → v6: cost and latency should generally decrease\n",
    "- **Large, consistent gains** (e.g., caching in v3, routing in v4) will be visible even at small scale\n",
    "- **Small or marginal differences** between versions may not be statistically significant\n",
    "\n",
    "In production, you would run a larger evaluation set (50-100+ queries) to get stable, reliable comparisons with tighter confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "rows = []\n",
    "for version_name, eval_result in all_results.items():\n",
    "    success_rate = (eval_result[\"successful\"] / eval_result[\"total_scenarios\"]) * 100\n",
    "\n",
    "    # Use Langfuse latency (server-side) instead of client-side E2E latency\n",
    "    lf_metrics = eval_result.get(\"langfuse_metrics\", [])\n",
    "    valid = [m for m in lf_metrics if \"error\" not in m]\n",
    "    avg_latency_s = sum(m.get(\"latency_seconds\", 0) or 0 for m in valid) / len(valid) if valid else 0\n",
    "\n",
    "    rows.append(\n",
    "        {\n",
    "            \"Version\": version_name,\n",
    "            \"Success Rate\": f\"{success_rate:.0f}%\",\n",
    "            \"Avg Latency (s)\": f\"{avg_latency_s:.2f}\",\n",
    "            \"Successful\": eval_result[\"successful\"],\n",
    "            \"Failed\": eval_result[\"total_scenarios\"] - eval_result[\"successful\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Langfuse metrics for each version\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"LANGFUSE METRICS BY VERSION\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "version_summaries = []\n",
    "for version_name, eval_result in all_results.items():\n",
    "    lf_metrics = eval_result.get(\"langfuse_metrics\", [])\n",
    "    valid = [m for m in lf_metrics if \"error\" not in m]\n",
    "\n",
    "    if valid:\n",
    "        total_input = sum(m.get(\"input_tokens\", 0) for m in valid)\n",
    "        total_output = sum(m.get(\"output_tokens\", 0) for m in valid)\n",
    "        total_cache_read = sum(m.get(\"cache_read_tokens\", 0) for m in valid)\n",
    "        total_cache_write = sum(m.get(\"cache_write_tokens\", 0) for m in valid)\n",
    "        total_cost = sum(m.get(\"cost_usd\", 0) for m in valid)\n",
    "        avg_latency = sum(m.get(\"latency_seconds\", 0) or 0 for m in valid) / len(valid)\n",
    "        avg_input = total_input / len(valid)\n",
    "\n",
    "        version_summaries.append(\n",
    "            {\n",
    "                \"Version\": version_name,\n",
    "                \"Avg Input Tokens\": f\"{avg_input:,.0f}\",\n",
    "                \"Avg Latency (s)\": f\"{avg_latency:.2f}\",\n",
    "                \"Total Cost\": f\"${total_cost:.4f}\",\n",
    "                \"Cache Read\": f\"{total_cache_read:,}\",\n",
    "                \"Cache Write\": f\"{total_cache_write:,}\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "if version_summaries:\n",
    "    df_langfuse = pd.DataFrame(version_summaries)\n",
    "    print(df_langfuse.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate comprehensive improvement from baseline (including token metrics)\n",
    "if \"v1-baseline\" in all_results and len(all_results) > 1:\n",
    "    baseline = all_results[\"v1-baseline\"]\n",
    "\n",
    "    # Get baseline Langfuse metrics\n",
    "    baseline_lf = baseline.get(\"langfuse_metrics\", [])\n",
    "    baseline_valid = [m for m in baseline_lf if \"error\" not in m]\n",
    "    baseline_input = (\n",
    "        sum(m.get(\"input_tokens\", 0) for m in baseline_valid) / len(baseline_valid) if baseline_valid else 0\n",
    "    )\n",
    "    baseline_cost = sum(m.get(\"cost_usd\", 0) for m in baseline_valid)\n",
    "    baseline_latency = (\n",
    "        sum(m.get(\"latency_seconds\", 0) or 0 for m in baseline_valid) / len(baseline_valid)\n",
    "        if baseline_valid\n",
    "        else 0\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"IMPROVEMENT VS BASELINE (v1)\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{'Version':<20} {'Input Tokens':<18} {'Latency':<15} {'Cost':<15}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'v1-baseline':<20} {'(baseline)':<18} {'(baseline)':<15} {'(baseline)':<15}\")\n",
    "\n",
    "    for version_name, eval_result in all_results.items():\n",
    "        if version_name == \"v1-baseline\":\n",
    "            continue\n",
    "\n",
    "        lf_metrics = eval_result.get(\"langfuse_metrics\", [])\n",
    "        valid = [m for m in lf_metrics if \"error\" not in m]\n",
    "\n",
    "        if valid and baseline_input > 0:\n",
    "            avg_input = sum(m.get(\"input_tokens\", 0) for m in valid) / len(valid)\n",
    "            total_cost = sum(m.get(\"cost_usd\", 0) for m in valid)\n",
    "            avg_latency = sum(m.get(\"latency_seconds\", 0) or 0 for m in valid) / len(valid)\n",
    "\n",
    "            token_change = ((baseline_input - avg_input) / baseline_input) * 100\n",
    "            latency_change = (\n",
    "                ((baseline_latency - avg_latency) / baseline_latency) * 100 if baseline_latency > 0 else 0\n",
    "            )\n",
    "            cost_change = ((baseline_cost - total_cost) / baseline_cost) * 100 if baseline_cost > 0 else 0\n",
    "\n",
    "            print(\n",
    "                f\"{version_name:<20} {token_change:+.1f}%{'':<12} {latency_change:+.1f}%{'':<10} {cost_change:+.1f}%\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"{version_name:<20} {'N/A':<18} {'N/A':<15} {'N/A':<15}\")\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nPositive % = improvement (reduction), Negative % = regression (increase)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: View Langfuse Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langfuse_base_url = os.environ.get(\"LANGFUSE_BASE_URL\", \"https://cloud.langfuse.com\")\n",
    "print(f\"View comprehensive metrics at: {langfuse_base_url}\")\n",
    "print(\"\\nFor detailed comparison:\")\n",
    "print(\"1. Filter by version tags (v1-baseline, v2-quick-wins, etc.)\")\n",
    "print(\"2. Compare token usage across versions\")\n",
    "print(\"3. Check cache hit rates (v3+)\")\n",
    "print(\"4. Verify model routing (v4+)\")\n",
    "print(\"5. Review guardrail interventions (v5+)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"WORKSHOP SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "Optimizations Applied:\n",
    "\n",
    "1. Quick Wins (v2)\n",
    "   - Concise system prompt: ~60% token reduction\n",
    "   - max_tokens limit: Bounded output\n",
    "   - stop_sequences: Early termination\n",
    "\n",
    "2. Prompt Caching (v3)\n",
    "   - System prompt caching: 90% discount on repeated requests\n",
    "   - Tool definition caching: Additional savings\n",
    "\n",
    "3. Model Routing (v4)\n",
    "   - Haiku for simple queries: 12x cheaper input tokens\n",
    "   - Sonnet for complex queries: Maintained quality\n",
    "\n",
    "4. Guardrails (v5)\n",
    "   - Topic filtering: Block off-topic queries\n",
    "   - Content filtering: Improve safety\n",
    "   - Zero LLM tokens for blocked queries\n",
    "\n",
    "5. Gateway (v6)\n",
    "   - Semantic tool search: Load only relevant tools\n",
    "   - Reduced context size: Up to 75% fewer tool tokens\n",
    "\n",
    "Success Criteria:\n",
    "- Cost should decrease from v1 to v6\n",
    "- Latency should improve from v1 to v6\n",
    "- Quality (success rate) should NOT degrade\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup (Optional)\n",
    "\n",
    "Delete all deployed agents if you're done with the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to delete all customer-support agents and their ECR repositories\n",
    "# from utils.runtime_helpers import cleanup_agents\n",
    "# cleanup_agents(control_client, name_prefix=\"customer_support\", region=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You've completed the Prompt Optimization Workshop!\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Start with measurement (baseline metrics)\n",
    "- Apply optimizations incrementally\n",
    "- Validate quality at each step\n",
    "- Use observability (Langfuse) to verify improvements\n",
    "\n",
    "**Next Steps:**\n",
    "- Apply these techniques to your own agents\n",
    "- Explore additional Bedrock features\n",
    "- Set up automated evaluation pipelines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
